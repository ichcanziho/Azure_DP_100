{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65ddffba-4c0e-4155-a11f-0144177c98a2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Exercise: Horovod with Petastorm for training a deep learning model\n",
    "\n",
    "In this exercise we are going to build a model on the Boston housing dataset and distribute the deep learning training process using both HorovodRunner and Petastorm.\n",
    "\n",
    "**Required Libraries**: \n",
    "* `petastorm==0.8.2` via PyPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9071f16-ecdb-458e-bdbe-f03f41e3834d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Run the following cell to set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "042aac77-132b-4046-96f1-709d7121208c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run \"../Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b9c4ccd-0956-4578-ba18-bb6ce57ddd5a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1. Load and process data\n",
    "\n",
    "We again load the Boston housing data. However, as we saw in the demo, for Horovod we want to shard the data before passing into HorovodRunner. \n",
    "\n",
    "For the `get_dataset` function below, load the data, split into 80/20 train-test, standardize the features and return train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27369d75-a655-47c4-a7ed-3b83fa58ab31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_dataset(rank=0, size=1):\n",
    "  scaler = StandardScaler()\n",
    "  \n",
    "  boston_housing = load_boston()\n",
    "\n",
    "  # split 80/20 train-test\n",
    "  X_train, X_test, y_train, y_test = train_test_split(boston_housing.data,\n",
    "                                                          boston_housing.target,\n",
    "                                                          test_size=0.2,\n",
    "                                                          random_state=1)\n",
    "  \n",
    "  scaler.fit(X_train)\n",
    "  X_train = scaler.transform(X_train[rank::size])\n",
    "  y_train = y_train[rank::size]\n",
    "  X_test = scaler.transform(X_test[rank::size])\n",
    "  y_test = y_test[rank::size]\n",
    "  \n",
    "  return (X_train, y_train), (X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd6ae0c2-0576-43f0-9cfb-d7119f18c834",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##2. Build Model\n",
    "\n",
    "Using the same model from earlier, let's define our model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20211fa2-6352-4828-a694-b27d971798d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(42) # For reproducibility\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "def build_model():\n",
    "  return Sequential([Dense(50, input_dim=13, activation='relu'),\n",
    "                    Dense(20, activation='relu'),\n",
    "                    Dense(1, activation='linear')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f9974d5-c012-45f4-a793-7cad482a6a8f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 3. Horovod\n",
    "\n",
    "In order to distribute the training of our Keras model with Horovod, we must define our `run_training_horovod` training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "befba0b9-d640-42eb-9913-db5ee09dfa18",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ANSWER\n",
    "import horovod.tensorflow.keras as hvd\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import *\n",
    "\n",
    "def run_training_horovod():\n",
    "  # Horovod: initialize Horovod.\n",
    "  hvd.init()\n",
    "  print(f\"Rank is: {hvd.rank()}\")\n",
    "  print(f\"Size is: {hvd.size()}\")\n",
    "  \n",
    "  (X_train, y_train), (X_test, y_test) = get_dataset(hvd.rank(), hvd.size())\n",
    "  \n",
    "  model = build_model()\n",
    "  \n",
    "  from tensorflow.keras import optimizers\n",
    "  optimizer = optimizers.Adam(lr=0.001*hvd.size())\n",
    "  optimizer = hvd.DistributedOptimizer(optimizer)\n",
    "  model.compile(optimizer=optimizer, loss=\"mse\", metrics=[\"mse\"])\n",
    "  checkpoint_dir = f\"{ml_working_path}/horovod_checkpoint_weights_lab.ckpt\"\n",
    "  \n",
    "  callbacks = [\n",
    "    # Horovod: broadcast initial variable states from rank 0 to all other processes.\n",
    "    # This is necessary to ensure consistent initialization of all workers when\n",
    "    # training is started with random weights or restored from a checkpoint.\n",
    "    hvd.callbacks.BroadcastGlobalVariablesCallback(0),\n",
    "\n",
    "    # Horovod: average metrics among workers at the end of every epoch.\n",
    "    # Note: This callback must be in the list before the ReduceLROnPlateau,\n",
    "    # TensorBoard or other metrics-based callbacks.\n",
    "    hvd.callbacks.MetricAverageCallback(),\n",
    "\n",
    "    # Horovod: using `lr = 1.0 * hvd.size()` from the very beginning leads to worse final\n",
    "    # accuracy. Scale the learning rate `lr = 1.0` ---> `lr = 1.0 * hvd.size()` during\n",
    "    # the first five epochs. See https://arxiv.org/abs/1706.02677 for details.\n",
    "    hvd.callbacks.LearningRateWarmupCallback(warmup_epochs=5, verbose=1),\n",
    "    \n",
    "    # Reduce the learning rate if training plateaus.\n",
    "    ReduceLROnPlateau(patience=10, verbose=1)\n",
    "    \n",
    "  ]\n",
    "  \n",
    "  # Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them.\n",
    "  if hvd.rank() == 0:\n",
    "    callbacks.append(ModelCheckpoint(checkpoint_dir, save_weights_only=True))\n",
    "  \n",
    "  history = model.fit(X_train, y_train, callbacks=callbacks, validation_split=.2, epochs=30, batch_size=16, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ca78a56-5727-47be-b096-289948316024",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's now run our model on all workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ecee105d-f6dc-4651-952b-e97b28dea219",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ANSWER\n",
    "from sparkdl import HorovodRunner\n",
    "\n",
    "hr = HorovodRunner(np=0)\n",
    "hr.run(run_training_horovod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42ccb4f6-c218-4441-87b6-bd00718dcc98",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 4. Horovod with Petastorm\n",
    "\n",
    "We're now going to build a distributed deep learning model capable of handling data in Apache Parquet format. To do so, we can use Horovod along with Petastorm. \n",
    "\n",
    "First let's load the Boston housing data, and create a Spark DataFrame from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93bcaa14-e922-445b-af34-ab567ef5216c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "boston_housing = load_boston()\n",
    "\n",
    "# split 80/20 train-test\n",
    "X_train, X_test, y_train, y_test = train_test_split(boston_housing.data,\n",
    "                                                        boston_housing.target,\n",
    "                                                        test_size=0.2,\n",
    "                                                        random_state=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# concatenate our features and label, then create a Spark DataFrame from our Pandas DataFrame.\n",
    "data = pd.concat([pd.DataFrame(X_train, columns=boston_housing.feature_names), \n",
    "                  pd.DataFrame(y_train, columns=[\"label\"])], axis=1)\n",
    "trainDF = spark.createDataFrame(data)\n",
    "display(trainDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2955e447-426f-4167-970c-984229a704cf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Create Vectors\n",
    "\n",
    "Use the VectorAssembler to combine all the features (not including the label) into a single column called `features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8b5ed4e-546c-4d3f-b57d-1d7a4ffd4e98",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ANSWER\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols=boston_housing.feature_names, outputCol=\"features\")\n",
    "vecTrainDF = vecAssembler.transform(trainDF).select(\"features\", \"label\")\n",
    "display(vecTrainDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a201dd3-7a0b-4117-b2b0-94529764b95c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's now create a UDF to convert our Vector into an Array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce7bc99b-3912-417c-9004-a56a881e0d2b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "val toArray = udf { v: Vector => v.toArray }\n",
    "spark.udf.register(\"toArray\", toArray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57f39a7a-6abc-44cb-8b9e-78b16583e779",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Save the DataFrame out as a parquet file to DBFS. \n",
    "\n",
    "Let's remember to remove the committed and started metadata files in the Parquet folder! Horovod with Petastorm will not work otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f60a1044-5b13-4d58-9c57-62423ed34817",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_path = f\"{workingDir}/petastorm.parquet\"\n",
    "vecTrainDF.selectExpr(\"toArray(features) AS features\", \"label\").repartition(8).write.mode(\"overwrite\").parquet(file_path)\n",
    "[dbutils.fs.rm(i.path) for i in dbutils.fs.ls(file_path) if (\"_committed_\" in i.name) | (\"_started_\" in i.name)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1646946b-6672-40c3-817a-69e5b8cdca52",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Let's now define our `run_training_horovod` to format our data using Petastorm and distribute the training of our Keras model using Horovod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "817f25d8-d867-4d75-a397-27791a7bba70",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ANSWER\n",
    "from petastorm import make_batch_reader\n",
    "from petastorm.tf_utils import make_petastorm_dataset\n",
    "import horovod.tensorflow.keras as hvd\n",
    "\n",
    "abs_file_path = file_path.replace(\"dbfs:/\", \"/dbfs/\")\n",
    "\n",
    "def run_training_horovod():\n",
    "  # Horovod: initialize Horovod.\n",
    "  hvd.init()\n",
    "  with make_batch_reader(\"file://\" + abs_file_path, \n",
    "                         num_epochs=100, \n",
    "                         cur_shard=hvd.rank(), \n",
    "                         shard_count=hvd.size()) as reader:\n",
    "    \n",
    "    dataset = make_petastorm_dataset(reader).map(lambda x: (tf.reshape(x.features, [-1,13]),\n",
    "                                                            tf.reshape(x.label, [-1,1])))\n",
    "    \n",
    "    model = build_model()\n",
    "    from tensorflow.keras import optimizers\n",
    "    optimizer = optimizers.Adam(lr=0.001*hvd.size())\n",
    "    optimizer = hvd.DistributedOptimizer(optimizer)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    \n",
    "    checkpoint_dir = f\"{ml_working_path}/petastorm_checkpoint_weights_lab.ckpt\"\n",
    "    \n",
    "    callbacks = [\n",
    "      hvd.callbacks.BroadcastGlobalVariablesCallback(0),\n",
    "      hvd.callbacks.MetricAverageCallback(),\n",
    "      hvd.callbacks.LearningRateWarmupCallback(warmup_epochs=5, verbose=1),\n",
    "      ReduceLROnPlateau(monitor=\"loss\", patience=10, verbose=1)\n",
    "    ]\n",
    "\n",
    "    # Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them.\n",
    "    if hvd.rank() == 0:\n",
    "      callbacks.append(ModelCheckpoint(checkpoint_dir, save_weights_only=True))\n",
    "\n",
    "    history = model.fit(dataset, callbacks=callbacks, steps_per_epoch=10, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c021bb48-f646-4253-9bf6-53cc018b90a3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Finally, let's run our newly define Horovod training function with Petastorm to run across all workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "117b63f6-2bcd-4eae-acdd-4c1535d592f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ANSWER\n",
    "from sparkdl import HorovodRunner\n",
    "\n",
    "hr = HorovodRunner(np=0)\n",
    "hr.run(run_training_horovod)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "3. Exercise Horovod Petastorm",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
