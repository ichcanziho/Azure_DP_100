{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "775a56f4-2239-468c-aa21-881906036d0e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Horovod\n",
    "\n",
    "HorovodRunner is a general API to run distributed DL workloads on Databricks using Uber’s [Horovod](https://github.com/uber/horovod) framework. By integrating Horovod with Spark’s barrier mode, Databricks is able to provide higher stability for long-running deep learning training jobs on Spark. HorovodRunner takes a Python method that contains DL training code with Horovod hooks. This method gets pickled on the driver and sent to Spark workers. A Horovod MPI job is embedded as a Spark job using barrier execution mode. The first executor collects the IP addresses of all task executors using BarrierTaskContext and triggers a Horovod job using mpirun. Each Python MPI process loads the pickled program back, deserializes it, and runs it.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](https://files.training.databricks.com/images/horovod-runner.png)\n",
    "\n",
    "For additional resources, see:\n",
    "* [Horovod Runner Docs](https://docs.microsoft.com/en-us/azure/databricks/applications/deep-learning/distributed-training/horovod-runner)\n",
    "* [Horovod Runner webinar](https://vimeo.com/316872704/e79235f62c) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66cda9af-f842-4786-aa94-ed9b232a1ee4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Run the following cell to set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c47df75-89f6-4ae5-b729-0af95e0f98ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run \"./Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3cb5903-266e-4e46-aa4a-775fa08fb437",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e3fae6f-a139-4968-b43d-63fd4bcbe4fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(42) # For reproducibility\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "def build_model():\n",
    "  return Sequential([Dense(20, input_dim=8, activation='relu'),\n",
    "                    Dense(20, activation='relu'),\n",
    "                    Dense(1, activation='linear')]) # Keep the output layer as linear because this is a regression problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7c31c95-eea4-4b5c-a32e-bae0b2c84f96",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Shard Data\n",
    "\n",
    "From the [Horovod docs](https://github.com/horovod/horovod/blob/master/docs/concepts.rst):\n",
    "\n",
    "Horovod core principles are based on the MPI concepts size, rank, local rank, allreduce, allgather, and broadcast. These are best explained by example. Say we launched a training script on 4 servers, each having 4 GPUs. If we launched one copy of the script per GPU:\n",
    "\n",
    "* Size would be the number of processes, in this case, 16.\n",
    "\n",
    "* Rank would be the unique process ID from 0 to 15 (size - 1).\n",
    "\n",
    "* Local rank would be the unique process ID within the server from 0 to 3.\n",
    "\n",
    "We need to shard our data across our processes.  **NOTE:** We are using a Pandas DataFrame for demo purposes. In the next notebook we will use Parquet files with Petastorm for better scalability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f958472b-4ba5-4dc3-b3ed-84bd1926b2c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets.california_housing import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def get_dataset(rank=0, size=1):\n",
    "  scaler = StandardScaler()\n",
    "  cal_housing = fetch_california_housing(data_home=\"/dbfs/ml/\" + str(rank) + \"/\")\n",
    "  X_train, X_test, y_train, y_test = train_test_split(cal_housing.data,\n",
    "                                                       cal_housing.target,\n",
    "                                                       test_size=0.2,\n",
    "                                                       random_state=1)\n",
    "  scaler.fit(X_train)\n",
    "  X_train = scaler.transform(X_train[rank::size])\n",
    "  y_train = y_train[rank::size]\n",
    "  X_test = scaler.transform(X_test[rank::size])\n",
    "  y_test = y_test[rank::size]\n",
    "  return (X_train, y_train), (X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "847b7777-0482-4bea-8d54-c20d2cda6cec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Horovod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cedc96bb-2bf8-4805-a9da-06f73bf56533",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "import horovod.tensorflow.keras as hvd\n",
    "from keras import backend as K\n",
    "\n",
    "def run_training_horovod():\n",
    "  # Horovod: initialize Horovod.\n",
    "  hvd.init()\n",
    "  # If using GPU: pin GPU to be used to process local rank (one GPU per process)\n",
    "  # config = tf.ConfigProto()\n",
    "  # config.gpu_options.allow_growth = True\n",
    "  # config.gpu_options.visible_device_list = str(hvd.local_rank())\n",
    "  # K.set_session(tf.Session(config=config))\n",
    "  print(f\"Rank is: {hvd.rank()}\")\n",
    "  print(f\"Size is: {hvd.size()}\")\n",
    "  \n",
    "  (X_train, y_train), (X_test, y_test) = get_dataset(hvd.rank(), hvd.size())\n",
    "  \n",
    "  model = build_model()\n",
    "  \n",
    "  from tensorflow.keras import optimizers\n",
    "  # Horovod: adjust learning rate based on number of GPUs/CPUs.\n",
    "  optimizer = optimizers.Adam(lr=0.001*hvd.size())\n",
    "  \n",
    "  # Horovod: add Horovod Distributed Optimizer.\n",
    "  optimizer = hvd.DistributedOptimizer(optimizer)\n",
    "\n",
    "  model.compile(optimizer=optimizer, loss=\"mse\", metrics=[\"mse\"])\n",
    "  \n",
    "  history = model.fit(X_train, y_train, validation_split=.2, epochs=10, batch_size=64, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d05f35e6-6a72-4ee3-8e59-44015f0999cd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Test it out on just the driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "440ef8f1-7b65-4199-ae89-d24e5d480c9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sparkdl import HorovodRunner\n",
    "hr = HorovodRunner(np=-1)\n",
    "hr.run(run_training_horovod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73a8be20-c71c-4aa0-8631-ce634d5d85ab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Better Horovod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51c3e36d-9f9f-480d-93d1-fd5b6d8ba0fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import *\n",
    "\n",
    "def run_training_horovod():\n",
    "  # Horovod: initialize Horovod.\n",
    "  hvd.init()\n",
    "  # If using GPU: pin GPU to be used to process local rank (one GPU per process)\n",
    "  # config = tf.ConfigProto()\n",
    "  # config.gpu_options.allow_growth = True\n",
    "  # config.gpu_options.visible_device_list = str(hvd.local_rank())\n",
    "  # K.set_session(tf.Session(config=config))\n",
    "  \n",
    "  \n",
    "  \n",
    "  print(f\"Rank is: {hvd.rank()}\")\n",
    "  print(f\"Size is: {hvd.size()}\")\n",
    "  \n",
    "  (X_train, y_train), (X_test, y_test) = get_dataset(hvd.rank(), hvd.size())\n",
    "  \n",
    "  model = build_model()\n",
    "  \n",
    "  from tensorflow.keras import optimizers\n",
    "  # Horovod: adjust learning rate based on number of GPUs.\n",
    "  optimizer = optimizers.Adam(lr=0.001*hvd.size())\n",
    "  \n",
    "  # Horovod: add Horovod Distributed Optimizer.\n",
    "  optimizer = hvd.DistributedOptimizer(optimizer)\n",
    "\n",
    "  model.compile(optimizer=optimizer, loss=\"mse\", metrics=[\"mse\"])\n",
    "\n",
    "  # Use the optimized FUSE Mount\n",
    "  checkpoint_dir = f\"{ml_working_path}/horovod_checkpoint_weights.ckpt\"\n",
    "  \n",
    "  callbacks = [\n",
    "    # Horovod: broadcast initial variable states from rank 0 to all other processes.\n",
    "    # This is necessary to ensure consistent initialization of all workers when\n",
    "    # training is started with random weights or restored from a checkpoint.\n",
    "    hvd.callbacks.BroadcastGlobalVariablesCallback(0),\n",
    "\n",
    "    # Horovod: average metrics among workers at the end of every epoch.\n",
    "    # Note: This callback must be in the list before the ReduceLROnPlateau,\n",
    "    # TensorBoard or other metrics-based callbacks.\n",
    "    hvd.callbacks.MetricAverageCallback(),\n",
    "\n",
    "    # Horovod: using `lr = 1.0 * hvd.size()` from the very beginning leads to worse final\n",
    "    # accuracy. Scale the learning rate `lr = 1.0` ---> `lr = 1.0 * hvd.size()` during\n",
    "    # the first five epochs. See https://arxiv.org/abs/1706.02677 for details.\n",
    "    hvd.callbacks.LearningRateWarmupCallback(warmup_epochs=5, verbose=1),\n",
    "    \n",
    "    # Reduce the learning rate if training plateaus.\n",
    "    ReduceLROnPlateau(patience=10, verbose=1)\n",
    "  ]\n",
    "  \n",
    "  # Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them.\n",
    "  if hvd.rank() == 0:\n",
    "    callbacks.append(ModelCheckpoint(checkpoint_dir, save_weights_only=True))\n",
    "  \n",
    "  history = model.fit(X_train, y_train, validation_split=.2, epochs=10, batch_size=64, verbose=2, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bcd9f180-0530-4eb1-b773-441c1f3708c3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Test it out on just the driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be4a9170-da6b-45b6-a09c-073210930087",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sparkdl import HorovodRunner\n",
    "hr = HorovodRunner(np=-1)\n",
    "hr.run(run_training_horovod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f8f0dd9-698b-4cd0-9b4e-0f261db8937c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Run on all workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82393b05-faa7-4dba-8e80-2ae384ca843a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sparkdl import HorovodRunner\n",
    "hr = HorovodRunner(np=0)\n",
    "hr.run(run_training_horovod)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "1. Horovod",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
