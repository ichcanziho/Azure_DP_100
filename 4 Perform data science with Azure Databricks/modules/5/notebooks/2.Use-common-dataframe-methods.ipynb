{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfaad047-e724-45f4-bb49-e68e9406a6c0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Use common DataFrame methods\n",
    "\n",
    "In the previous notebook, you ended off by executing a count of records in a DataFrame. We will now build upon that concept by introducing common DataFrame methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed6059ef-f2dc-489c-bb5e-99fbbe639df8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Technical Accomplishments:**\n",
    "* Develop familiarity with the `DataFrame` APIs\n",
    "* Use common DataFrame methods for performance\n",
    "* Explore the Spark API documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35dc62e4-3218-41ed-a85e-d961141d9a7f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Getting Started\n",
    "\n",
    "Run the following cell to configure our \"classroom.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc69bf37-44dc-46d7-8437-c212afbbc2e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run \"./Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "539cbf7b-54dd-4dcb-b350-ab579ae73b81",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Prepare the data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3e01f35-91cc-4644-9882-66290ed83a02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(source, sasEntity, sasToken) = getAzureDataSource()\n",
    "\n",
    "spark.conf.set(sasEntity, sasToken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74020868-ad23-4ad5-bf43-3036c4ac8ab2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Create the DataFrame. This is the same one we created in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d7134f2-c0c5-43b0-a9f8-c29000e133d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parquetDir = source + \"/wikipedia/pagecounts/staging_parquet_en_only_clean/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cb4fe6a-d412-4400-9603-f69a7ad089b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pagecountsEnAllDF = (spark  # Our SparkSession & Entry Point\n",
    "  .read                     # Our DataFrameReader\n",
    "  .parquet(parquetDir)      # Returns an instance of DataFrame\n",
    ")\n",
    "print(pagecountsEnAllDF)    # Python hack to see the data type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1859d34f-2ba6-462e-ab0a-e684ca4e98fb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Execute a count on the DataFrame as we did at the end of the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de0a0822-ce3d-4bfe-a01d-2f8646a5a5a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "total = pagecountsEnAllDF.count()\n",
    "\n",
    "print(\"Record Count: {0:,}\".format( total ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f53f346-b98b-4c1a-a949-1da639770d9a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "That tells us that there are around 2 million rows in the `DataFrame`. \n",
    "\n",
    "Before we take a closer look at the contents of the `DataFrame`, let us introduce a technique that speeds up processing.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "228f11de-d5d0-4a08-aeb4-f0e4d54bff82",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) cache() & persist()\n",
    "\n",
    "The ability to cache data is one technique for achieving better performance with Apache Spark. \n",
    "\n",
    "This is because every action requires Spark to read the data from its source (Azure Blob, Amazon S3, HDFS, etc.) but caching moves that data into the memory of the local executor for \"instant\" access.\n",
    "\n",
    "`cache()` is just an alias for `persist()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f656498-04aa-4614-8a90-db9b42745f09",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(pagecountsEnAllDF\n",
    "  .cache()         # Mark the DataFrame as cached\n",
    "  .count()         # Materialize the cache\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df94821c-69c9-45bd-b1d6-d71e777e4144",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "If you re-run that command, it should take significantly less time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24bc3170-8666-4ecd-83df-624261883db4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pagecountsEnAllDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a91bccf-b063-4cb4-945f-f9269287b84d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Performance considerations of Caching Data\n",
    "\n",
    "When Caching Data you are placing it on the workers of the cluster. \n",
    "\n",
    "Caching takes resources, before moving a notebook into production please check and verify that you are appropriately using cache. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "beb7c66f-8a54-4cd7-8f53-4a4845b1e065",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "And as a quick side note, you can remove a cache by calling the `DataFrame`'s `unpersist()` method but, it is not necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0748fb3-645a-4f9b-8c68-8ada4f11883c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Our Data\n",
    "\n",
    "Let's continue by taking a look at the type of data we have. \n",
    "\n",
    "We can do this with the `printSchema()` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cfda131-cf28-4180-82a4-474508052304",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pagecountsEnAllDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66a58d01-c361-4a60-974d-16655f6f50b0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We should now be able to see that we have four columns of data:\n",
    "* **project** (*string*): The name of the Wikipedia project. This will include values such as:\n",
    "  * **en**: The English version of Wikipedia.\n",
    "  * **fr**: The French version of Wikipedia.\n",
    "  * **en.d**: The English version of Wiktionary.\n",
    "  * **fr.b**: The French version of Wikibooks.\n",
    "  * **de.n**: The German version of Wikinews.\n",
    "* **article** (*string*): The name of the article in the corresponding project. This will include values such as:\n",
    "  * <a href=\"https://en.wikipedia.org/wiki/Apache_Spark\" target=\"_blank\">Apache_Spark</a>\n",
    "  * <a href=\"https://en.wikipedia.org/wiki/Matei_Zaharia\" target=\"_blank\">Matei_Zaharia</a>\n",
    "  * <a href=\"https://en.wikipedia.org/wiki/Kevin_Bacon\" target=\"_blank\">Kevin_Bacon</a>\n",
    "* **requests** (*integer*): The number of requests (clicks) the article has received in the hour this data represents.\n",
    "* **bytes_served** (*long*): The total number of bytes delivered for the requested article.\n",
    "  * **Note:** In our copy of the data, this value is zero for all records and consequently is of no value to us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b5a957e-bed6-493c-89c4-1dccd5792942",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Spark API\n",
    "\n",
    "You have already seen one command available to the `DataFrame` class, namely `DataFrame.printSchema()`\n",
    "  \n",
    "Let's take a look at the API to see what other operations we have available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f03dd17-ee8a-495b-b1f5-442c22c8fd08",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **Spark API Home Page**\n",
    "0. Open a new browser tab\n",
    "0. Google for **Spark API Latest** or **Spark API _x.x.x_** for a specific version.\n",
    "0. Select **Spark API Documentation - Spark _x.x.x_ Documentation - Apache Spark** \n",
    "\n",
    "Other Documentation:\n",
    "* Programming Guides for DataFrames, SQL, Graphs, Machine Learning, Streaming...\n",
    "* Deployment Guides for Spark Standalone, Mesos, Yarn...\n",
    "* Configuration, Monitoring, Tuning, Security...\n",
    "\n",
    "Here are some shortcuts\n",
    "  * <a href=\"https://spark.apache.org/docs/latest/\" target=\"_blank\">Spark API Documentation - Latest</a>\n",
    "  * <a href=\"https://spark.apache.org/docs/2.1.1/api.html\" target=\"_blank\">Spark API Documentation - 2.1.1</a>\n",
    "  * <a href=\"https://spark.apache.org/docs/2.1.0/api.html\" target=\"_blank\">Spark API Documentation - 2.1.0</a>\n",
    "  * <a href=\"https://spark.apache.org/docs/2.0.2/api.html\" target=\"_blank\">Spark API Documentation - 2.0.2</a>\n",
    "  * <a href=\"https://spark.apache.org/docs/1.6.3/api.html\" target=\"_blank\">Spark API Documentation - 1.6.3</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fe76860-877c-4435-9a29-4c50ee28c8bc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Naturally, which set of documentation you will use depends on which language you will use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23b02c3c-3065-4e83-8f41-3f30006fd875",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Spark API (Python)\n",
    "\n",
    "0. Select **Spark Python API (Sphinx)**.\n",
    "0. Look up the documentation for `pyspark.sql.DataFrame`.\n",
    "  0. In the lower-left-hand-corner type **DataFrame** into the search field.\n",
    "  0. Hit **[Enter]**.\n",
    "  0. The search results should appear in the right-hand pane.\n",
    "  0. Click on **pyspark.sql.DataFrame (Python class, in pyspark.sql module)**\n",
    "  0. The documentation should open in the right-hand pane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a1901d3-660c-47c8-8147-bb3a42d99b27",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Spark API (Scala)\n",
    "\n",
    "0. Select **Spark Scala API (Scaladoc)**.\n",
    "0. Look up the documentation for `org.apache.spark.sql.DataFrame`.\n",
    "  0. In the upper-left-hand-corner type **DataFrame** into the search field.\n",
    "  0. The search will execute automatically.\n",
    "  0. In the class/package list, click on **DataFrame**.\n",
    "  0. The documentation should open in the right-hand pane.\n",
    "  \n",
    "This isn't going to work, but why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3c2a5da-e68c-47af-a953-ade302c94acf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Spark API (Scala), Try #2\n",
    "\n",
    "Look up the documentation for `org.apache.spark.sql.Dataset`.\n",
    "  0. In the upper-left-hand-corner type **Dataset** into the search field.\n",
    "  0. The search will execute automatically.\n",
    "  0. In the class/package list, click on **Dataset**.\n",
    "  0. The documentation should open in the right-hand pane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "922edbde-186a-4791-b511-502aa9b5cf10",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now that we have found the proper documentation, we can take a quick peek at the function `printSchema()`.\n",
    "\n",
    "Nothing special here.\n",
    "\n",
    "If you look at the API docs, `printSchema(..)` is described like this:\n",
    "> Prints the schema to the console in a nice tree format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efee9892-e77e-4319-a3b9-8e6b3ebc79b6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Next steps\n",
    "\n",
    "Start the next lesson, [Use the Display function]($./3.Display-function)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "2.Use-common-dataframe-methods",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
