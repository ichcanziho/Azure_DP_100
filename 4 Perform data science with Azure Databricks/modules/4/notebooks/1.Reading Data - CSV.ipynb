{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d6af4be-fd18-4efc-8d6a-34fa4ac4e0ba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Reading Data - CSV Files\n",
    "\n",
    "**Technical Accomplishments:**\n",
    "- Start working with the API documentation\n",
    "- Introduce the class `SparkSession` and other entry points\n",
    "- Introduce the class `DataFrameReader`\n",
    "- Read data from:\n",
    "  * CSV without a Schema.\n",
    "  * CSV with a Schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f10c057-ba59-4aca-8d7d-318cf0f179d5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Getting Started\n",
    "\n",
    "Run the following cell to configure our \"classroom.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f474d12c-207b-49c8-b004-3eb3c59a9760",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "The execution of this command did not finish successfully",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run \"./Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6aeada66-3f01-40b0-a7bb-106c6ab79762",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "The execution of this command did not finish successfully",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run \"./Includes/Utility-Methods\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3279e9a-b700-4dcd-b72e-b09d93e53e51",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Entry Points\n",
    "\n",
    "Our entry point for Spark 2.0 applications is the class `SparkSession`.\n",
    "\n",
    "An instance of this object is already instantiated for us which can be easily demonstrated by running the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d557cd7b-c03d-4992-95a1-2779c5d98a88",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed3fccb7-7efd-46a6-984d-a95208efa0b6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "It's worth noting that in Spark 2.0 `SparkSession` is a replacement for the other entry points:\n",
    "* `SparkContext`, available in our notebook as **sc**.\n",
    "* `SQLContext`, or more specifically it's subclass `HiveContext`, available in our notebook as **sqlContext**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42b400e5-67ef-4846-854f-8dd774d378b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(sc)\n",
    "print(sqlContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffa4dfbb-7a99-410d-993e-c9cda9a78ac5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Before we can dig into the functionality of the `SparkSession` class, we need to know how to access the API documentation for Apache Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ab9f237-4f21-4a3c-92ea-86b7e47e47a1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Spark API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "020e7039-2106-40f5-85f6-b88709d88d3d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### **Spark API Home Page**\n",
    "0. Open a new browser tab.\n",
    "0. Search for **Spark API Latest** or **Spark API _x.x.x_** for a specific version.\n",
    "0. Select **Spark API Documentation - Spark _x.x.x_ Documentation - Apache Spark**. \n",
    "0. Which set of documentation you will use depends on which language you will use.\n",
    "\n",
    "Other Documentation:\n",
    "* Programming Guides for DataFrames, SQL, Graphs, Machine Learning, Streaming...\n",
    "* Deployment Guides for Spark Standalone, Mesos, Yarn...\n",
    "* Configuration, Monitoring, Tuning, Security...\n",
    "\n",
    "Here are some shortcuts\n",
    "  * <a href=\"https://spark.apache.org/docs/latest/api/\" target=\"_blank\">Spark API Documentation - Latest</a>\n",
    "  * <a href=\"https://spark.apache.org/docs/2.4.0/\" target=\"_blank\">Spark API Documentation - 2.4.0</a>\n",
    "  * <a href=\"https://spark.apache.org/docs/2.2.0/\" target=\"_blank\">Spark API Documentation - 2.2.0</a>\n",
    "  * <a href=\"https://spark.apache.org/docs/2.1.1/\" target=\"_blank\">Spark API Documentation - 2.1.1</a>\n",
    "  * <a href=\"https://spark.apache.org/docs/2.0.2/\" target=\"_blank\">Spark API Documentation - 2.0.2</a>\n",
    "  * <a href=\"https://spark.apache.org/docs/1.6.3/\" target=\"_blank\">Spark API Documentation - 1.6.3</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90d34181-ea67-463a-bb29-bf34156c1b60",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Spark API (Scala)\n",
    "\n",
    "0. Select **Spark Scala API (Scaladoc)**.\n",
    "0. Look up the documentation for `org.apache.spark.sql.SparkSession`.\n",
    "  0. In the upper-left-hand-corner type **SparkSession** into the search field.\n",
    "  0. The search will execute automatically.\n",
    "  0. In the class/package list, click on **SparkSession**.\n",
    "  0. The documentation should open in the right-hand pane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72b74fcc-2b4b-4ca9-92a3-31ad9bc9700a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Spark API (Python)\n",
    "\n",
    "0. Select **Spark Python API (Sphinx)**.\n",
    "0. Look up the documentation for `pyspark.sql.SparkSession`.\n",
    "  0. In the lower-left-hand-corner type **SparkSession** into the search field.\n",
    "  0. Hit **[Enter]**.\n",
    "  0. The search results should appear in the right-hand pane.\n",
    "  0. Click on **pyspark.sql.SparkSession (Python class, in pyspark.sql module)**\n",
    "  0. The documentation should open in the right-hand pane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8689ccc9-b20c-43eb-a743-28bdfdd88bb5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) SparkSession\n",
    "\n",
    "Quick function review:\n",
    "* `createDataSet(..)`\n",
    "* `createDataFrame(..)`\n",
    "* `emptyDataSet(..)`\n",
    "* `emptyDataFrame(..)`\n",
    "* `range(..)`\n",
    "* `read(..)`\n",
    "* `readStream(..)`\n",
    "* `sparkContext(..)`\n",
    "* `sqlContext(..)`\n",
    "* `sql(..)`\n",
    "* `streams(..)`\n",
    "* `table(..)`\n",
    "* `udf(..)`\n",
    "\n",
    "The function we are most interested in is `SparkSession.read()` which returns a `DataFrameReader`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c1b720a-84a7-44f2-942d-1ddfad84d2ab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) DataFrameReader\n",
    "\n",
    "Look up the documentation for `DataFrameReader`.\n",
    "\n",
    "Quick function review:\n",
    "* `csv(path)`\n",
    "* `jdbc(url, table, ..., connectionProperties)`\n",
    "* `json(path)`\n",
    "* `format(source)`\n",
    "* `load(path)`\n",
    "* `orc(path)`\n",
    "* `parquet(path)`\n",
    "* `table(tableName)`\n",
    "* `text(path)`\n",
    "* `textFile(path)`\n",
    "\n",
    "Configuration methods:\n",
    "* `option(key, value)`\n",
    "* `options(map)`\n",
    "* `schema(schema)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a798101-9acb-45a7-9893-36230db8ef1f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Reading from CSV w/InferSchema\n",
    "\n",
    "We are going to start by reading in a very simple text file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17c38f5a-8a2c-44a0-bd4b-a895d6edbfc7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### The Data Source\n",
    "* For this exercise, we will be using a tab-separated file called **pageviews_by_second.tsv** (255 MB file from Wikipedia)\n",
    "* We can use **&percnt;fs ls ...** to view the file on the DBFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41438df1-4ef7-4516-a7a8-43423e33068c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs ls /mnt/training/wikipedia/pageviews/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30bd7bd2-413c-4ccc-8e10-ed3cd3421ffb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can use **&percnt;fs head ...** to peek at the first couple thousand characters of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04e32c34-7944-4177-9163-b814c5348935",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs head /mnt/training/wikipedia/pageviews/pageviews_by_second.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a6edacd-7d8f-4c03-9ff5-9f17d34492dc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "There are a couple of things to note here:\n",
    "* The file has a header.\n",
    "* The file is tab separated (we can infer that from the file extension and the lack of other characters between each \"column\").\n",
    "* The first two columns are strings and the third is a number.\n",
    "\n",
    "Knowing those details, we can read in the \"CSV\" file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92a9b557-4014-4d03-9599-b7943c48fb81",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Step #1 - Read The CSV File\n",
    "Let's start with the bare minimum by specifying the tab character as the delimiter and the location of the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "375f0f28-7d73-4306-a862-0e4fa6b31323",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# A reference to our tab-separated-file\n",
    "csvFile = \"/mnt/training/wikipedia/pageviews/pageviews_by_second.tsv\"\n",
    "\n",
    "tempDF = (spark.read           # The DataFrameReader\n",
    "   .option(\"sep\", \"\\t\")        # Use tab delimiter (default is comma-separator)\n",
    "   .csv(csvFile)               # Creates a DataFrame from CSV after reading in the file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7608aec-eae8-4223-9d72-f14843e601a0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This is guaranteed to <u>trigger one job</u>.\n",
    "\n",
    "A *Job* is triggered anytime we are \"physically\" __required to touch the data__.\n",
    "\n",
    "In some cases, __one action may create multiple jobs__ (multiple reasons to touch the data).\n",
    "\n",
    "In this case, the reader has to __\"peek\" at the first line__ of the file to determine how many columns of data we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81f8b6e7-a446-4d89-a508-4387761ebc37",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "We can see the structure of the `DataFrame` by executing the command `printSchema()`\n",
    "\n",
    "It prints to the console the name of each column, its data type and if it's null or not.\n",
    "\n",
    "** *Note:* ** *We will be covering the other `DataFrame` functions in other notebooks.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9c7e04a-94b9-427d-9a4b-a7b537ee471f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tempDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf62db21-8319-4909-bf1b-c1097738cd6d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can see from the schema that...\n",
    "* there are three columns\n",
    "* the column names **_c0**, **_c1**, and **_c2** (automatically generated names)\n",
    "* all three columns are **strings**\n",
    "* all three columns are **nullable**\n",
    "\n",
    "And if we take a quick peek at the data, we can see that line #1 contains the headers and not data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4140a1e1-a598-46f3-9f8b-0656803171c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(tempDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5aaf1087-504e-4ac4-af38-23c61bf5cdea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Step #2 - Use the File's Header\n",
    "Next, we can add an option that tells the reader that the data contains a header and to use that header to determine our column names.\n",
    "\n",
    "** *NOTE:* ** *We know we have a header based on what we can see in \"head\" of the file from earlier.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f480ea6e-271a-450d-a55c-b6dd223952db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(spark.read                    # The DataFrameReader\n",
    "   .option(\"sep\", \"\\t\")        # Use tab delimiter (default is comma-separator)\n",
    "   .option(\"header\", \"true\")   # Use first line of all files as header\n",
    "   .csv(csvFile)               # Creates a DataFrame from CSV after reading in the file\n",
    "   .printSchema()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd50aa1b-4599-44d6-9aae-dad547af5a99",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "A couple of notes about this iteration:\n",
    "* again, only one job\n",
    "* there are three columns\n",
    "* all three columns are **strings**\n",
    "* all three columns are **nullable**\n",
    "* the column names are specified: **timestamp**, **site**, and **requests** (the change we were looking for)\n",
    "\n",
    "A \"peek\" at the first line of the file is all that the reader needs to determine the number of columns and the name of each column.\n",
    "\n",
    "Before going on, make a note of the duration of the previous call - it should be just under 3 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22dbd13b-a420-406b-8ae7-0814e066d4b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Step #3 - Infer the Schema\n",
    "\n",
    "Lastly, we can add an option that tells the reader to infer each column's data type (aka the schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e55b1650-893e-44f0-85a1-aa3b6f6b8a1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(spark.read                        # The DataFrameReader\n",
    "   .option(\"header\", \"true\")       # Use first line of all files as header\n",
    "   .option(\"sep\", \"\\t\")            # Use tab delimiter (default is comma-separator)\n",
    "   .option(\"inferSchema\", \"true\")  # Automatically infer data types\n",
    "   .csv(csvFile)                   # Creates a DataFrame from CSV after reading in the file\n",
    "   .printSchema()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2be62807-4bbe-4144-b7eb-bcf5ba90b3dc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Review: Reading CSV w/InferSchema\n",
    "* we still have three columns\n",
    "* all three columns are still **nullable**\n",
    "* all three columns have their proper names\n",
    "* two jobs were executed (not one as in the previous example)\n",
    "* our three columns now have distinct data types:\n",
    "  * **timestamp** == **timestamp**\n",
    "  * **site** == **string**\n",
    "  * **requests** == **integer**\n",
    "\n",
    "**Question:** Why were there two jobs?\n",
    "\n",
    "**Question:** How long did the last job take?\n",
    "\n",
    "**Question:** Why did it take so much longer?\n",
    "\n",
    "Discuss..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6037d958-e0d3-4cf8-9bf5-ee89d326c59d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Reading from CSV w/User-Defined Schema\n",
    "\n",
    "This time we are going to read the same file.\n",
    "\n",
    "The difference here is that we are going to define the schema beforehand and hopefully avoid the execution of any extra jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adef95b1-5477-4af7-955d-b9217f7265c7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Step #1\n",
    "Declare the schema.\n",
    "\n",
    "This is just a list of field names and data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74eb0217-77e3-4de4-a514-838e7f792ded",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Required for StructField, StringType, IntegerType, etc.\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "csvSchema = StructType([\n",
    "  StructField(\"timestamp\", StringType(), False),\n",
    "  StructField(\"site\", StringType(), False),\n",
    "  StructField(\"requests\", IntegerType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21dced93-84f0-4366-82f6-d01007b89b65",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Step #2\n",
    "Read in our data (and print the schema).\n",
    "\n",
    "We can specify the schema, or rather the `StructType`, with the `schema(..)` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3485d832-0c59-48b7-9baa-dff7cb710dfe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(spark.read                   # The DataFrameReader\n",
    "  .option('header', 'true')   # Ignore line #1 - it's a header\n",
    "  .option('sep', \"\\t\")        # Use tab delimiter (default is comma-separator)\n",
    "  .schema(csvSchema)          # Use the specified schema\n",
    "  .csv(csvFile)               # Creates a DataFrame from CSV after reading in the file\n",
    "  .printSchema()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "498b2b38-5c09-464a-bb31-771b1399dba6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Review: Reading CSV w/ User-Defined Schema\n",
    "* We still have three columns\n",
    "* All three columns are **NOT** nullable because we declared them as such.\n",
    "* All three columns have their proper names\n",
    "* Zero jobs were executed\n",
    "* Our three columns now have distinct data types:\n",
    "  * **timestamp** == **string**\n",
    "  * **site** == **string**\n",
    "  * **requests** == **integer**\n",
    "\n",
    "**Question:** Why were there no jobs?\n",
    "\n",
    "**Question:** What is different about the data types of these columns compared to the previous exercise & why?\n",
    "\n",
    "**Question:** Do I need to indicate that the file has a header?\n",
    "\n",
    "**Question:** Do the declared column names need to match the columns in the header of the TSV file?\n",
    "\n",
    "Discuss..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "052dc1bd-00c4-4745-8de3-fb5b07f94078",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "For a list of all the options related to reading CSV files, please see the documentation for `DataFrameReader.csv(..)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae91a822-ef8c-4c34-a238-ac9eb6eb7a83",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's take a look at some of the other details of the `DataFrame` we just created for comparison sake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0527eab-56ac-48e2-b1d4-4b06a9df7f7a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "csvDF = (spark.read\n",
    "  .option('header', 'true')\n",
    "  .option('sep', \"\\t\")\n",
    "  .schema(csvSchema)\n",
    "  .csv(csvFile)\n",
    ")\n",
    "print(\"Partitions: \" + str(csvDF.rdd.getNumPartitions()) )\n",
    "printRecordsPerPartition(csvDF)\n",
    "print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39815ea9-bfd4-4b33-8a3f-bc1d1066e1de",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Next steps\n",
    "\n",
    "Start the next lesson, [Reading Data - JSON]($./2.Reading%20Data%20-%20JSON)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 952756641965083,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "1.Reading Data - CSV",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
