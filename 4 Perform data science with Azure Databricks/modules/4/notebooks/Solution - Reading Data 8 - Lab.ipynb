{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7dca9818-6e34-437b-b345-4b88bb36194f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Reading Data Lab\n",
    "* The goal of this lab is to put into practice some of what you have learned about reading data with Apache Spark.\n",
    "* The instructions are provided below along with empty cells for you to do your work.\n",
    "* At the bottom of this notebook are additional cells that will help verify that your work is accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff5b5560-34d8-4b55-aab5-ee1c5c9defd2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Instructions\n",
    "0. Start with the file **dbfs:/mnt/training/wikipedia/clickstream/2015_02_clickstream.tsv**, some random file you haven't seen yet.\n",
    "0. Read in the data and assign it to a `DataFrame` named **testDF**.\n",
    "0. Run the last cell to verify that the data was loaded correctly and to print its schema.\n",
    "0. The one untestable requirement is that you should be able to create the `DataFrame` and print its schema **without** executing a single job.\n",
    "\n",
    "**Note:** For the test to pass, the following columns should have the specified data types:\n",
    " * **prev_id**: integer\n",
    " * **curr_id**: integer\n",
    " * **n**: integer\n",
    " * **prev_title**: string\n",
    " * **curr_title**: string\n",
    " * **type**: string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10d43bcb-4489-4e3e-bf50-bb7c00fd9e7e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Getting Started\n",
    "\n",
    "Run the following cell to configure our \"classroom.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "621eb7ae-9f4b-495b-9b0f-acf4b79a405e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run \"../Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "571efbd8-acac-44e1-b3fd-d600767b606a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Show Your Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9981d6e-5d2f-40f2-8283-46e381f126a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ANSWER\n",
    "\n",
    "# The students will actually need to do this in two steps.\n",
    "fileName = \"dbfs:/mnt/training/wikipedia/clickstream/2015_02_clickstream.tsv\"\n",
    "\n",
    "# The first step will be to use inferSchema = true \n",
    "# It's the only way to figure out what the column and data types are\n",
    "(spark.read\n",
    "  .option(\"sep\", \"\\t\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .csv(fileName)\n",
    "  .printSchema()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22fcb248-275c-4e16-8763-b2fcc685c547",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ANSWER\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# The second step is to create the schema\n",
    "schema = StructType([\n",
    "    StructField(\"prev_id\", IntegerType(), False),\n",
    "    StructField(\"curr_id\", IntegerType(), False),\n",
    "    StructField(\"n\", IntegerType(), False),\n",
    "    StructField(\"prev_title\", StringType(), False),\n",
    "    StructField(\"curr_title\", StringType(), False),\n",
    "    StructField(\"type\", StringType(), False)\n",
    "])\n",
    "\n",
    "fileName = \"dbfs:/mnt/training/wikipedia/clickstream/2015_02_clickstream.tsv\"\n",
    "\n",
    "#The third step is to read the data in with the user-defined schema\n",
    "testDF = (spark.read\n",
    "  .option(\"sep\", \"\\t\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .schema(schema)\n",
    "  .csv(fileName)\n",
    ")\n",
    "\n",
    "testDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8d303fe-0f82-471f-a26b-c486c94b41fc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Verify Your Work\n",
    "Run the following cell to verify that your `DataFrame` was created properly.\n",
    "\n",
    "**Remember:** This should execute without triggering a single job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eed24e15-4845-4c04-b06b-528e12a484e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "testDF.printSchema()\n",
    "\n",
    "columns = testDF.dtypes\n",
    "assert len(columns) == 6, \"Expected 6 columns but found \" + str(len(columns))\n",
    "\n",
    "assert columns[0][0] == \"prev_id\",    \"Expected column 0 to be \\\"prev_id\\\" but found \\\"\" + columns[0][0] + \"\\\".\"\n",
    "assert columns[0][1] == \"int\",        \"Expected column 0 to be of type \\\"int\\\" but found \\\"\" + columns[0][1] + \"\\\".\"\n",
    "\n",
    "assert columns[1][0] == \"curr_id\",    \"Expected column 1 to be \\\"curr_id\\\" but found \\\"\" + columns[1][0] + \"\\\".\"\n",
    "assert columns[1][1] == \"int\",        \"Expected column 1 to be of type \\\"int\\\" but found \\\"\" + columns[1][1] + \"\\\".\"\n",
    "\n",
    "assert columns[2][0] == \"n\",          \"Expected column 2 to be \\\"n\\\" but found \\\"\" + columns[2][0] + \"\\\".\"\n",
    "assert columns[2][1] == \"int\",        \"Expected column 2 to be of type \\\"int\\\" but found \\\"\" + columns[2][1] + \"\\\".\"\n",
    "\n",
    "assert columns[3][0] == \"prev_title\", \"Expected column 3 to be \\\"prev_title\\\" but found \\\"\" + columns[3][0] + \"\\\".\"\n",
    "assert columns[3][1] == \"string\",     \"Expected column 3 to be of type \\\"string\\\" but found \\\"\" + columns[3][1] + \"\\\".\"\n",
    "\n",
    "assert columns[4][0] == \"curr_title\", \"Expected column 4 to be \\\"curr_title\\\" but found \\\"\" + columns[4][0] + \"\\\".\"\n",
    "assert columns[4][1] == \"string\",     \"Expected column 4 to be of type \\\"string\\\" but found \\\"\" + columns[4][1] + \"\\\".\"\n",
    "\n",
    "assert columns[5][0] == \"type\",       \"Expected column 5 to be \\\"type\\\" but found \\\"\" + columns[5][0] + \"\\\".\"\n",
    "assert columns[5][1] == \"string\",     \"Expected column 5 to be of type \\\"string\\\" but found \\\"\" + columns[5][1] + \"\\\".\"\n",
    "\n",
    "print(\"Congratulations, all tests passed... that is if no jobs were triggered :-)\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Solution - Reading Data 8 - Lab",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
