{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "daa7757f-a6c0-434b-9065-1d92069e5020",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# MLflow\n",
    "\n",
    "[MLflow](https://mlflow.org/docs/latest/concepts.html) seeks to address these three core issues:\n",
    "\n",
    "* It’s difficult to keep track of experiments\n",
    "* It’s difficult to reproduce code\n",
    "* There’s no standard way to package and deploy models\n",
    "\n",
    "In the past, when examining a problem, you would have to manually keep track of the many models you created, as well as their associated parameters and metrics. This can quickly become tedious and take up valuable time, which is where MLflow comes in.\n",
    "\n",
    "## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lesson you:<br>\n",
    "* Use MLflow to track experiments, log metrics, and compare runs\n",
    "\n",
    "**Required Libraries**: \n",
    "* `mlflow==1.7.0` via PyPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46ad9f57-7856-4c39-8216-2dbe665594dc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<div><img src=\"https://files.training.databricks.com/images/eLearning/ML-Part-4/mlflow-tracking.png\" style=\"height: 400px; margin: 20px\"/></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b552035b-6670-4dce-8280-9e6318d6dbd4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69185f77-983d-4114-acf2-529b2060f7c8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's start by loading SF Airbnb Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cbe240b-231a-4f04-b854-61ebaf567cf9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "filePath = \"dbfs:/mnt/training/airbnb/sf-listings/sf-listings-2019-03-06-clean.parquet/\"\n",
    "airbnbDF = spark.read.parquet(filePath)\n",
    "\n",
    "(trainDF, testDF) = airbnbDF.randomSplit([.8, .2], seed=42)\n",
    "print(trainDF.cache().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ade0b6e0-2ce9-44e2-9276-105bcf5881e7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### MLflow Tracking\n",
    "\n",
    "MLflow Tracking is a logging API specific for machine learning and agnostic to libraries and environments that do the training.  It is organized around the concept of **runs**, which are executions of data science code.  Runs are aggregated into **experiments** where many runs can be a part of a given experiment and an MLflow server can host many experiments.\n",
    "\n",
    "\n",
    "MLflow tracking also serves as a **model registry** so tracked models can easily be stored and, as necessary, deployed into production. This also standardizes this process, which significantly accelerates it and allows for scalability. Experiments can be tracked using libraries in Python, R, and Java as well as by using the CLI and REST calls.  This module will use Python, though the majority of MLflow functionality is also exposed in these other APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6147f125-1fa1-4928-b9f9-378d01f107de",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Track Runs\n",
    "\n",
    "Each run can record the following information:<br><br>\n",
    "\n",
    "- **Parameters:** Key-value pairs of input parameters such as the number of trees in a random forest model\n",
    "- **Metrics:** Evaluation metrics such as RMSE or Area Under the ROC Curve\n",
    "- **Artifacts:** Arbitrary output files in any format.  This can include images, pickled models, and data files\n",
    "- **Source:** The code that originally ran the experiment\n",
    "\n",
    "**NOTE**: MLflow can only log PipelineModels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d4c88e7-b9be-448e-b389-e5d0058ffdad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "mlflow.set_experiment(f\"/Users/{username}/tr-mlflow\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"LR-Single-Feature\") as run:\n",
    "  # Define pipeline\n",
    "  vecAssembler = VectorAssembler(inputCols=[\"bedrooms\"], outputCol=\"features\")\n",
    "  lr = LinearRegression(featuresCol=\"features\", labelCol=\"price\")\n",
    "  pipeline = Pipeline(stages=[vecAssembler, lr])\n",
    "  pipelineModel = pipeline.fit(trainDF)\n",
    "  \n",
    "  # Log parameters\n",
    "  mlflow.log_param(\"label\", \"price-bedrooms\")\n",
    "  \n",
    "  # Log model\n",
    "  mlflow.spark.log_model(pipelineModel, \"model\")\n",
    "  \n",
    "  # Evaluate predictions\n",
    "  predDF = pipelineModel.transform(testDF)\n",
    "  regressionEvaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"price\", metricName=\"rmse\")\n",
    "  rmse = regressionEvaluator.evaluate(predDF)\n",
    "  \n",
    "  # Log metrics\n",
    "  mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "# display_run_uri(run.info.experiment_id, run.info.run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0792342-0e3f-4c41-bb5f-24dbf29af0a2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Next let's build our linear regression model but use all of our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55085005-7a05-4486-80fe-1d10c2481f79",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.ml.feature import RFormula\n",
    "with mlflow.start_run(run_name=\"LR-All-Features\") as run:\n",
    "  # Create pipeline\n",
    "  rFormula = RFormula(formula=\"price ~ .\", featuresCol=\"features\", labelCol=\"price\", handleInvalid=\"skip\")\n",
    "  lr = LinearRegression(labelCol=\"price\", featuresCol=\"features\")\n",
    "  pipeline = Pipeline(stages = [rFormula, lr])\n",
    "  pipelineModel = pipeline.fit(trainDF)\n",
    "  \n",
    "  # Log pipeline\n",
    "  mlflow.spark.log_model(pipelineModel, \"model\")\n",
    "  \n",
    "  # Log parameter\n",
    "  mlflow.log_param(\"label\", \"price-all-features\")\n",
    "  \n",
    "  # Create predictions and metrics\n",
    "  predDF = pipelineModel.transform(testDF)\n",
    "  regressionEvaluator = RegressionEvaluator(labelCol=\"price\", predictionCol=\"prediction\")\n",
    "  rmse = regressionEvaluator.setMetricName(\"rmse\").evaluate(predDF)\n",
    "  r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(predDF)\n",
    "  \n",
    "  # Log both metrics\n",
    "  mlflow.log_metric(\"rmse\", rmse)\n",
    "  mlflow.log_metric(\"r2\", r2)\n",
    "\n",
    "# display_run_uri(run.info.experiment_id, run.info.run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13c41ad7-5678-4fc8-bba7-4e111315372a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Finally, we will use Linear Regression to predict the log of the price, due to its log normal distribution.\n",
    "\n",
    "We'll also practice logging artifacts to keep a visual of our log normal histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1848b143-f7ac-4188-a082-f8b7dacdc9b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.ml.feature import RFormula\n",
    "from pyspark.sql.functions import col, log, exp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with mlflow.start_run(run_name=\"LR-Log-Price\") as run:\n",
    "  # Take log of price\n",
    "  logTrainDF = trainDF.withColumn(\"log_price\", log(col(\"price\")))\n",
    "  logTestDF = testDF.withColumn(\"log_price\", log(col(\"price\")))\n",
    "  \n",
    "  # Log parameter\n",
    "  mlflow.log_param(\"label\", \"log-price\")\n",
    "  \n",
    "  # Create pipeline\n",
    "  rFormula = RFormula(formula=\"log_price ~ . - price\", featuresCol=\"features\", labelCol=\"log_price\", handleInvalid=\"skip\")  \n",
    "  lr = LinearRegression(labelCol=\"log_price\", predictionCol=\"log_prediction\")\n",
    "  pipeline = Pipeline(stages = [rFormula, lr])\n",
    "  pipelineModel = pipeline.fit(logTrainDF)\n",
    "  \n",
    "  # Log model\n",
    "  mlflow.spark.log_model(pipelineModel, \"log-model\")\n",
    "  \n",
    "  # Make predictions\n",
    "  predDF = pipelineModel.transform(logTestDF)\n",
    "  expDF = predDF.withColumn(\"prediction\", exp(col(\"log_prediction\")))\n",
    "  \n",
    "  # Evaluate predictions\n",
    "  rmse = regressionEvaluator.setMetricName(\"rmse\").evaluate(expDF)\n",
    "  r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(expDF)\n",
    "  \n",
    "  # Log metrics\n",
    "  mlflow.log_metric(\"rmse\", rmse)\n",
    "  mlflow.log_metric(\"r2\", r2)\n",
    "  \n",
    "  # Log artifact\n",
    "  plt.clf()\n",
    "  logTrainDF.toPandas().hist(column=\"log_price\", bins=100)\n",
    "  figPath = username + \"logNormal.png\" \n",
    "  plt.savefig(figPath)\n",
    "  mlflow.log_artifact(figPath)\n",
    "  display(plt.show())\n",
    "  \n",
    "# display_run_uri(run.info.experiment_id, run.info.run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5012410-3047-428a-bbc2-c1385043d798",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "That's it! Now, let's use MLflow to easily look over our work and compare model performance. You can either query past runs programmatically or use the MLflow UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a688fc82-1795-4fe0-951d-1131ce46c2be",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Querying Past Runs\n",
    "\n",
    "You can query past runs programatically in order to use this data back in Python.  The pathway to doing this is an `MlflowClient` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a0a4349-23e3-4ce4-bf55-f198102837a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83c62246-22a0-4779-a845-438ba56e5f71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "client.list_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8fd3c55-502e-4230-916c-f0c2aaa93374",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "You can also use [search_runs](https://mlflow.org/docs/latest/search-syntax.html) to find all runs for a given experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cbcbbf2-45c4-4431-8403-697718fe45bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "experiment_id = run.info.experiment_id\n",
    "runs_df = mlflow.search_runs(experiment_id)\n",
    "\n",
    "display(runs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75ec5aa3-dff3-447a-94b2-c5914aa38cbe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Pull the last run and look at metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "113a16dc-6595-4ba1-a4e0-8155b7d0f2ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "runs = client.search_runs(experiment_id, order_by=[\"attributes.start_time desc\"], max_results=1)\n",
    "runs[0].data.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdcf0e49-b3ce-4290-9856-f19d1f9ddaec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "run_id = runs[0].info.run_id\n",
    "# display_run_uri(run.info.experiment_id, run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a46b4bfa-4109-43c9-920f-51d118987b9a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Examine the results in the UI.  Look for the following:<br><br>\n",
    "\n",
    "1. The `Experiment ID`\n",
    "2. The artifact location.  This is where the artifacts are stored in DBFS.\n",
    "3. The time the run was executed.  **Click this to see more information on the run.**\n",
    "4. The code that executed the run.\n",
    "\n",
    "\n",
    "After clicking on the time of the run, take a look at the following:<br><br>\n",
    "\n",
    "1. The Run ID will match what we printed above\n",
    "2. The model that we saved, included a pickled version of the model as well as the Conda environment and the `MLmodel` file.\n",
    "\n",
    "Note that you can add notes under the \"Notes\" tab to help keep track of important information about your models. \n",
    "\n",
    "Also, click on the run for the log normal distribution and see that the histogram is saved in \"Artifacts\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04955131-6929-41a3-a759-9c15a478dc4e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Load Saved Model\n",
    "\n",
    "Let's practice [loading](https://www.mlflow.org/docs/latest/python_api/mlflow.spark.html) our logged log-normal model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d06bd840-cdf9-4f95-ba2e-b6e82281d68c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "loaded_model = mlflow.spark.load_model(f\"runs:/{run.info.run_uuid}/log-model\")\n",
    "display(loaded_model.transform(testDF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1e4dce1-ee39-4cc4-b427-ebc9436a7125",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Log Param, Metrics, and Artifacts\n",
    "\n",
    "Now it's your turn! Log your name, your height, and a fun [matplotlib visualization](https://matplotlib.org/3.1.0/gallery/lines_bars_and_markers/scatter_with_legend.html#sphx-glr-gallery-lines-bars-and-markers-scatter-with-legend-py) (by calling the `generate_plot` function below - feel free to modify the viz!) under a run with name `MLflow-Lab` in our new MLflow experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2685ea67-b37c-4e42-9108-530f9a122b72",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "def generate_plot():\n",
    "  import numpy as np\n",
    "  np.random.seed(19680801)\n",
    "  import matplotlib.pyplot as plt\n",
    "\n",
    "  fig, ax = plt.subplots()\n",
    "  for color in ['tab:blue', 'tab:orange', 'tab:green']:\n",
    "      n = 750\n",
    "      x, y = np.random.rand(2, n)\n",
    "      scale = 200.0 * np.random.rand(n)\n",
    "      ax.scatter(x, y, c=color, s=scale, label=color,\n",
    "                 alpha=0.3, edgecolors='none')\n",
    "\n",
    "  ax.legend()\n",
    "  ax.grid(True)\n",
    "#   display(plt.show())\n",
    "  return fig, plt\n",
    "\n",
    "generate_plot()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "1. MLflow",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
