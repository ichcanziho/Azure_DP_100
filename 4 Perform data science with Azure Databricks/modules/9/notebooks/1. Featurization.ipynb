{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98c592da-5835-4a15-bc2c-72658e74ff0d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Featurization\n",
    "\n",
    "Cleaning data and adding features creates the inputs for machine learning models, which are only as strong as the data they are fed.  This lesson examines the process of featurization including common tasks such as handling categorical features and normalization, imputing missing data, and creating a pipeline of featurization steps.\n",
    "\n",
    "## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lesson you:<br>\n",
    "* Differentiate Spark transformers, estimators, and pipelines\n",
    "* One-hot encode categorical features\n",
    "* Impute missing data\n",
    "* Combine different featurization stages into a pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2aea2077-d133-4901-86ae-94c24ad2c3b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<iframe  \n",
    "src=\"//fast.wistia.net/embed/iframe/9j0djq95kk?videoFoam=true\"\n",
    "style=\"border:1px solid #1cb1c2;\"\n",
    "allowtransparency=\"true\" scrolling=\"no\" class=\"wistia_embed\"\n",
    "name=\"wistia_embed\" allowfullscreen mozallowfullscreen webkitallowfullscreen\n",
    "oallowfullscreen msallowfullscreen width=\"640\" height=\"360\" ></iframe>\n",
    "<div>\n",
    "<a target=\"_blank\" href=\"https://fast.wistia.net/embed/iframe/9j0djq95kk?seo=false\">\n",
    "  <img alt=\"Opens in new tab\" src=\"https://files.training.databricks.com/static/images/external-link-icon-16x16.png\"/>&nbsp;Watch full-screen.</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f06bf47-eb7a-4611-9331-1c6552b3c132",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Transformers, Estimators, and Pipelines\n",
    "\n",
    "Spark's machine learning library, `MLlib`, has three main abstractions:<br><br>\n",
    "\n",
    "1. A **transformer** takes a DataFrame as an input and returns a new DataFrame with one or more columns appended to it.  \n",
    "  - Transformers implement a `.transform()` method.  \n",
    "2. An **estimator** takes a DataFrame as an input and returns a model, which itself is a transformer.\n",
    "  - Estimators implements a `.fit()` method.\n",
    "3. A **pipeline** combines together transformers and estimators to make it easier to combine multiple algorithms.\n",
    "  - Pipelines implement a `.fit()` method.\n",
    "\n",
    "These basic building blocks form the machine learning process in Spark from featurization through model training and deployment.  \n",
    "\n",
    "Machine learning models are only as strong as the data they see and can only work on numerical data.  **Featurization is the process of creating this input data for a model.**  There are a number of common featurization approaches:<br><br>\n",
    "\n",
    "* Encoding categorical variables\n",
    "* Normalizing\n",
    "* Creating new features\n",
    "* Handling missing values\n",
    "* Binning/discretizing\n",
    "\n",
    "This lesson builds a pipeline of transformers and estimators in order to featurize a dataset.\n",
    "\n",
    "<div><img src=\"https://files.training.databricks.com/images/eLearning/ML-Part-1/pipeline.jpg\" style=\"height: 400px; margin: 20px\"/></div>\n",
    "\n",
    "<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> `MLlib` can refer to both the general machine learning library in Spark or the RDD-specific API.  `SparkML` refers to the DataFrame-specific API, which is preferred over working on RDD's wherever possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50b0507d-8559-48b0-a3f1-50a72c089bf4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Run the following cell to set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a420be0-3e87-497b-b4b2-de600817e80f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run \"./Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2c43b6b-95b9-4a4a-b66c-5ed432f31888",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Categorical Features and One-Hot Encoding\n",
    "\n",
    "Categorical features refer to a discrete number of groups.  In the case of the AirBnB dataset we'll use in this lesson, one categorical variable is room type.  There are three types of rooms: `Private room`, `Entire home/apt`, and `Shared room`.\n",
    "\n",
    "A machine learning model does not know how to handle these room types.  Instead, we must first *encode* each unique string into a number.  Second, we must *one-hot encode* each of those values to a location in an array.  This allows our machine learning algorithms to model effects of each category.\n",
    "\n",
    "| Room type       | Room type index | One-hot encoded room type index |\n",
    "|-----------------|-----------------|---------------------------------|\n",
    "| Private room    | 0               | [1, 0 ]                         |\n",
    "| Entire home/apt | 1               | [0, 1]                          |\n",
    "| Shared room     | 2               | [0, 0]                          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96ced35e-236e-4203-ac2a-a546b07a7573",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Import the AirBnB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09f9816b-1335-485c-993a-2599a857f5e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "airbnbDF = spark.read.parquet(\"/mnt/training/airbnb/sf-listings/sf-listings-correct-types.parquet\")\n",
    "\n",
    "display(airbnbDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ff171fb-a411-4342-9457-3a9b087c5bc4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Take the unique values of `room_type` and index them to a numerical value.  Fit the `StringIndexer` estimator to the unique room types using the `.fit()` method and by passing in the data.\n",
    "\n",
    "The trained `StringIndexer` model then becomes a transformer.  Use it to transform the results using the `.transform()` method and by passing in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d344010-1bdd-443c-8ba4-65ebaa1cab78",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "uniqueTypesDF = airbnbDF.select(\"room_type\").distinct() # Use distinct values to demonstrate how StringIndexer works\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"room_type\", outputCol=\"room_type_index\") # Set input column and new output column\n",
    "indexerModel = indexer.fit(uniqueTypesDF)                                  # Fit the indexer to learn room type/index pairs\n",
    "indexedDF = indexerModel.transform(uniqueTypesDF)                          # Append a new column with the index\n",
    "\n",
    "display(indexedDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca460991-b950-4607-8653-ec0288923fd5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now each room has a unique numerical value assigned.  While we could pass the new `room_type_index` into a machine learning model, it would assume that `Shared room` is twice as much as `Entire home/apt`, which is not the case.  Instead, we need to change these values to a binary yes/no value if a listing is for a shared room, entire home, or private room.\n",
    "\n",
    "Do this by training and fitting the `OneHotEncoderEstimator`, which only operates on numerical values (this is why we needed to use `StringIndexer` first).\n",
    "\n",
    "<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Certain models, such as random forest, do not need one-hot encoding (and can actually be negatively affected by the process).  The models we'll explore in this course, however, do need this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04119c89-9bed-440d-bf1f-5dcbe26ad824",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=[\"room_type_index\"], outputCols=[\"encoded_room_type\"])\n",
    "encoderModel = encoder.fit(indexedDF)\n",
    "encodedDF = encoderModel.transform(indexedDF)\n",
    "\n",
    "display(encodedDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95d0f44b-7e5c-4641-8e61-622e134b2531",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The new column `encoded_room_type` is a vector.  The difference between a sparse and dense vector is whether Spark records all of the empty values.  In a sparse vector, like we see here, Spark saves space by only recording the places where the vector has a non-zero value.  The value of 0 in the first position indicates that it's a sparse vector.  The second value indicates the length of the vector.\n",
    "\n",
    "Here's how to read the mapping above:<br><br>\n",
    "\n",
    "* `Shared room` maps to the vector `[0, 0]`\n",
    "* `Entire home/apt` maps to the vector `[0, 1]`\n",
    "* `Private room` maps to the vector `[1, 0]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49076b43-764f-42f5-9f77-f97385bbcd61",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Imputing Null or Missing Data\n",
    "\n",
    "Null values refer to unknown or missing data as well as irrelevant responses. Strategies for dealing with this scenario include:<br><br>\n",
    "\n",
    "* **Dropping these records:** Works when you do not need to use the information for downstream workloads\n",
    "* **Adding a placeholder (e.g. `-1`):** Allows you to see missing data later on without violating a schema\n",
    "* **Basic imputing:** Allows you to have a \"best guess\" of what the data could have been, often by using the mean of non-missing data\n",
    "* **Advanced imputing:** Determines the \"best guess\" of what data should be using more advanced strategies such as clustering machine learning algorithms or oversampling techniques <a href=\"https://jair.org/index.php/jair/article/view/10302\" target=\"_blank\">such as SMOTE.</a>\n",
    "\n",
    "<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> Try to determine why a value is null.  This can provide information that can be helpful to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d3772fe-0678-4d64-9ba9-8a865e33b748",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Describe the dataset and take a look at the `count` values.  There's a fair amount of missing data in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1da1de37-3e4a-44be-a7ed-b84d402ffae2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(airbnbDF.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f925a94-aa50-4149-a7ee-64f1f9cbd1b4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Try dropping missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "033b1db0-6227-43cd-acc6-4cbf699387a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "countWithoutDropping = airbnbDF.count()\n",
    "countWithDropping = airbnbDF.na.drop(subset=[\"zipcode\", \"host_is_superhost\"]).count()\n",
    "\n",
    "print(\"Count without dropping nulls:\\t\", countWithoutDropping)\n",
    "print(\"Count with dropping nulls:\\t\", countWithDropping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5216f531-a594-4f3d-9cd7-3eff362865e7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Another common option for working with missing data is to impute the missing values with a best guess for their value.  Try imputing a list of columns with their median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b63aee39-84f3-4959-a769-e55831f8f527",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputeCols = [\n",
    "  \"host_total_listings_count\",\n",
    "  \"bathrooms\",\n",
    "  \"beds\", \n",
    "  \"review_scores_rating\",\n",
    "  \"review_scores_accuracy\",\n",
    "  \"review_scores_cleanliness\",\n",
    "  \"review_scores_checkin\",\n",
    "  \"review_scores_communication\",\n",
    "  \"review_scores_location\",\n",
    "  \"review_scores_value\"\n",
    "]\n",
    "\n",
    "imputer = Imputer(strategy=\"median\", inputCols=imputeCols, outputCols=imputeCols)\n",
    "imputerModel = imputer.fit(airbnbDF)\n",
    "imputedDF = imputerModel.transform(airbnbDF)\n",
    "\n",
    "display(imputedDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca5cff23-4ff6-4f56-8b59-5dad14950bf1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Creating a Pipeline\n",
    "\n",
    "Passing around estimator objects, trained estimators, and transformed dataframes quickly becomes cumbersome.  Spark uses the convention established by `scikit-learn` to combine each of these steps into a single pipeline.\n",
    "We can now combine all of these steps into a single pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d67dd6c-c4cc-49a0-bfd2-e9e955743b62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "  indexer, \n",
    "  encoder, \n",
    "  imputer\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "baec029c-3f78-4213-8e33-2cf363773357",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The pipeline is itself is now an estimator.  Train the model with its `.fit()` method and then transform the original dataset.  We've now combined all of our featurization steps into one pipeline with three stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfafd5a6-77df-4b7d-b1bf-611e169df79b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipelineModel = pipeline.fit(airbnbDF)\n",
    "transformedDF = pipelineModel.transform(airbnbDF)\n",
    "\n",
    "display(transformedDF)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "1. Featurization",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
