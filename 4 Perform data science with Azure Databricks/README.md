# Perform Data Science with Azure Databricks

## INDEX 0

- [1 Welcome to the Course](#1-welcome-to-the-course)
- [2 Describe Azure Databricks](#2-describe-azure-databricks)
- [3 Spark Architecture Fundamentals](#3-spark-architecture-fundamentals)
- [4 Use Azure Databricks to Prepare the Data for Advanced Analytics and Machine Learning Operations](#4-use-azure-databricks-to-prepare-the-data-for-advanced-analytics-and-machine-learning-operations)
- [5 Work with DataFrames in Azure Databricks](#5-work-with-dataframes-in-azure-databricks)
- [6 Build and Query a Delta Lake](#6-build-and-query-a-delta-lake)
- [7 Work with user-defined functions](#7-work-with-user-defined-functions)
- [8 Perform Machine Learning with Azure Databricks](#8-perform-machine-learning-with-azure-databricks)
- [9 Train a Machine Learning Model](#9-train-a-machine-learning-model)
- [10 Work with MLFlow in Azure Databricks](#10-work-with-mlflow-in-azure-databricks)
- [11 Perform Model Selection with Hyperparameter Tuning](#11-perform-model-selection-with-hyperparameter-tuning)
- [12 Deep Learning with Horovod for distributed training](#12-deep-learning-with-horovod-for-distributed-training)
- [13 Work with Azure Machine Learning to deploy serving models](#13-work-with-azure-machine-learning-to-deploy-serving-models)

# 1 Welcome to the Course

## INDEX 1:

- [1 Introduction to the course](#1-introduction-to-the-course)
- [1 Course syllabus](#1-course-syllabus)
- [1 How to be successful in this course](#1-how-to-be-successful-in-this-course)

[< Back to index](#index-0)

## 1 Introduction to the course
[< Back to index 1](#index-1)

![1.png](modules%2F1%2Fims%2F1%2F1.png)

Hola y bienvenidos a este curso, realiza ciencia de datos con Azure Databricks. En este curso aprenderás cómo realice 
ciencia de datos con Azure Dataricks. Este curso está compuesto de los siguientes módulos.

![2.png](modules%2F1%2Fims%2F1%2F2.png)

1. Introducción a Azure Databricks. 
2. Trabajar con datos en Azure Databricks. 
3. Procesamiento de datos en Azure Databricks. 
4. Comience a usar Azure Databricks y aprendizaje automático. 
5. Gestione los ciclos de vida del aprendizaje automático y modelos afinados. 
6. Entrena una red neuronal distribuida y sirva modelos con el aprendizaje automático de Azure.


![3.png](modules%2F1%2Fims%2F1%2F3.png)

Comencemos con el primer módulo. **Introducción a Azure Databricks**. En este módulo, describirá como tus Databricks 
y Apache Spark para procesar archivos de gran tamaño y describe los fundamentos de la arquitectura de Spark. 

![4.png](modules%2F1%2Fims%2F1%2F4.png)

En el siguiente módulo, **trabajarás con datos en Azure Databricks.** Use Azure Databricks para leer varios archivos 
tipos con y sin esquema. Combina entradas de archivos y almacenes de datos como la base de datos Azure CQL. Transforme 
y almacene esos datos para análisis avanzados. Procese datos en Azure Databricks definiendo marcos de datos para leer 
y procesar los datos. Y realice transformaciones de datos en marcos de datos y ejecuta acciones para mostrar los datos 
transformados hacia arriba.

![5.png](modules%2F1%2Fims%2F1%2F5.png)

A continuación, aprenderá cómo **procesar datos en Azure Databricks**. En este módulo aprenderás cómo crear y consulta 
un Delta Lake. 

![6.png](modules%2F1%2Fims%2F1%2F6.png)

A continuación, **comenzarás con Azure Databricks y el aprendizaje automático**. En este módulo entenderás los conceptos 
básicos del aprendizaje automático y el flujo de trabajo del aprendizaje automático. Este módulo cubrirá el rendimiento de la máquina aprender con Azure Databricks y aprender a capacitarse un modelo de aprendizaje automático.

![7.png](modules%2F1%2Fims%2F1%2F7.png)

A continuación, aprenderás a gestionar ciclos de vida de aprendizaje automático y modelos ajustados. En este módulo, 
aprenderás cómo trabajar con Mlflow en Azure Databricks y realizar una selección de modelos con hiperajuste de 
parámetros.

![8.png](modules%2F1%2Fims%2F1%2F8.png)

Esto incluirá cómo usar los módulos de Biblioteca de aprendizaje automático PySparks para ajustar hiperparámetros.

![9.png](modules%2F1%2Fims%2F1%2F9.png)

A continuación, aprenderás a entrenar una red neuronal distribuida y modelos de servicio con el aprendizaje automático 
de Azure. En este módulo entenderás en profundidad aprender con Haravad para una formación distribuida y cómo 
trabajar con Azure aprendizaje automático para implementar modelos de servicio. 

![10.png](modules%2F1%2Fims%2F1%2F10.png)

Además, a lo largo de este curso tendrá la oportunidad de crear y trabajar con Azure Databricks a través de ejercicios 
interactivos y ejemplos del mundo real de la ciencia de datos en acción, comprobaciones de conocimientos y exámenes de 
práctica. 

![11.png](modules%2F1%2Fims%2F1%2F11.png)

Por último, te prepararás para el programa Microsoft Certified Associate realicé la prueba realizando un examen de 
práctica. Le deseamos el mayor de los éxitos como usted comience este viaje de aprendizaje.

## 1 Course syllabus
[< Back to index 1](#index-1)

### Realice ciencia de datos con Azure Databricks - Programa del curso 

En este curso, aprenderá a aprovechar la potencia de Apache Spark y los potentes clústeres que se ejecutan en la 
plataforma Azure Databricks para ejecutar cargas de trabajo de ciencia de datos en la nube.

### Módulo 1 - Introducción a Azure Databricks

En este módulo, descubrirá las capacidades de Azure Databricks y del cuaderno Apache Spark para procesar archivos 
enormes. Llegará a comprender la plataforma Azure Databricks e identificará los tipos de tareas más adecuados para 
Apache Spark. También conocerá la arquitectura de un clúster Azure Databricks Spark y los trabajos Spark.

### Módulo 2 - Trabajar con datos en Azure Databricks

Azure Databricks soporta funciones cotidianas de manejo de datos, como lecturas, escrituras y consultas. En este módulo, 
trabajará con grandes cantidades de datos procedentes de múltiples fuentes en diferentes formatos sin procesar. También 
aprenderá a utilizar la clase de columna DataFrame Azure Databricks para aplicar transformaciones a nivel de columna, 
como **ordenaciones**, **filtros** y **agregaciones**. También utilizará operaciones avanzadas de funciones **DataFrame** 
para **manipular datos, aplicar agregados y realizar operaciones de fecha y hora** en Azure Databricks.

### Módulo 3 - Procesamiento de datos en Azure Databricks

Azure Databricks soporta una serie de funciones SQL incorporadas, sin embargo, a veces tendrá que escribir una función 
personalizada, conocida como **Función Definida por el Usuario (UDF)**. En este módulo, aprenderá a registrar e invocar UDFs. 
También aprenderá a utilizar **Delta Lake** para **crear, añadir y subir datos a tablas de Apache Spark**, aprovechando 
la fiabilidad y las optimizaciones incorporadas.

### Módulo 4 - Empezar con Databricks y el aprendizaje automático

En este módulo, aprenderá a utilizar el paquete de **aprendizaje automático de PySpark** para construir componentes 
clave de los flujos de trabajo de aprendizaje automático que incluyen el **análisis exploratorio de datos**, el 
**entrenamiento de modelos y la evaluación de modelos**. También aprenderá a construir **pipelines** para tareas 
comunes de **featurización** de datos.

### Módulo 5 - Gestionar los ciclos de vida del aprendizaje automático y ajustar los modelos

En este módulo, aprenderá a utilizar **MLflow** para realizar un seguimiento de los experimentos de aprendizaje 
automático y a utilizar módulos de la biblioteca de aprendizaje automático de Spark para el ajuste de hiperparámetros 
y la selección de modelos.

### Módulo 6 - Entrenar una red neuronal distribuida y servir modelos con Azure Machine Learning

En este módulo, aprenderá a utilizar el **marco Horovod de Uber** junto con la biblioteca **Petastorm** para ejecutar 
trabajos de **entrenamiento distribuidos de aprendizaje profundo en Spark** utilizando conjuntos de datos de 
entrenamiento en el formato **Apache Parquet**. También aprenderá a utilizar **MLflow** y el servicio Azure Machine 
Learning para registrar, empaquetar y desplegar un modelo entrenado tanto en **Azure Container Instance**, como en 
**Azure Kubernetes Service** como servicio web de puntuación

## 1 How to be successful in this course
[< Back to index 1](#index-1)

Hacer un curso en línea puede resultar abrumador. ¿Cómo puede aprender a su propio ritmo y alcanzar con éxito sus objetivos?

He aquí algunos consejos generales que pueden ayudarle a mantenerse centrado y en el buen camino:

#### 1: Fíjese objetivos diarios de estudio
Pregúntese qué espera conseguir en su curso cada día. Establecer un objetivo claro puede ayudarle a mantenerse motivado y 
a vencer la procrastinación. El objetivo debe ser específico y fácil de medir, como "Veré todos los vídeos del módulo 2 
y completaré la primera tarea de programación" Y no olvide recompensarse cuando avance hacia su objetivo.

### 2: Cree un espacio dedicado al estudio
Es más fácil recordar la información si se está en el mismo lugar donde se aprendió por primera vez, por lo que tener un 
espacio dedicado en casa para tomar cursos en línea puede hacer que su aprendizaje sea más eficaz. Elimine cualquier 
distracción del espacio y, si es posible, sepárelo de su cama o sofá. Una clara distinción entre el lugar donde estudia 
y el lugar donde se toma los descansos puede ayudarle a concentrarse.

### 3: Programe tiempo para estudiar en su calendario
Abra su calendario y elija un horario predecible y fiable que pueda dedicar a ver las clases y completar las tareas. 
Esto le ayudará a asegurarse de que sus cursos no se conviertan en la última cosa de su lista de tareas.

> Consejo: Puede añadir las fechas límite de un curso de Coursera su calendario de Google, al calendario de Apple o a otra aplicación de calendario.

### 4: Hágase responsable
Cuénteles a sus amigos los cursos que está realizando, publique sus logros en sus cuentas de las redes sociales o publique 
en un blog sus tareas. Contar con una comunidad y una red de apoyo de amigos y familiares que le animen marca la diferencia.

### 5: Tome notas activamente
Tomar apuntes puede fomentar el pensamiento activo, impulsar la comprensión y ampliar su capacidad de atención. Es una 
buena estrategia para interiorizar los conocimientos tanto si aprende en línea como en el aula. Así pues, coja un cuaderno 
o encuentre la aplicación digital que mejor se adapte a usted y empiece a sintetizar los puntos clave.

> Consejo: Mientras ve una clase en Coursera, puede hacer clic en el botón "Guardar nota" situado debajo del vídeo para guardar una captura de pantalla en sus notas del curso y añadir sus propios comentarios.

### 6: Únase a la discusión
Los foros de discusión del curso son un lugar estupendo para hacer preguntas sobre las tareas, discutir temas, compartir 
recursos y hacer amigos. Nuestras investigaciones demuestran que los alumnos que participan en los foros de debate tienen 
un 37% más de probabilidades de completar un curso. Así que ¡haga un post hoy mismo!

### 7: Haga una cosa cada vez
La multitarea es menos productiva que centrarse en una sola tarea a la vez. Investigadores de la Universidad de Stanford 
descubrieron que "las personas que son bombardeadas regularmente con varios flujos de información electrónica no pueden 
prestar atención, recordar información o cambiar de un trabajo a otro tan bien como los que completan una tarea a la vez" 
Concéntrese en una cosa cada vez. Absorberá más información y completará las tareas con mayor productividad y facilidad 
que si intentara hacer muchas cosas a la vez.

### 8: Tómese descansos
Descansar el cerebro después de aprender es fundamental para un alto rendimiento. Si se encuentra trabajando en un problema 
difícil sin avanzar mucho durante una hora, tómese un descanso. Caminar al aire libre, darse una ducha o hablar con un amigo 
puede re-energizarle e incluso darle nuevas ideas sobre cómo abordar ese proyecto.

¡Su viaje de aprendizaje de Microsoft Azure comienza ahora!
Mientras se prepara para el examen o trabaja en la consecución de sus objetivos de aprendizaje, le animamos a:

- Revise las directrices del examen y las habilidades medidas como punto de partida.
- Trabaje a través de cada lección del itinerario de aprendizaje. 
- Intente no saltarse ninguna actividad o lección a menos que esté seguro de que ya conoce esta información lo suficientemente bien como para seguir adelante.
- Aproveche la oportunidad para volver atrás y ver un vídeo o leer la información adicional que se le proporcione antes de pasar a la siguiente lección o módulo.
- Complete todos los cuestionarios, las preguntas de práctica del examen y los ejercicios. Durante las sesiones de práctica, tendrá la oportunidad de volver a repasar las preguntas para asegurarse de que está satisfecho con su progreso.
- Lea atentamente los comentarios cuando responda a los cuestionarios o a los exámenes prácticos, ya que le ayudarán a reforzar lo que está aprendiendo.

Aproveche el entorno de aprendizaje práctico que le proporcionan los ejercicios. Podrá obtener un refuerzo sustancial de su aprendizaje mediante la aplicación paso a paso de sus conocimientos.

# 2 Describe Azure Databricks

## INDEX 2:

- [2 Explain Azure Databricks](#2-explain-azure-databricks)
- [2 Create an Azure Databricks Workspace and cluster](#2-create-an-azure-databricks-workspace-and-cluster)
- [2 Create and execute a notebook](#2-create-and-execute-a-notebook)
- [2 Exercise: Work with Notebooks](#2-exercise-work-with-notebooks)
- [2 Exercise quiz](#2-exercise-quiz)
- [2 Knowledge check](#2-knowledge-check)
- [2 Lesson summary](#2-lesson-summary)

[< Back to index](#index-0)

## 2 Explain Azure Databricks
[< Back to index 2](#index-2)

![1.png](modules%2F2%2Fims%2F1%2F1.png)

Bienvenido a la segunda lección de este módulo. Esta lección se centra en cómo Azure Databricks funciona con Apache Spark Notebook.

![2.png](modules%2F2%2Fims%2F1%2F2.png)

En esta lección, desarrollará: 

- Una comprensión de la plataforma Azure Databricks. 
- Cree su propio espacio de trabajo Azure Databricks. 
- Cree un Notebook dentro de su carpeta de inicio en Databricks. 
- Comprender los fundamentos de Apache Spark Notebook. 
- Crear o adjuntar a un clúster Spark
- Identificar los tipos de tareas bien adaptados al motor de análisis unificado, Apache Spark. 

![3.png](modules%2F2%2Fims%2F1%2F3.png)

Comencemos con un vistazo a la plataforma Azure Databricks. Azure Databricks proporciona un entorno de espacio de trabajo 
Apache Spark como servicio orientado a Notebook. 

Ofrece a los equipos de ciencia de datos e ingeniería una única plataforma para gestionar clústeres y explorar datos de forma interactiva.

![4.png](modules%2F2%2Fims%2F1%2F4.png)

Azure Databricks combina la potencia de Data bricks y la plataforma Apache Spark gestionada de extremo a extremo optimizada 
para la nube y la escala empresarial y la seguridad de la plataforma Microsoft Azure. 

![5.png](modules%2F2%2Fims%2F1%2F5.png)

Con el fin de simplificar la ejecución de cargas de trabajo Spark a gran escala, Databricks fue fundada por los creadores 
de Apache Spark, Delta Lake, y MLflow para capacitar a los desarrolladores para acelerar la IA y la innovación 
simplificando el proceso de construcción de aplicaciones de datos de producción de nivel empresarial.

![6.png](modules%2F2%2Fims%2F1%2F6.png)

En la actualidad, más de 2.000 empresas de todo el mundo utilizan la plataforma Databricks en todo el ciclo de vida de Big Data y Machine Learning. 

![7.png](modules%2F2%2Fims%2F1%2F7.png)

La visión de Databricks es acelerar la innovación unificando la ciencia de datos, la ingeniería de datos y el negocio 
utilizando la plataforma de análisis de Big Data como solución. 

![8.png](modules%2F2%2Fims%2F1%2F8.png)

El propósito de Azure Databricks es abordar los problemas encontrados en otras plataformas de Big Data. Para evitarlos, 
Azure Databricks se optimizó desde cero centrándose en el rendimiento y la eficiencia de costes en la nube

![9.png](modules%2F2%2Fims%2F1%2F9.png)

El tiempo de ejecución de Databricks añade varias capacidades clave a las cargas de trabajo de Apache Spark que pueden 
aumentar el rendimiento y reducir los costes en hasta 10 o 100 veces los de otras plataformas que no se ejecutarían en 
Azure. 

![10.png](modules%2F2%2Fims%2F1%2F10.png)

Estas capacidades incluyen: 

- Conectores de alta velocidad a servicios de almacenamiento Azure como Azure Blob Store y Azure Data Lake.
- autoescalado y terminación de auditoría de clústeres Spark para minimizar costes.
- almacenamiento en caché e indexación.
- optimización avanzada de consultas. 

![11.png](modules%2F2%2Fims%2F1%2F11.png)

Al proporcionar un entorno optimizado, fácil de aprovisionar y configurar, Azure Databricks ofrece a los desarrolladores 
una plataforma rentable que les permite dedicar más tiempo a la creación de aplicaciones y menos tiempo a la gestión de 
clústeres e infraestructura

![12.png](modules%2F2%2Fims%2F1%2F12.png)

Echemos un vistazo más de cerca a cómo Azure Databricks funciona con el resto de la plataforma. 

![13.png](modules%2F2%2Fims%2F1%2F13.png)

Como motor informático, Azure Databricks se sitúa en el centro de su plataforma de software basada en Azure y proporciona 
integración nativa con Azure Data Services, aprendizaje automático, integraciones de gestión del ciclo de vida, y 
tiempo de ejecución de aprendizaje automático. 

![14.png](modules%2F2%2Fims%2F1%2F14.png)

Una de las ventajas clave de Databricks es las numerosas ofertas que no son Spark de código abierto. Estas ofertas incluyen 

- el espacio de trabajo Databricks para ciencia de datos interactiva y colaboración
- flujos de trabajo Databricks para trabajos de producción y automatización de flujos de trabajo
- tiempo de ejecución Databricks
- Databricks IO o DBIO, que es una capa de acceso a datos optimizada
- Databricks Serverless, una plataforma de autoajuste totalmente gestionada
- seguridad empresarial Databricks, también conocida como DBES, que proporciona seguridad y conformidad de extremo a extremo. 

![15.png](modules%2F2%2Fims%2F1%2F15.png)

A continuación, veamos cómo funciona Apache Spark con Azure Databricks. 

![16.png](modules%2F2%2Fims%2F1%2F16.png)

Spark es un motor de procesamiento unificado que puede: 

**analizar big data utilizando SQL**

En su núcleo se encuentra el motor Spark. La API de marcos de datos proporciona una abstracción por encima de los 
conjuntos de datos distribuidos resistentes o RDD, al tiempo que mejora el rendimiento 5-20 veces por encima de los 
RDD tradicionales con su optimizador Catalyst. 

**aprendizaje automático**

Spark ML proporciona algoritmos de aprendizaje automático de alta calidad y finalmente sintonizados para el procesamiento de big data.

**Procesamiento de grafos**

La API de procesamiento de grafos proporciona una API fácilmente accesible para modelar relaciones por pares entre personas, objetos o nodos de la red. 

**Análisis de flujos en tiempo real.** 

Por último, las API de streaming ofrecen una tolerancia a fallos de extremo a extremo con semántica de exactamente una vez 
y la posibilidad de una latencia de submilisegundos.

![17.png](modules%2F2%2Fims%2F1%2F17.png)

Además de ejecutarse en muchos entornos, Apache Spark también es compatible con varios lenguajes. 

- Scala es el lenguaje principal de Apache Spark. 
- La plataforma también ejecuta Python, que se conoce más comúnmente como PySpark. 
- También existe soporte para R, conocido como SparkR o R on Spark, 
- Java 
- SQL.

![18.png](modules%2F2%2Fims%2F1%2F18.png)

Tenga en cuenta que SQL se acerca más al cumplimiento de NC SQL 2003. Ahora ejecuta las 99 evaluaciones comparativas 
del Transaction Process Performance Council Decision Support o consultas TPC-DS. 

Ofrece un nuevo analizador sintáctico conforme a las nuevas normas con buenos mensajes de error, subconsultas, tanto 
correlacionadas como no correlacionadas, y estadísticas agregadas aproximadas

![19.png](modules%2F2%2Fims%2F1%2F19.png)

Por último, con Dataframes API, las diferencias de rendimiento entre lenguajes son casi inexistentes, especialmente para Scala, Java y Python.

## 2 Create an Azure Databricks Workspace and cluster
[< Back to index 2](#index-2)

> Nota: En esta lectura puede ver los pasos implicados en el proceso de creación de un espacio de trabajo y un clúster de Azure Databricks.

Cuando hablamos del espacio de trabajo de Azure Databricks, nos referimos a dos cosas diferentes. 

La primera referencia es el entorno lógico de Azure Databricks en el que se crean los clústeres, se almacenan los datos 
(a través de DBFS) y en el que se alojan los recursos del servidor. 

La segunda referencia es la más común utilizada en el contexto de Azure Databricks. Se trata de la carpeta raíz especial para todos los activos de Databricks de su organización, incluidos cuadernos, bibliotecas y cuadros de mando, como se muestra a continuación:

![1.png](modules%2F2%2Fims%2F2%2F1.png)

El primer paso para utilizar Azure Databricks es crear y desplegar un espacio de trabajo Databricks, que es el entorno lógico. 
Puede hacerlo en el portal de Azure.

### Desplegar un espacio de trabajo Azure Databricks

1. Abra el portal Azure

   ![s1.gif](modules%2F2%2Fims%2F2%2Fs1.gif)

2. Haga clic en **Crear un recurso** en la parte superior izquierda

3. Busque "Databricks"

4. Seleccione **Azure Databricks**

5. En la página Azure Databricks seleccioné Crear

6. Proporcione los valores necesarios para crear su espacio de trabajo Azure Databricks:

   ![s2.gif](modules%2F2%2Fims%2F2%2Fs2.gif)

    - **Suscripción**: Elija la suscripción Azure en la que desplegar el espacio de trabajo.
    
    - **Grupo de recursos**: UtiliceCrear nuevo y proporcioné un nombre para el nuevo grupo de recursos.
    
    - **Ubicación**: Seleccione una ubicación cercana para el despliegue. Para ver la lista de regiones compatibles con Azure Databricks, consulte
    Servicios Azure disponibles por región.
    
    - **Nombre del espacio de trabajo**: Proporcione un nombre único para su espacio de trabajo.
    
    - **Nivel de precio:Prueba (Premium - 14 días de DBU gratuitos)**. Debe seleccionar esta opción al crear su espacio de trabajo o se le cobrará. El espacio de trabajo se suspenderá automáticamente después de 14 días. Cuando finalice el periodo de prueba podrá convertir el espacio de trabajo aPremium, pero entonces se le cobrará por su uso.

7. Seleccione **Revisar + Crear**.

8. Seleccione **Crear**.

La creación del espacio de trabajo tarda unos minutos. Durante la creación del espacio de trabajo, aparecerá el mosaico
Envío de despliegue para Azure Databricks en la parte derecha del portal. Es posible que tenga que desplazarse hacia la 
derecha en su tablero para ver el mosaico. También aparece una barra de progreso cerca de la parte superior de la pantalla. 
Puede observar el progreso en cualquiera de las dos áreas.

![3.png](modules%2F2%2Fims%2F2%2F3.png)


### ¿Qué es un clúster?

Los cuadernos están respaldados por clústeres, u ordenadores conectados en red, que trabajan juntos para procesar sus 
datos. El primer paso es crear un clúster.

### Crear un clúster

1. Cuando haya finalizado la creación de su espacio de trabajo Azure Databricks, seleccione el enlace para ir al recurso.

   ![s3.gif](modules%2F2%2Fims%2F2%2Fs3.gif)

2. Seleccione **Lanzar espacio de trabajo** para abrir su espacio de trabajo Databricks en una nueva pestaña.

3. En el menú de la izquierda de su espacio de trabajo Databricks, seleccione **Clusters**.

4. Seleccione **Crear clúster** para añadir un nuevo clúster.

    ![2.png](modules%2F2%2Fims%2F2%2F2.png)

5. Introduzca un nombre para su cluster. Utilice su nombre o sus iniciales para diferenciar fácilmente su cluster de los de sus compañeros.

   ![s4.gif](modules%2F2%2Fims%2F2%2Fs4.gif)

6. Seleccione el **modo de clúster**:Nodo único.

7. Seleccione el **Databricks Runtime Version**:Runtime: 7.3 LTS (Scala 2.12, Spark 3.0.1).

8. En O**pciones de piloto automático**, dejem arcada la casilla y en el cuadro de texto introduzca **45**.

9. Seleccione el **Tipo de nodo**: Standard_DS3_v2.

10. Seleccione **Crear clúster**.

![4.png](modules%2F2%2Fims%2F2%2F4.png)

## 2 Create and execute a notebook
[< Back to index 2](#index-2)

> Nota: En esta lectura puede ver los pasos que intervienen en el proceso de creación y ejecución de un cuaderno.

Después de crear su espacio de trabajo Databricks, es hora de crear su primer cuaderno. Para ejecutar su cuaderno, 
adjuntará el clúster que creó en el ejercicio anterior.

### ¿Qué es un cuaderno Apache Spark?

Un cuaderno es una colección de celdas. Estas celdas se ejecutan para ejecutar código, renderizar texto formateado o 
mostrar visualizaciones gráficas.

### Crear un cuaderno

1. En el portal Azure, haga clic en el menú Todos los recursos de la parte izquierda de la navegación y seleccione el espacio de trabajo Databricks que creó en la última unidad.

2. Seleccione **Lanzar espacio de trabajo** para abrir su espacio de trabajo Databricks en una nueva pestaña.

3. En el menú de la izquierda de su espacio de trabajo Databricks, seleccione **Inicio**.

4. Haga clic con el botón derecho en su carpeta de inicio.

   ![s1.gif](modules%2F2%2Fims%2F3%2Fs1.gif)

5. Seleccione **Crear**.

6. Seleccione **Cuaderno**.

7. Nombre su cuaderno **Primer cuaderno**.

8. Establezca el Idioma en **Python**.

9. Seleccione el cluster al que adjuntar este cuaderno.

   > Nota: Esta opción sólo aparece cuando hay un cluster en funcionamiento. Todavía puede crear su bloc de notas y 
   > adjuntarlo a un cluster más tarde.

10. Seleccione Crear.

Ahora que ha creado su bloc de notas, vamos a utilizarlo para ejecutar algo de código.

### Acoplar y desacoplar su bloc de notas

Para utilizar su bloc de notas para ejecutar un código, debe adjuntarlo a un clúster. También puede desacoplar su bloc de notas de un clúster y acoplarlo a otro en función de los requisitos de su organización.

![s2.gif](modules%2F2%2Fims%2F3%2Fs2.gif)

Si su portátil está unido a un clúster, puede

- Separar su portátil del cluster
- Reiniciar el clúster
- Adjuntarlo a otro clúster
- Abrir la interfaz de usuario de Spark
- Ver los archivos de registro del controlador

## 2 Exercise: Work with Notebooks
[< Back to index 2](#index-2)

> Nota: Para ejecutar su cuaderno, adjuntará el clúster que creó en el ejercicio anterior.

Puede utilizar los cuadernos Apache Spark para:

- Leer y procesar archivos y conjuntos de datos enormes
- Consultar, explorar y visualizar conjuntos de datos
- Unir conjuntos de datos dispares que se encuentran en lagos de datos
- Entrenar y evaluar modelos de aprendizaje automático
- Procesar flujos de datos en directo
- Realizar análisis en grandes conjuntos de datos gráficos y redes sociales

Para obtener más información sobre el uso de cuadernos, clone el archivo de laboratorios donde se proporcionan cuadernos 
de muestra. Estos cuadernos le ayudarán a comprender cómo utilizar los cuadernos para sus tareas cotidianas.

### Clone el archivo Databricks

1. En el portal Azure, navegue hasta su espacio de trabajo Azure Databricks desplegado y seleccione Lanzar espacio de 
trabajo.

2. En el panel izquierdo, seleccione Espacio de trabajo>Usuarios, y seleccione su nombre de usuario (la entrada con el 
icono de la casa).

3. En el panel que aparece, seleccione la flecha situada junto a su nombre y seleccione Importar.

   ![s1.gif](modules%2F2%2Fims%2F4%2Fs1.gif)

4. En el cuadro de diálogoImportar cuadernos, seleccione la dirección URL y pegue la siguiente:

   > https://github.com/solliancenet/microsoft-learning-paths-databricks-notebooks/blob/master/data-engineering/DBC/01-Introduction-to-Azure-Databricks.dbc?raw=true

5. Seleccione Importar.

   ![s2.gif](modules%2F2%2Fims%2F4%2Fs2.gif)

6. Seleccione la carpeta01-Introduction-to-Azure-Databricks que aparece.

7. Utilice el conjunto de cuadernos de esta carpeta para completar este laboratorio.

### Complete el siguiente cuaderno

**01-Entorno Databricks**- Este cuaderno ilustra los fundamentos de un cuaderno Databricks.

![s1.gif](modules%2F2%2Fims%2F5%2Fs1.gif)

![s2.gif](modules%2F2%2Fims%2F5%2Fs2.gif)

![s3.gif](modules%2F2%2Fims%2F5%2Fs3.gif)

![s4.gif](modules%2F2%2Fims%2F5%2Fs4.gif)

![s5.gif](modules%2F2%2Fims%2F5%2Fs5.gif)

## 2 Exercise quiz
[< Back to index 2](#index-2)

![1.png](modules%2F2%2Fims%2F4%2F1.png)

## 2 Knowledge check
[< Back to index 2](#index-2)

![1.png](modules%2F2%2Ftest%2F1.png)

![2.png](modules%2F2%2Ftest%2F2.png)

![3.png](modules%2F2%2Ftest%2F3.png)

![4.png](modules%2F2%2Ftest%2F4.png)

![5.png](modules%2F2%2Ftest%2F5.png)

## 2 Lesson summary
[< Back to index 2](#index-2)

![1.png](modules%2F2%2Fims%2F6%2F1.png)

En esta lección, tú descubrió los conceptos básicos del espacio de trabajo de Databrick y Cuadernos Apache Spark. 
Dediquemos un momento a recapitulemos lo que aprendimos.

![2.png](modules%2F2%2Fims%2F6%2F2.png)

Azure Databricks ofrece a los equipos de ciencia e ingeniería de datos una plataforma única para administrar clústeres 
y explorar datos de forma interactiva. Databricks impulsa a Apache Genere cargas de trabajo aumentando el rendimiento 
y, al mismo tiempo, reducir los costos otras capacidades. 

![3.png](modules%2F2%2Fims%2F6%2F3.png)

Sitio de Azure Databricks en el centro de la plataforma basada en Azure plataforma de software y proporciona integración nativa con los servicios de Azure.

![4.png](modules%2F2%2Fims%2F6%2F4.png)

Los cuadernos permiten la interacción con diferentes tipos de datos. Se pueden usar para 

- procesar archivos de datos de gran tamaño
- consulta, lee y escribe datos desde diferentes fuentes
- entrene modelos de aprendizaje automático 
- procese flujos de datos en tiempo real.

![5.png](modules%2F2%2Fims%2F6%2F5.png)

Puede ejecutar código Python, Scala , SQL y R en una celda de cuaderno. 

![6.png](modules%2F2%2Fims%2F6%2F6.png)

Es posible crear un clúster Spark y conectarlo y desconectarlo hacia o desde un cuaderno.

![7.png](modules%2F2%2Fims%2F6%2F7.png)

Azure Databricks desempeña un papel clave en el ecosistema de Azure. El sistema de archivos de Databricks, también 
conocido como DBFS, funciona para presentar Blob El almacenamiento como sistema de archivos.

![8.png](modules%2F2%2Fims%2F6%2F8.png)

Si planea completar otros Los módulos de Azure Databricks, no elimines tu Azure Instancia de Databricks todavía. 
Puedes usar el mismo entorno para los demás módulos. Sin embargo, si desea eliminar el Azure Instancia de Databricks 
y, a continuación, complete el siguientes pasos. 

![s0.gif](modules%2F2%2Fims%2F6%2Fs0.gif)

Navegue hasta el portal de recursos y abrir grupos de recursos. Busque y seleccione el grupo de recursos requerido. 
Abra el requerido grupo de recursos y seleccione «Eliminar grupo de recursos» Escriba el nombre del grupo de recursos 
que desea eliminar para confirme la eliminación. Azure Studio confirma su confirmación y elimina el grupo de recursos 
seleccionado.

# 3 Spark Architecture Fundamentals

## INDEX 3:

- [3 Lesson introduction](#3-lesson-introduction)
- [3 Understand the Architecture of Azure Databricks Spark Cluster](#3-understand-the-architecture-of-azure-databricks-spark-cluster)
- [3 Understand the architecture of spark job](#3-understand-the-architecture-of-spark-job)
- [3 Knowledge check](#3-knowledge-check)
- [3 Test prep](#3-test-prep)
- [3 Lesson summary](#3-lesson-summary)

[< Back to index](#index-0)

## 3 Lesson introduction
[< Back to index 3](#index-3)

## 3 Understand the Architecture of Azure Databricks Spark Cluster
[< Back to index 3](#index-3)

## 3 Understand the architecture of spark job
[< Back to index 3](#index-3)

## 3 Knowledge check
[< Back to index 3](#index-3)

## 3 Test prep
[< Back to index 3](#index-3)

## 3 Lesson summary
[< Back to index 3](#index-3)

# 4 Use Azure Databricks to Prepare the Data for Advanced Analytics and Machine Learning Operations

## INDEX 4:

- [4 Lesson introduction](#4-lesson-introduction)
- [4 Read data in CSV format](#4-read-data-in-csv-format)
- [4 Read data in JSON format](#4-read-data-in-json-format)
- [4 Read data in Parquet format](#4-read-data-in-parquet-format)
- [4 Read data stored in tables and views](#4-read-data-stored-in-tables-and-views)
- [4 Write data](#4-write-data)
- [4 Exercise: Read and Write Data](#4-exercise-read-and-write-data)
- [4 Exercise quiz](#4-exercise-quiz)
- [4 Knowledge check](#4-knowledge-check)
- [4 Lesson summary](#4-lesson-summary)


[< Back to index](#index-0)

## 4 Lesson introduction
[< Back to index 4](#index-4)

## 4 Read data in CSV format
[< Back to index 4](#index-4)

## 4 Read data in JSON format
[< Back to index 4](#index-4)

## 4 Read data in Parquet format
[< Back to index 4](#index-4)

## 4 Read data stored in tables and views
[< Back to index 4](#index-4)

## 4 Write data
[< Back to index 4](#index-4)

## 4 Exercise: Read and Write Data
[< Back to index 4](#index-4)

## 4 Exercise quiz
[< Back to index 4](#index-4)

## 4 Knowledge check
[< Back to index 4](#index-4)

## 4 Lesson summary
[< Back to index 4](#index-4)

# 5 Work with DataFrames in Azure Databricks

## INDEX 5:

- [5 Lesson introduction](#5-lesson-introduction)
- [5 Describe a DataFrame](#5-describe-a-dataframe)
- [5 Use common DataFrame Methods](#5-use-common-dataframe-methods)
- [5 Use the display function](#5-use-the-display-function)
- [5 Exercise: Distinct Articles](#5-exercise-distinct-articles)
- [5 Knowledge check](#5-knowledge-check)
- [5 Test prep](#5-test-prep)
- [5 Lesson summary](#5-lesson-summary)


[< Back to index](#index-0)

## 5 Lesson introduction
[< Back to index 5](#index-5)

## 5 Describe a DataFrame
[< Back to index 5](#index-5)

## 5 Use common DataFrame Methods
[< Back to index 5](#index-5)

## 5 Use the display function
[< Back to index 5](#index-5)

## 5 Exercise: Distinct Articles
[< Back to index 5](#index-5)

## 5 Knowledge check
[< Back to index 5](#index-5)

## 5 Test prep
[< Back to index 5](#index-5)

## 5 Lesson summary
[< Back to index 5](#index-5)

# 6 Build and Query a Delta Lake

## INDEX 6:

- [6 Describe the open source Delta Lake](#6-describe-the-open-source-delta-lake)
- [6 Get started with Delta using Spark APIs](#6-get-started-with-delta-using-spark-apis)
- [6 Exercise: Work with basic Delta Lake functionality](#6-exercise-work-with-basic-delta-lake-functionality)
- [6 Exercise quiz 1](#6-exercise-quiz-1)
- [6 Describe how Azure Databricks manages Delta Lake](#6-describe-how-azure-databricks-manages-delta-lake)
- [6 Exercise: Use the Delta Lake Time Machine and perform optimization](#6-exercise-use-the-delta-lake-time-machine-and-perform-optimization)
- [6 Exercise quiz 2](#6-exercise-quiz-2)
- [6 Knowledge check](#6-knowledge-check)
- [6 Lesson summary](#6-lesson-summary)

[< Back to index](#index-0)

## 6 Describe the open source Delta Lake
[< Back to index 6](#index-6)

## 6 Get started with Delta using Spark APIs
[< Back to index 6](#index-6)

## 6 Exercise: Work with basic Delta Lake functionality
[< Back to index 6](#index-6)

## 6 Exercise quiz 1
[< Back to index 6](#index-6)

## 6 Describe how Azure Databricks manages Delta Lake
[< Back to index 6](#index-6)

## 6 Exercise: Use the Delta Lake Time Machine and perform optimization
[< Back to index 6](#index-6)

## 6 Exercise quiz 2
[< Back to index 6](#index-6)

## 6 Knowledge check
[< Back to index 6](#index-6)

## 6 Lesson summary
[< Back to index 6](#index-6)

# 7 Work with user-defined functions

## INDEX 7:

- [Lesson introduction](#7-lesson-introduction)
- [Write user-defined functions](#7-write-user-defined-functions)
- [Exercise: Perform Extract, Transform, Load (ETL) operations using user-defined functions](#7-exercise-perform-extract-transform-load-etl-operations-using-user-defined-functions)
- [Exercise quiz](#7-exercise-quiz)
- [Knowledge check](#7-knowledge-check)
- [Test prep](#7-test-prep)
- [Lesson summary](#7-lesson-summary)
- [Additional resources](#7-additional-resources)


[< Back to index](#index-0)

## 7 Lesson introduction
[< Back to index 7](#index-7)

## 7 Write user defined functions
[< Back to index 7](#index-7)

## 7 Exercise: Perform Extract, Transform, Load (ETL) operations using user-defined functions
[< Back to index 7](#index-7)

## 7 Exercise quiz
[< Back to index 7](#index-7)

## 7 Knowledge check
[< Back to index 7](#index-7)

## 7 Test prep
[< Back to index 7](#index-7)

## 7 Lesson summary
[< Back to index 7](#index-7)

## 7 Additional resources
[< Back to index 7](#index-7)

# 8 Perform Machine Learning with Azure Databricks

## INDEX 8:

- [8 Lesson introduction](#8-lesson-introduction)
- [8 Understand Machine Learning](#8-understand-machine-learning)
- [8 Exercise: Train a Model and Create Predictions](#8-exercise-train-a-model-and-create-predictions)
- [8 Exercise quiz 1](#8-exercise-quiz-1)
- [8 Understand Data Using Exploratory Data Analysis](#8-understand-data-using-exploratory-data-analysis)
- [8 Exercise: Perform Exploratory Data Analysis](#8-exercise-perform-exploratory-data-analysis)
- [8 Exercise quiz 2](#8-exercise-quiz-2)
- [8 Describe Machine Learning Workflows](#8-describe-machine-learning-workflows)
- [8 Exercise: Build and evaluate a baseline machine learning model](#8-exercise-build-and-evaluate-a-baseline-machine-learning-model)
- [8 Exercise quiz 3](#8-exercise-quiz-3)
- [8 Knowledge check](#8-knowledge-check)
- [8 Lesson summary](#8-lesson-summary)


[< Back to index](#index-0)

## 8 Lesson introduction
[< Back to index 8](#index-8)

## 8 Understand Machine Learning
[< Back to index 8](#index-8)

## 8 Exercise: Train a Model and Create Predictions
[< Back to index 8](#index-8)

## 8 Exercise quiz 1
[< Back to index 8](#index-8)

## 8 Understand Data Using Exploratory Data Analysis
[< Back to index 8](#index-8)

## 8 Exercise: Perform Exploratory Data Analysis
[< Back to index 8](#index-8)

## 8 Exercise quiz 2
[< Back to index 8](#index-8)

## 8 Describe Machine Learning Workflows
[< Back to index 8](#index-8)

## 8 Exercise: Build and evaluate a baseline machine learning model
[< Back to index 8](#index-8)

## 8 Exercise quiz 3
[< Back to index 8](#index-8)

## 8 Knowledge check
[< Back to index 8](#index-8)

## 8 Lesson summary
[< Back to index 8](#index-8)

# 9 Train a Machine Learning Model

## INDEX 9:

- [9 Lesson introduction](#9-lesson-introduction)
- [9 Perform featurization of the dataset](#9-perform-featurization-of-the-dataset)
- [9 Exercise: Finish featurization of the dataset](#9-exercise-finish-featurization-of-the-dataset)
- [9 Exercise quiz 1](#9-exercise-quiz-1)
- [9 Understanding regression modeling](#9-understanding-regression-modeling)
- [9 Exercise: Build and interpret a regression model](#9-exercise-build-and-interpret-a-regression-model)
- [9 Exercise quiz 2](#9-exercise-quiz-2)
- [9 Knowledge check](#9-knowledge-check)
- [9 Test prep](#9-test-prep)
- [9 Lesson summary](#9-lesson-summary)
- [9 Additional resources](#9-additional-resources)


[< Back to index](#index-0)

## 9 Lesson introduction
[< Back to index 9](#index-9)

## 9 Perform featurization of the dataset
[< Back to index 9](#index-9)

## 9 Exercise: Finish featurization of the dataset
[< Back to index 9](#index-9)

## 9 Exercise quiz 1
[< Back to index 9](#index-9)

## 9 Understanding regression modeling
[< Back to index 9](#index-9)

## 9 Exercise: Build and interpret a regression model
[< Back to index 9](#index-9)

## 9 Exercise quiz 2
[< Back to index 9](#index-9)

## 9 Knowledge check
[< Back to index 9](#index-9)

## 9 Test prep
[< Back to index 9](#index-9)

## 9 Lesson summary
[< Back to index 9](#index-9)

## 9 Additional resources
[< Back to index 9](#index-9)

# 10 Work with MLFlow in Azure Databricks

## INDEX 10:

- [Lesson introduction](#10-lesson-introduction)
- [Use MLFlow to track experiments, log metrics, and compare runs](#10-use-mlflow-to-track-experiments-log-metrics-and-compare-runs)
- [Exercise: Work with MLFlow to track experiment metrics, parameters, artifacts, and models](#10-exercise-work-with-mlflow-to-track-experiment-metrics-parameters-artifacts-and-models)
- [Exercise quiz](#10-exercise-quiz)
- [Knowledge check](#10-knowledge-check)
- [Lesson summary](#10-lesson-summary)


[< Back to index](#index-0)

## 10 Lesson introduction
[< Back to index 10](#index-10)

## 10 Use MLFlow to track experiments, log metrics, and compare runs
[< Back to index 10](#index-10)

## 10 Exercise: Work with MLFlow to track experiment metrics, parameters, artifacts and models
[< Back to index 10](#index-10)

## 10 Exercise quiz
[< Back to index 10](#index-10)

## 10 Knowledge check
[< Back to index 10](#index-10)

## 10 Lesson summary
[< Back to index 10](#index-10)

# 11 Perform Model Selection with Hyperparameter Tuning

## INDEX 11:

- [11 Lesson introduction](#11-lesson-introduction)
- [11 Describe model selection and hyperparameter tuning](#11-describe-model-selection-and-hyperparameter-tuning)
- [11 Exercise: Select optimal model by tuning hyperparameters](#11-exercise-select-optimal-model-by-tuning-hyperparameters)
- [11 Exercise quiz](#11-exercise-quiz)
- [11 Knowledge check](#11-knowledge-check)
- [11 Test prep](#11-test-prep)
- [11 Lesson summary](#11-lesson-summary)
- [11 Additional resources](#11-additional-resources)


[< Back to index](#index-0)

## 11 Lesson introduction
[< Back to index 11](#index-11)

## 11 Describe model selection and hyperparameter tuning
[< Back to index 11](#index-11)

## 11 Exercise: Select optimal model by tuning hyperparameters
[< Back to index 11](#index-11)

## 11 Exercise quiz
[< Back to index 11](#index-11)

## 11 Knowledge check
[< Back to index 11](#index-11)

## 11 Test prep
[< Back to index 11](#index-11)

## 11 Lesson summary
[< Back to index 11](#index-11)

## 11 Additional resources
[< Back to index 11](#index-11)

# 12 Deep Learning with Horovod for distributed training

## INDEX 12:

- [12 Lesson introduction](#12-lesson-introduction)
- [12 Use Horovod to train a Deep Learning Model](#12-use-horovod-to-train-a-deep-learning-model)
- [12 Use Petastorm to read in Apache Parquet format with Horovod for distributed model training](#12-use-petastorm-to-read-in-apache-parquet-format-with-horovod-for-distributed-model-training)
- [12 Exercise: Work with Horovod and Petastorm for training a deep learning model](#12-exercise-work-with-horovod-and-petastorm-for-training-a-deep-learning-model)
- [12 Exercise quiz](#12-exercise-quiz)
- [12 Knowledge check](#12-knowledge-check)
- [12 Lesson summary](#12-lesson-summary)


[< Back to index](#index-0)

## 12 Lesson introduction
[< Back to index 12](#index-12)

## 12 Use Horovod to train a Deep Learning Model
[< Back to index 12](#index-12)

## 12 Use Petastorm to read in Apache Parquet format with Horovod for distributed model training
[< Back to index 12](#index-12)

## 12 Exercise: Work with Horovod and Petastorm for training a deep learning model
[< Back to index 12](#index-12)

## 12 Exercise quiz
[< Back to index 12](#index-12)

## 12 Knowledge check
[< Back to index 12](#index-12)

## 12 Lesson summary
[< Back to index 12](#index-12)

# 13 Work with Azure Machine Learning to deploy serving models

## INDEX 13:

- [13 Lesson introduction](#13-lesson-introduction)
- [13 Use Azure Machine Learning to Deploy Serving Models](#13-use-azure-machine-learning-to-deploy-serving-models)
- [13 Knowledge check](#13-knowledge-check)
- [13 Test prep](#13-test-prep)
- [13 Lesson summary](#13-lesson-summary)
- [13 Additional resources](#13-additional-resources)

[< Back to index](#index-0)

## 13 Lesson introduction
[< Back to index 13](#index-13)

## 13 Use Azure Machine Learning to Deploy Serving Models
[< Back to index 13](#index-13)

## 13 Knowledge check
[< Back to index 13](#index-13)

## 13 Test prep
[< Back to index 13](#index-13)

## 13 Lesson summary
[< Back to index 13](#index-13)

## 13 Additional resources
[< Back to index 13](#index-13)
