# Perform Data Science with Azure Databricks

## INDEX 0

- [1 Welcome to the Course](#1-welcome-to-the-course)
- [2 Describe Azure Databricks](#2-describe-azure-databricks)
- [3 Spark Architecture Fundamentals](#3-spark-architecture-fundamentals)
- [4 Use Azure Databricks to Prepare the Data for Advanced Analytics and Machine Learning Operations](#4-use-azure-databricks-to-prepare-the-data-for-advanced-analytics-and-machine-learning-operations)
- [5 Work with DataFrames in Azure Databricks](#5-work-with-dataframes-in-azure-databricks)
- [6 Build and Query a Delta Lake](#6-build-and-query-a-delta-lake)
- [7 Work with user-defined functions](#7-work-with-user-defined-functions)
- [8 Perform Machine Learning with Azure Databricks](#8-perform-machine-learning-with-azure-databricks)
- [9 Train a Machine Learning Model](#9-train-a-machine-learning-model)
- [10 Work with MLFlow in Azure Databricks](#10-work-with-mlflow-in-azure-databricks)
- [11 Perform Model Selection with Hyperparameter Tuning](#11-perform-model-selection-with-hyperparameter-tuning)
- [12 Deep Learning with Horovod for distributed training](#12-deep-learning-with-horovod-for-distributed-training)
- [13 Work with Azure Machine Learning to deploy serving models](#13-work-with-azure-machine-learning-to-deploy-serving-models)

# 1 Welcome to the Course

## INDEX 1:

- [1 Introduction to the course](#1-introduction-to-the-course)
- [1 Course syllabus](#1-course-syllabus)
- [1 How to be successful in this course](#1-how-to-be-successful-in-this-course)

[< Back to index](#index-0)

## 1 Introduction to the course
[< Back to index 1](#index-1)

![1.png](modules%2F1%2Fims%2F1%2F1.png)

Hola y bienvenidos a este curso, realiza ciencia de datos con Azure Databricks. En este curso aprenderás cómo realice 
ciencia de datos con Azure Dataricks. Este curso está compuesto de los siguientes módulos.

![2.png](modules%2F1%2Fims%2F1%2F2.png)

1. Introducción a Azure Databricks. 
2. Trabajar con datos en Azure Databricks. 
3. Procesamiento de datos en Azure Databricks. 
4. Comience a usar Azure Databricks y aprendizaje automático. 
5. Gestione los ciclos de vida del aprendizaje automático y modelos afinados. 
6. Entrena una red neuronal distribuida y sirva modelos con el aprendizaje automático de Azure.


![3.png](modules%2F1%2Fims%2F1%2F3.png)

Comencemos con el primer módulo. **Introducción a Azure Databricks**. En este módulo, describirá como tus Databricks 
y Apache Spark para procesar archivos de gran tamaño y describe los fundamentos de la arquitectura de Spark. 

![4.png](modules%2F1%2Fims%2F1%2F4.png)

En el siguiente módulo, **trabajarás con datos en Azure Databricks.** Use Azure Databricks para leer varios archivos 
tipos con y sin esquema. Combina entradas de archivos y almacenes de datos como la base de datos Azure CQL. Transforme 
y almacene esos datos para análisis avanzados. Procese datos en Azure Databricks definiendo marcos de datos para leer 
y procesar los datos. Y realice transformaciones de datos en marcos de datos y ejecuta acciones para mostrar los datos 
transformados hacia arriba.

![5.png](modules%2F1%2Fims%2F1%2F5.png)

A continuación, aprenderá cómo **procesar datos en Azure Databricks**. En este módulo aprenderás cómo crear y consulta 
un Delta Lake. 

![6.png](modules%2F1%2Fims%2F1%2F6.png)

A continuación, **comenzarás con Azure Databricks y el aprendizaje automático**. En este módulo entenderás los conceptos 
básicos del aprendizaje automático y el flujo de trabajo del aprendizaje automático. Este módulo cubrirá el rendimiento de la máquina aprender con Azure Databricks y aprender a capacitarse un modelo de aprendizaje automático.

![7.png](modules%2F1%2Fims%2F1%2F7.png)

A continuación, aprenderás a gestionar ciclos de vida de aprendizaje automático y modelos ajustados. En este módulo, 
aprenderás cómo trabajar con Mlflow en Azure Databricks y realizar una selección de modelos con hiperajuste de 
parámetros.

![8.png](modules%2F1%2Fims%2F1%2F8.png)

Esto incluirá cómo usar los módulos de Biblioteca de aprendizaje automático PySparks para ajustar hiperparámetros.

![9.png](modules%2F1%2Fims%2F1%2F9.png)

A continuación, aprenderás a entrenar una red neuronal distribuida y modelos de servicio con el aprendizaje automático 
de Azure. En este módulo entenderás en profundidad aprender con Haravad para una formación distribuida y cómo 
trabajar con Azure aprendizaje automático para implementar modelos de servicio. 

![10.png](modules%2F1%2Fims%2F1%2F10.png)

Además, a lo largo de este curso tendrá la oportunidad de crear y trabajar con Azure Databricks a través de ejercicios 
interactivos y ejemplos del mundo real de la ciencia de datos en acción, comprobaciones de conocimientos y exámenes de 
práctica. 

![11.png](modules%2F1%2Fims%2F1%2F11.png)

Por último, te prepararás para el programa Microsoft Certified Associate realicé la prueba realizando un examen de 
práctica. Le deseamos el mayor de los éxitos como usted comience este viaje de aprendizaje.

## 1 Course syllabus
[< Back to index 1](#index-1)

### Realice ciencia de datos con Azure Databricks - Programa del curso 

En este curso, aprenderá a aprovechar la potencia de Apache Spark y los potentes clústeres que se ejecutan en la 
plataforma Azure Databricks para ejecutar cargas de trabajo de ciencia de datos en la nube.

### Módulo 1 - Introducción a Azure Databricks

En este módulo, descubrirá las capacidades de Azure Databricks y del cuaderno Apache Spark para procesar archivos 
enormes. Llegará a comprender la plataforma Azure Databricks e identificará los tipos de tareas más adecuados para 
Apache Spark. También conocerá la arquitectura de un clúster Azure Databricks Spark y los trabajos Spark.

### Módulo 2 - Trabajar con datos en Azure Databricks

Azure Databricks soporta funciones cotidianas de manejo de datos, como lecturas, escrituras y consultas. En este módulo, 
trabajará con grandes cantidades de datos procedentes de múltiples fuentes en diferentes formatos sin procesar. También 
aprenderá a utilizar la clase de columna DataFrame Azure Databricks para aplicar transformaciones a nivel de columna, 
como **ordenaciones**, **filtros** y **agregaciones**. También utilizará operaciones avanzadas de funciones **DataFrame** 
para **manipular datos, aplicar agregados y realizar operaciones de fecha y hora** en Azure Databricks.

### Módulo 3 - Procesamiento de datos en Azure Databricks

Azure Databricks soporta una serie de funciones SQL incorporadas, sin embargo, a veces tendrá que escribir una función 
personalizada, conocida como **Función Definida por el Usuario (UDF)**. En este módulo, aprenderá a registrar e invocar UDFs. 
También aprenderá a utilizar **Delta Lake** para **crear, añadir y subir datos a tablas de Apache Spark**, aprovechando 
la fiabilidad y las optimizaciones incorporadas.

### Módulo 4 - Empezar con Databricks y el aprendizaje automático

En este módulo, aprenderá a utilizar el paquete de **aprendizaje automático de PySpark** para construir componentes 
clave de los flujos de trabajo de aprendizaje automático que incluyen el **análisis exploratorio de datos**, el 
**entrenamiento de modelos y la evaluación de modelos**. También aprenderá a construir **pipelines** para tareas 
comunes de **featurización** de datos.

### Módulo 5 - Gestionar los ciclos de vida del aprendizaje automático y ajustar los modelos

En este módulo, aprenderá a utilizar **MLflow** para realizar un seguimiento de los experimentos de aprendizaje 
automático y a utilizar módulos de la biblioteca de aprendizaje automático de Spark para el ajuste de hiperparámetros 
y la selección de modelos.

### Módulo 6 - Entrenar una red neuronal distribuida y servir modelos con Azure Machine Learning

En este módulo, aprenderá a utilizar el **marco Horovod de Uber** junto con la biblioteca **Petastorm** para ejecutar 
trabajos de **entrenamiento distribuidos de aprendizaje profundo en Spark** utilizando conjuntos de datos de 
entrenamiento en el formato **Apache Parquet**. También aprenderá a utilizar **MLflow** y el servicio Azure Machine 
Learning para registrar, empaquetar y desplegar un modelo entrenado tanto en **Azure Container Instance**, como en 
**Azure Kubernetes Service** como servicio web de puntuación

## 1 How to be successful in this course
[< Back to index 1](#index-1)

Hacer un curso en línea puede resultar abrumador. ¿Cómo puede aprender a su propio ritmo y alcanzar con éxito sus objetivos?

He aquí algunos consejos generales que pueden ayudarle a mantenerse centrado y en el buen camino:

#### 1: Fíjese objetivos diarios de estudio
Pregúntese qué espera conseguir en su curso cada día. Establecer un objetivo claro puede ayudarle a mantenerse motivado y 
a vencer la procrastinación. El objetivo debe ser específico y fácil de medir, como "Veré todos los vídeos del módulo 2 
y completaré la primera tarea de programación" Y no olvide recompensarse cuando avance hacia su objetivo.

### 2: Cree un espacio dedicado al estudio
Es más fácil recordar la información si se está en el mismo lugar donde se aprendió por primera vez, por lo que tener un 
espacio dedicado en casa para tomar cursos en línea puede hacer que su aprendizaje sea más eficaz. Elimine cualquier 
distracción del espacio y, si es posible, sepárelo de su cama o sofá. Una clara distinción entre el lugar donde estudia 
y el lugar donde se toma los descansos puede ayudarle a concentrarse.

### 3: Programe tiempo para estudiar en su calendario
Abra su calendario y elija un horario predecible y fiable que pueda dedicar a ver las clases y completar las tareas. 
Esto le ayudará a asegurarse de que sus cursos no se conviertan en la última cosa de su lista de tareas.

> Consejo: Puede añadir las fechas límite de un curso de Coursera su calendario de Google, al calendario de Apple o a otra aplicación de calendario.

### 4: Hágase responsable
Cuénteles a sus amigos los cursos que está realizando, publique sus logros en sus cuentas de las redes sociales o publique 
en un blog sus tareas. Contar con una comunidad y una red de apoyo de amigos y familiares que le animen marca la diferencia.

### 5: Tome notas activamente
Tomar apuntes puede fomentar el pensamiento activo, impulsar la comprensión y ampliar su capacidad de atención. Es una 
buena estrategia para interiorizar los conocimientos tanto si aprende en línea como en el aula. Así pues, coja un cuaderno 
o encuentre la aplicación digital que mejor se adapte a usted y empiece a sintetizar los puntos clave.

> Consejo: Mientras ve una clase en Coursera, puede hacer clic en el botón "Guardar nota" situado debajo del vídeo para guardar una captura de pantalla en sus notas del curso y añadir sus propios comentarios.

### 6: Únase a la discusión
Los foros de discusión del curso son un lugar estupendo para hacer preguntas sobre las tareas, discutir temas, compartir 
recursos y hacer amigos. Nuestras investigaciones demuestran que los alumnos que participan en los foros de debate tienen 
un 37% más de probabilidades de completar un curso. Así que ¡haga un post hoy mismo!

### 7: Haga una cosa cada vez
La multitarea es menos productiva que centrarse en una sola tarea a la vez. Investigadores de la Universidad de Stanford 
descubrieron que "las personas que son bombardeadas regularmente con varios flujos de información electrónica no pueden 
prestar atención, recordar información o cambiar de un trabajo a otro tan bien como los que completan una tarea a la vez" 
Concéntrese en una cosa cada vez. Absorberá más información y completará las tareas con mayor productividad y facilidad 
que si intentara hacer muchas cosas a la vez.

### 8: Tómese descansos
Descansar el cerebro después de aprender es fundamental para un alto rendimiento. Si se encuentra trabajando en un problema 
difícil sin avanzar mucho durante una hora, tómese un descanso. Caminar al aire libre, darse una ducha o hablar con un amigo 
puede re-energizarle e incluso darle nuevas ideas sobre cómo abordar ese proyecto.

¡Su viaje de aprendizaje de Microsoft Azure comienza ahora!
Mientras se prepara para el examen o trabaja en la consecución de sus objetivos de aprendizaje, le animamos a:

- Revise las directrices del examen y las habilidades medidas como punto de partida.
- Trabaje a través de cada lección del itinerario de aprendizaje. 
- Intente no saltarse ninguna actividad o lección a menos que esté seguro de que ya conoce esta información lo suficientemente bien como para seguir adelante.
- Aproveche la oportunidad para volver atrás y ver un vídeo o leer la información adicional que se le proporcione antes de pasar a la siguiente lección o módulo.
- Complete todos los cuestionarios, las preguntas de práctica del examen y los ejercicios. Durante las sesiones de práctica, tendrá la oportunidad de volver a repasar las preguntas para asegurarse de que está satisfecho con su progreso.
- Lea atentamente los comentarios cuando responda a los cuestionarios o a los exámenes prácticos, ya que le ayudarán a reforzar lo que está aprendiendo.

Aproveche el entorno de aprendizaje práctico que le proporcionan los ejercicios. Podrá obtener un refuerzo sustancial de su aprendizaje mediante la aplicación paso a paso de sus conocimientos.

# 2 Describe Azure Databricks

## INDEX 2:

- [2 Explain Azure Databricks](#2-explain-azure-databricks)
- [2 Create an Azure Databricks Workspace and cluster](#2-create-an-azure-databricks-workspace-and-cluster)
- [2 Create and execute a notebook](#2-create-and-execute-a-notebook)
- [2 Exercise: Work with Notebooks](#2-exercise-work-with-notebooks)
- [2 Exercise quiz](#2-exercise-quiz)
- [2 Knowledge check](#2-knowledge-check)
- [2 Lesson summary](#2-lesson-summary)

[< Back to index](#index-0)

## 2 Explain Azure Databricks
[< Back to index 2](#index-2)

![1.png](modules%2F2%2Fims%2F1%2F1.png)

Bienvenido a la segunda lección de este módulo. Esta lección se centra en cómo Azure Databricks funciona con Apache Spark Notebook.

![2.png](modules%2F2%2Fims%2F1%2F2.png)

En esta lección, desarrollará: 

- Una comprensión de la plataforma Azure Databricks. 
- Cree su propio espacio de trabajo Azure Databricks. 
- Cree un Notebook dentro de su carpeta de inicio en Databricks. 
- Comprender los fundamentos de Apache Spark Notebook. 
- Crear o adjuntar a un clúster Spark
- Identificar los tipos de tareas bien adaptados al motor de análisis unificado, Apache Spark. 

![3.png](modules%2F2%2Fims%2F1%2F3.png)

Comencemos con un vistazo a la plataforma Azure Databricks. Azure Databricks proporciona un entorno de espacio de trabajo 
Apache Spark como servicio orientado a Notebook. 

Ofrece a los equipos de ciencia de datos e ingeniería una única plataforma para gestionar clústeres y explorar datos de forma interactiva.

![4.png](modules%2F2%2Fims%2F1%2F4.png)

Azure Databricks combina la potencia de Data bricks y la plataforma Apache Spark gestionada de extremo a extremo optimizada 
para la nube y la escala empresarial y la seguridad de la plataforma Microsoft Azure. 

![5.png](modules%2F2%2Fims%2F1%2F5.png)

Con el fin de simplificar la ejecución de cargas de trabajo Spark a gran escala, Databricks fue fundada por los creadores 
de Apache Spark, Delta Lake, y MLflow para capacitar a los desarrolladores para acelerar la IA y la innovación 
simplificando el proceso de construcción de aplicaciones de datos de producción de nivel empresarial.

![6.png](modules%2F2%2Fims%2F1%2F6.png)

En la actualidad, más de 2.000 empresas de todo el mundo utilizan la plataforma Databricks en todo el ciclo de vida de Big Data y Machine Learning. 

![7.png](modules%2F2%2Fims%2F1%2F7.png)

La visión de Databricks es acelerar la innovación unificando la ciencia de datos, la ingeniería de datos y el negocio 
utilizando la plataforma de análisis de Big Data como solución. 

![8.png](modules%2F2%2Fims%2F1%2F8.png)

El propósito de Azure Databricks es abordar los problemas encontrados en otras plataformas de Big Data. Para evitarlos, 
Azure Databricks se optimizó desde cero centrándose en el rendimiento y la eficiencia de costes en la nube

![9.png](modules%2F2%2Fims%2F1%2F9.png)

El tiempo de ejecución de Databricks añade varias capacidades clave a las cargas de trabajo de Apache Spark que pueden 
aumentar el rendimiento y reducir los costes en hasta 10 o 100 veces los de otras plataformas que no se ejecutarían en 
Azure. 

![10.png](modules%2F2%2Fims%2F1%2F10.png)

Estas capacidades incluyen: 

- Conectores de alta velocidad a servicios de almacenamiento Azure como Azure Blob Store y Azure Data Lake.
- autoescalado y terminación de auditoría de clústeres Spark para minimizar costes.
- almacenamiento en caché e indexación.
- optimización avanzada de consultas. 

![11.png](modules%2F2%2Fims%2F1%2F11.png)

Al proporcionar un entorno optimizado, fácil de aprovisionar y configurar, Azure Databricks ofrece a los desarrolladores 
una plataforma rentable que les permite dedicar más tiempo a la creación de aplicaciones y menos tiempo a la gestión de 
clústeres e infraestructura

![12.png](modules%2F2%2Fims%2F1%2F12.png)

Echemos un vistazo más de cerca a cómo Azure Databricks funciona con el resto de la plataforma. 

![13.png](modules%2F2%2Fims%2F1%2F13.png)

Como motor informático, Azure Databricks se sitúa en el centro de su plataforma de software basada en Azure y proporciona 
integración nativa con Azure Data Services, aprendizaje automático, integraciones de gestión del ciclo de vida, y 
tiempo de ejecución de aprendizaje automático. 

![14.png](modules%2F2%2Fims%2F1%2F14.png)

Una de las ventajas clave de Databricks es las numerosas ofertas que no son Spark de código abierto. Estas ofertas incluyen 

- el espacio de trabajo Databricks para ciencia de datos interactiva y colaboración
- flujos de trabajo Databricks para trabajos de producción y automatización de flujos de trabajo
- tiempo de ejecución Databricks
- Databricks IO o DBIO, que es una capa de acceso a datos optimizada
- Databricks Serverless, una plataforma de autoajuste totalmente gestionada
- seguridad empresarial Databricks, también conocida como DBES, que proporciona seguridad y conformidad de extremo a extremo. 

![15.png](modules%2F2%2Fims%2F1%2F15.png)

A continuación, veamos cómo funciona Apache Spark con Azure Databricks. 

![16.png](modules%2F2%2Fims%2F1%2F16.png)

Spark es un motor de procesamiento unificado que puede: 

**analizar big data utilizando SQL**

En su núcleo se encuentra el motor Spark. La API de marcos de datos proporciona una abstracción por encima de los 
conjuntos de datos distribuidos resistentes o RDD, al tiempo que mejora el rendimiento 5-20 veces por encima de los 
RDD tradicionales con su optimizador Catalyst. 

**aprendizaje automático**

Spark ML proporciona algoritmos de aprendizaje automático de alta calidad y finalmente sintonizados para el procesamiento de big data.

**Procesamiento de grafos**

La API de procesamiento de grafos proporciona una API fácilmente accesible para modelar relaciones por pares entre personas, objetos o nodos de la red. 

**Análisis de flujos en tiempo real.** 

Por último, las API de streaming ofrecen una tolerancia a fallos de extremo a extremo con semántica de exactamente una vez 
y la posibilidad de una latencia de submilisegundos.

![17.png](modules%2F2%2Fims%2F1%2F17.png)

Además de ejecutarse en muchos entornos, Apache Spark también es compatible con varios lenguajes. 

- Scala es el lenguaje principal de Apache Spark. 
- La plataforma también ejecuta Python, que se conoce más comúnmente como PySpark. 
- También existe soporte para R, conocido como SparkR o R on Spark, 
- Java 
- SQL.

![18.png](modules%2F2%2Fims%2F1%2F18.png)

Tenga en cuenta que SQL se acerca más al cumplimiento de NC SQL 2003. Ahora ejecuta las 99 evaluaciones comparativas 
del Transaction Process Performance Council Decision Support o consultas TPC-DS. 

Ofrece un nuevo analizador sintáctico conforme a las nuevas normas con buenos mensajes de error, subconsultas, tanto 
correlacionadas como no correlacionadas, y estadísticas agregadas aproximadas

![19.png](modules%2F2%2Fims%2F1%2F19.png)

Por último, con Dataframes API, las diferencias de rendimiento entre lenguajes son casi inexistentes, especialmente para Scala, Java y Python.

## 2 Create an Azure Databricks Workspace and cluster
[< Back to index 2](#index-2)

> Nota: En esta lectura puede ver los pasos implicados en el proceso de creación de un espacio de trabajo y un clúster de Azure Databricks.

Cuando hablamos del espacio de trabajo de Azure Databricks, nos referimos a dos cosas diferentes. 

La primera referencia es el entorno lógico de Azure Databricks en el que se crean los clústeres, se almacenan los datos 
(a través de DBFS) y en el que se alojan los recursos del servidor. 

La segunda referencia es la más común utilizada en el contexto de Azure Databricks. Se trata de la carpeta raíz especial para todos los activos de Databricks de su organización, incluidos cuadernos, bibliotecas y cuadros de mando, como se muestra a continuación:

![1.png](modules%2F2%2Fims%2F2%2F1.png)

El primer paso para utilizar Azure Databricks es crear y desplegar un espacio de trabajo Databricks, que es el entorno lógico. 
Puede hacerlo en el portal de Azure.

### Desplegar un espacio de trabajo Azure Databricks

1. Abra el portal Azure

   ![s1.gif](modules%2F2%2Fims%2F2%2Fs1.gif)

2. Haga clic en **Crear un recurso** en la parte superior izquierda

3. Busque "Databricks"

4. Seleccione **Azure Databricks**

5. En la página Azure Databricks seleccioné Crear

6. Proporcione los valores necesarios para crear su espacio de trabajo Azure Databricks:

   ![s2.gif](modules%2F2%2Fims%2F2%2Fs2.gif)

    - **Suscripción**: Elija la suscripción Azure en la que desplegar el espacio de trabajo.
    
    - **Grupo de recursos**: UtiliceCrear nuevo y proporcioné un nombre para el nuevo grupo de recursos.
    
    - **Ubicación**: Seleccione una ubicación cercana para el despliegue. Para ver la lista de regiones compatibles con Azure Databricks, consulte
    Servicios Azure disponibles por región.
    
    - **Nombre del espacio de trabajo**: Proporcione un nombre único para su espacio de trabajo.
    
    - **Nivel de precio:Prueba (Premium - 14 días de DBU gratuitos)**. Debe seleccionar esta opción al crear su espacio de trabajo o se le cobrará. El espacio de trabajo se suspenderá automáticamente después de 14 días. Cuando finalice el periodo de prueba podrá convertir el espacio de trabajo aPremium, pero entonces se le cobrará por su uso.

7. Seleccione **Revisar + Crear**.

8. Seleccione **Crear**.

La creación del espacio de trabajo tarda unos minutos. Durante la creación del espacio de trabajo, aparecerá el mosaico
Envío de despliegue para Azure Databricks en la parte derecha del portal. Es posible que tenga que desplazarse hacia la 
derecha en su tablero para ver el mosaico. También aparece una barra de progreso cerca de la parte superior de la pantalla. 
Puede observar el progreso en cualquiera de las dos áreas.

![3.png](modules%2F2%2Fims%2F2%2F3.png)


### ¿Qué es un clúster?

Los cuadernos están respaldados por clústeres, u ordenadores conectados en red, que trabajan juntos para procesar sus 
datos. El primer paso es crear un clúster.

### Crear un clúster

1. Cuando haya finalizado la creación de su espacio de trabajo Azure Databricks, seleccione el enlace para ir al recurso.

   ![s3.gif](modules%2F2%2Fims%2F2%2Fs3.gif)

2. Seleccione **Lanzar espacio de trabajo** para abrir su espacio de trabajo Databricks en una nueva pestaña.

3. En el menú de la izquierda de su espacio de trabajo Databricks, seleccione **Clusters**.

4. Seleccione **Crear clúster** para añadir un nuevo clúster.

    ![2.png](modules%2F2%2Fims%2F2%2F2.png)

5. Introduzca un nombre para su cluster. Utilice su nombre o sus iniciales para diferenciar fácilmente su cluster de los de sus compañeros.

   ![s4.gif](modules%2F2%2Fims%2F2%2Fs4.gif)

6. Seleccione el **modo de clúster**:Nodo único.

7. Seleccione el **Databricks Runtime Version**:Runtime: 7.3 LTS (Scala 2.12, Spark 3.0.1).

8. En O**pciones de piloto automático**, dejem arcada la casilla y en el cuadro de texto introduzca **45**.

9. Seleccione el **Tipo de nodo**: Standard_DS3_v2.

10. Seleccione **Crear clúster**.

![4.png](modules%2F2%2Fims%2F2%2F4.png)

## 2 Create and execute a notebook
[< Back to index 2](#index-2)

> Nota: En esta lectura puede ver los pasos que intervienen en el proceso de creación y ejecución de un cuaderno.

Después de crear su espacio de trabajo Databricks, es hora de crear su primer cuaderno. Para ejecutar su cuaderno, 
adjuntará el clúster que creó en el ejercicio anterior.

### ¿Qué es un cuaderno Apache Spark?

Un cuaderno es una colección de celdas. Estas celdas se ejecutan para ejecutar código, renderizar texto formateado o 
mostrar visualizaciones gráficas.

### Crear un cuaderno

1. En el portal Azure, haga clic en el menú Todos los recursos de la parte izquierda de la navegación y seleccione el espacio de trabajo Databricks que creó en la última unidad.

2. Seleccione **Lanzar espacio de trabajo** para abrir su espacio de trabajo Databricks en una nueva pestaña.

3. En el menú de la izquierda de su espacio de trabajo Databricks, seleccione **Inicio**.

4. Haga clic con el botón derecho en su carpeta de inicio.

   ![s1.gif](modules%2F2%2Fims%2F3%2Fs1.gif)

5. Seleccione **Crear**.

6. Seleccione **Cuaderno**.

7. Nombre su cuaderno **Primer cuaderno**.

8. Establezca el Idioma en **Python**.

9. Seleccione el cluster al que adjuntar este cuaderno.

   > Nota: Esta opción sólo aparece cuando hay un cluster en funcionamiento. Todavía puede crear su bloc de notas y 
   > adjuntarlo a un cluster más tarde.

10. Seleccione Crear.

Ahora que ha creado su bloc de notas, vamos a utilizarlo para ejecutar algo de código.

### Acoplar y desacoplar su bloc de notas

Para utilizar su bloc de notas para ejecutar un código, debe adjuntarlo a un clúster. También puede desacoplar su bloc de notas de un clúster y acoplarlo a otro en función de los requisitos de su organización.

![s2.gif](modules%2F2%2Fims%2F3%2Fs2.gif)

Si su portátil está unido a un clúster, puede

- Separar su portátil del cluster
- Reiniciar el clúster
- Adjuntarlo a otro clúster
- Abrir la interfaz de usuario de Spark
- Ver los archivos de registro del controlador

## 2 Exercise: Work with Notebooks
[< Back to index 2](#index-2)

> Nota: Para ejecutar su cuaderno, adjuntará el clúster que creó en el ejercicio anterior.

Puede utilizar los cuadernos Apache Spark para:

- Leer y procesar archivos y conjuntos de datos enormes
- Consultar, explorar y visualizar conjuntos de datos
- Unir conjuntos de datos dispares que se encuentran en lagos de datos
- Entrenar y evaluar modelos de aprendizaje automático
- Procesar flujos de datos en directo
- Realizar análisis en grandes conjuntos de datos gráficos y redes sociales

Para obtener más información sobre el uso de cuadernos, clone el archivo de laboratorios donde se proporcionan cuadernos 
de muestra. Estos cuadernos le ayudarán a comprender cómo utilizar los cuadernos para sus tareas cotidianas.

### Clone el archivo Databricks

1. En el portal Azure, navegue hasta su espacio de trabajo Azure Databricks desplegado y seleccione Lanzar espacio de 
trabajo.

2. En el panel izquierdo, seleccione Espacio de trabajo>Usuarios, y seleccione su nombre de usuario (la entrada con el 
icono de la casa).

3. En el panel que aparece, seleccione la flecha situada junto a su nombre y seleccione Importar.

   ![s1.gif](modules%2F2%2Fims%2F4%2Fs1.gif)

4. En el cuadro de diálogoImportar cuadernos, seleccione la dirección URL y pegue la siguiente:

   > https://github.com/solliancenet/microsoft-learning-paths-databricks-notebooks/blob/master/data-engineering/DBC/01-Introduction-to-Azure-Databricks.dbc?raw=true

5. Seleccione Importar.

   ![s2.gif](modules%2F2%2Fims%2F4%2Fs2.gif)

6. Seleccione la carpeta01-Introduction-to-Azure-Databricks que aparece.

7. Utilice el conjunto de cuadernos de esta carpeta para completar este laboratorio.

### Complete el siguiente cuaderno

[01-The-Databricks-Environment.ipynb](modules%2F2%2Fnotebooks%2F01-The-Databricks-Environment.ipynb)- Este cuaderno ilustra los fundamentos de un cuaderno Databricks.

![s1.gif](modules%2F2%2Fims%2F5%2Fs1.gif)

![s2.gif](modules%2F2%2Fims%2F5%2Fs2.gif)

![s3.gif](modules%2F2%2Fims%2F5%2Fs3.gif)

![s4.gif](modules%2F2%2Fims%2F5%2Fs4.gif)

![s5.gif](modules%2F2%2Fims%2F5%2Fs5.gif)

## 2 Exercise quiz
[< Back to index 2](#index-2)

![1.png](modules%2F2%2Fims%2F4%2F1.png)

## 2 Knowledge check
[< Back to index 2](#index-2)

![1.png](modules%2F2%2Ftest%2F1.png)

![2.png](modules%2F2%2Ftest%2F2.png)

![3.png](modules%2F2%2Ftest%2F3.png)

![4.png](modules%2F2%2Ftest%2F4.png)

![5.png](modules%2F2%2Ftest%2F5.png)

## 2 Lesson summary
[< Back to index 2](#index-2)

![1.png](modules%2F2%2Fims%2F6%2F1.png)

En esta lección, tú descubrió los conceptos básicos del espacio de trabajo de Databrick y Cuadernos Apache Spark. 
Dediquemos un momento a recapitulemos lo que aprendimos.

![2.png](modules%2F2%2Fims%2F6%2F2.png)

Azure Databricks ofrece a los equipos de ciencia e ingeniería de datos una plataforma única para administrar clústeres 
y explorar datos de forma interactiva. Databricks impulsa a Apache Genere cargas de trabajo aumentando el rendimiento 
y, al mismo tiempo, reducir los costos otras capacidades. 

![3.png](modules%2F2%2Fims%2F6%2F3.png)

Sitio de Azure Databricks en el centro de la plataforma basada en Azure plataforma de software y proporciona integración nativa con los servicios de Azure.

![4.png](modules%2F2%2Fims%2F6%2F4.png)

Los cuadernos permiten la interacción con diferentes tipos de datos. Se pueden usar para 

- procesar archivos de datos de gran tamaño
- consulta, lee y escribe datos desde diferentes fuentes
- entrene modelos de aprendizaje automático 
- procese flujos de datos en tiempo real.

![5.png](modules%2F2%2Fims%2F6%2F5.png)

Puede ejecutar código Python, Scala , SQL y R en una celda de cuaderno. 

![6.png](modules%2F2%2Fims%2F6%2F6.png)

Es posible crear un clúster Spark y conectarlo y desconectarlo hacia o desde un cuaderno.

![7.png](modules%2F2%2Fims%2F6%2F7.png)

Azure Databricks desempeña un papel clave en el ecosistema de Azure. El sistema de archivos de Databricks, también 
conocido como DBFS, funciona para presentar Blob El almacenamiento como sistema de archivos.

![8.png](modules%2F2%2Fims%2F6%2F8.png)

Si planea completar otros Los módulos de Azure Databricks, no elimines tu Azure Instancia de Databricks todavía. 
Puedes usar el mismo entorno para los demás módulos. Sin embargo, si desea eliminar el Azure Instancia de Databricks 
y, a continuación, complete el siguientes pasos. 

![s0.gif](modules%2F2%2Fims%2F6%2Fs0.gif)

Navegue hasta el portal de recursos y abrir grupos de recursos. Busque y seleccione el grupo de recursos requerido. 
Abra el requerido grupo de recursos y seleccione «Eliminar grupo de recursos» Escriba el nombre del grupo de recursos 
que desea eliminar para confirme la eliminación. Azure Studio confirma su confirmación y elimina el grupo de recursos 
seleccionado.

# 3 Spark Architecture Fundamentals

## INDEX 3:

- [3 Lesson introduction](#3-lesson-introduction)
- [3 Understand the Architecture of Azure Databricks Spark Cluster](#3-understand-the-architecture-of-azure-databricks-spark-cluster)
- [3 Understand the architecture of spark job](#3-understand-the-architecture-of-spark-job)
- [3 Knowledge check](#3-knowledge-check)
- [3 Test prep](#3-test-prep)
- [3 Lesson summary](#3-lesson-summary)

[< Back to index](#index-0)

## 3 Lesson introduction
[< Back to index 3](#index-3)

![1.png](modules%2F3%2Fims%2F1%2F1.png)

Bienvenido a la tercera y última lección de este módulo. En esta lección, se familiarizará con los fundamentos básicos de 
la arquitectura Spark. 

Al final de esta lección: 

- Comprenderá la arquitectura de un clúster Azure Databricks Spark
- Entenderá la arquitectura de un trabajo Spark. 

![2.png](modules%2F3%2Fims%2F1%2F2.png)

Comencemos con una visión general de el entorno Azure Databricks y las características de big data. Como ya sabrá por 
lecciones anteriores, Azure Databricks proporciona un entorno de espacio de trabajo Apache Spark as a service orientado 
a cuadernos. Es el servicio alojado con más funciones disponible para ejecutar cargas de trabajo Spark en Azure, un motor 
analítico unificado para el procesamiento de datos a gran escala y el aprendizaje automático. 

![3.png](modules%2F3%2Fims%2F1%2F3.png)

Veamos un ejemplo de cuándo la arquitectura Spark puede ser de ayuda. Supongamos que trabaja con big data como ingeniero 
de datos o científico de datos, su trabajo implicará el procesamiento de datos que contengan una o más de las siguientes 
características. 

- Alto volumen, procesando un volumen extremadamente grande de datos y escalando su equipo en consecuencia. 
- Alta velocidad, desplegando, streaming y capacidades de procesamiento en tiempo real. 
- Variedad, utilizando una gran variedad de tipos de datos, desde conjuntos de datos relacionales estructurados y transacciones financieras hasta datos no estructurados como mensajes de chat y SMS, dispositivos IoT, imágenes, registros y resonancias magnéticas. 

![4.png](modules%2F3%2Fims%2F1%2F4.png)

Estas características suelen denominarse las tres V de los big data. Cuando se trata de trabajar con big data de forma 
unificada, tanto si los procesa en tiempo real a medida que llegan o por lotes.

![5.png](modules%2F3%2Fims%2F1%2F5.png)

Apache Spark proporciona un motor rápido y capaz que también admite procesos de ciencia de datos como aprendizaje automático 
y análisis avanzados.

## 3 Understand the Architecture of Azure Databricks Spark Cluster
[< Back to index 3](#index-3)

![1.png](modules%2F3%2Fims%2F2%2F1.png)

Para comprender mejor cómo desarrollar con Azure Databricks, es importante entender primero la arquitectura subyacente. 
Dos aspectos que veremos incluyen el servicio Azure Databricks y Apache spark clusters. 

![2.png](modules%2F3%2Fims%2F2%2F2.png)

Desde un alto nivel el servicio Azure Databrick lanza y gestiona Apache spark clusters dentro de su suscripción Azure. 
Apache spark clusters o grupos de ordenadores que son tratados como un único ordenador y manejan la ejecución de comandos 
emitidos desde notebooks. 

Y utilizando una arquitectura de tipo master worker los clusters permiten que el procesamiento de datos se paralelice a 
través de muchos ordenadores con el fin de mejorar la escala y el rendimiento. 

Y el cluster Apache spark consiste en un controlador spark que se conoce como Master. En Databricks la interfaz del bloc 
de notas es el programa controlador o Master. Contiene el bucle principal para el programa y crea conjuntos de datos 
distribuidos en el cluster. 

![3.png](modules%2F3%2Fims%2F2%2F3.png)

Databricks también consiste en clusters spark o conjuntos de datos también conocidos como workers. Databrick suministra 
operaciones en forma de transformaciones y acciones a los clústeres. Normalmente, esto implica que el nodo controlador 
envíe trabajo o tareas a los nodos trabajadores. Esto asigna trabajo a las ranuras, ordenándoles que extraigan datos de 
una fuente de datos especificada. 

![4.png](modules%2F3%2Fims%2F2%2F4.png)

Los programas controladores acceden a Apache spark a través de un objeto de sesión spark independientemente de la ubicación 
de despliegue. 

![5.png](modules%2F3%2Fims%2F2%2F5.png)

Microsoft Azure gestiona el clúster autoescalándolo según sea necesario basándose tanto en su uso como en 
la configuración utilizada al configurar el clúster. 

![6.png](modules%2F3%2Fims%2F2%2F6.png)

También se puede habilitar la terminación automática. Esto permite a Azure terminar el clúster después de un número especificado 
de minutos de inactividad. 

![7.png](modules%2F3%2Fims%2F2%2F7.png)

Ahora echemos un vistazo más de cerca bajo el capó de la plataforma Databricks. Cuando usted crea un servicio Azure databricks, 
un dispositivo databricks desplegado como un recurso Azure en su suscripción y en el momento de la creación del cluster 
usted especifica los tipos y tamaños de las máquinas virtuales, también conocidas como VMS, a utilizar tanto para el 
controlador como para los nodos trabajadores. 

![8.png](modules%2F3%2Fims%2F2%2F8.png)

Pero como su Databricks gestiona todos los demás aspectos del clúster, también tiene la opción de utilizar un pool sin 
servidor. Un pool sin servidor es un pool autogestionado de recursos en la nube que se autoconfigura para cargas de trabajo 
interactivas de Spark. Usted proporciona el número mínimo y máximo de trabajadores y el tipo de trabajador y, a continuación, Databricks aprovisiona la computación y el almacenamiento local en función de su uso. 

![9.png](modules%2F3%2Fims%2F2%2F9.png)

El dispositivo Databricks se despliega en Azure como un grupo de recursos gestionados dentro de su suscripción. Este grupo 
de recursos contiene **el controlador** y **el VMS** trabajador junto con otros recursos necesarios, incluyendo 
**una red virtual** y **un grupo de seguridad** y **una cuenta de almacenamiento**. 

![10.png](modules%2F3%2Fims%2F2%2F10.png)

Todos los metadatos de su clúster, como los trabajos programados, se almacenan en una base de datos Azure con replicación 
Geo para tolerancia a fallos. Internamente se utiliza Azure Kubernetes Service o AKS para ejecutar el plano de control y 
los planos de datos de Azure Databricks a través de contenedores. 

![11.png](modules%2F3%2Fims%2F2%2F11.png)

Estos contenedores se ejecutan en la última generación de hardware Azure conocida como DV 3VMS. Este hardware se basa en 
memorias no volátiles exprés o SSD NVMe capaces de una latencia fulgurante de 100 microsegundos en E/S. Esto hace que el 
rendimiento de E/S de Databricks sea aún mejor. Y la red acelerada proporciona la infraestructura de red virtualizada más 
rápida en la nube. Azure Databricks utiliza estas características para mejorar aún más el rendimiento de chispa. 

![12.png](modules%2F3%2Fims%2F2%2F12.png)

Una vez que los servicios dentro de este grupo de recursos gestionados ya, a continuación, puede gestionar el clúster 
Databricks a través de la Azure databricks UI y a través de características tales como auto-escalado y auto-terminación. 
Los usuarios pueden integrar Databricks con una serie de servicios de Microsoft Azure

![13.png](modules%2F3%2Fims%2F2%2F13.png)

El plano de control se integra con el gestor de recursos de Azure, donde puede acceder a los servicios de almacenamiento, 
red informática y RP de Microsoft Databricks. Los usuarios también pueden interactuar con Databricks UX, que se integra 
con los clústeres y DBFS. Los clústeres y DBFS proporcionan acceso al espacio de trabajo de Databricks y desde aquí se 
integran con Vinet y el almacenamiento blob y el Gestor de recursos de Azure también puede integrarse con el espacio de 
trabajo de Databricks.

## 3 Understand the architecture of spark job
[< Back to index 3](#index-3)

![1.png](modules%2F3%2Fims%2F3%2F1.png)

En la unidad anterior, hablamos sobre los Azure Databricks en general y añadimos algo de contexto en torno a la arquitectura 
de un clúster de Spark. Antes de desglosarlos componentes en detalle, vamos a resumir los fundamentos de una arquitectura 
Spark.

![2.png](modules%2F3%2Fims%2F3%2F2.png)

Como ya sabrá, Spark es una computación distribuida el entorno y la unidad de distribución es un clúster de Spark. 
Cada clúster tiene un controlador y uno o más ejecutores o trabajadores, se denominan JVM o máquinas virtuales java. 

![3.png](modules%2F3%2Fims%2F3%2F3.png)

Trabajo presentado a el clúster se divide en otros tantos independientes trabajos según sea necesario. Así es como se 
distribuye el trabajo los clústeres, los nodos. Los trabajos enviados por el conductor se subdividen a su vez en tareas. 

La entrada a un trabajo se divide en una o más particiones, estas particiones son unidad de trabajo para cada ranura. Entre 
tareas, es posible que sea necesario reorganizar las particiones y compartido a través de la red. Vamos a explorar el 
agrupar con más detalle.

![4.png](modules%2F3%2Fims%2F3%2F4.png)

El secreto de Spark es increíble el rendimiento es el paralelismo. Esta es la capacidad de escalar tareas o trabajos 
horizontalmente a paralelizados o acciones distribuidas. Esto significa que podemos simplemente añadir nuevos nodos al 
clúster de forma casi infinita sin tener que preocuparse por las limitaciones de la memoria y potencia de procesamiento.

![5.png](modules%2F3%2Fims%2F3%2F5.png)

Anteriormente, habríamos tenido para escalar tareas verticalmente, pero el escalado vertical se limita a un número finito 
cantidad de RAM , subprocesos y velocidades de CPU. 

![6.png](modules%2F3%2Fims%2F3%2F6.png)

Paralelizamos en dos niveles. El primero de los cuales es el ejecutor, esta es una máquina virtual de Java o una JVM que 
se ejecuta en un nodo. Una JVM normalmente se ejecuta a una velocidad de una instancia por nodo.

![7.png](modules%2F3%2Fims%2F3%2F7.png)

El segundo nivel de la paralelización es la ranura, que tiene cada ejecutor varias ranuras a las que el conductor puede 
asignar tareas para ejecución en paralelo.

El número de ranuras es determinado por el número de núcleos y CPU de cada nodo. 

![8.png](modules%2F3%2Fims%2F3%2F8.png)

El controlador es la JVM en en el que se ejecuta nuestra aplicación. La JVM es, naturalmente tiene varios subprocesos, 
pero una sola JVM, como nuestro controlador, tiene un límite superior finito. 

Sin embargo, el controlador puede evitar estos límites mediante ranuras. El conductor asigna tareas a ranuras para su ejecución en paralelo con el fin de superar los límites de escalamiento vertical. 
En otras palabras, el conductor escala horizontalmente mediante ranuras. 

Dependiendo del trabajo requerido, se pueden crear varios puestos de trabajo, cada uno de los cuales puede ser desglosado 
en etapas.

![9.png](modules%2F3%2Fims%2F3%2F9.png)

 En otras palabras, el conductor escala horizontalmente mediante ranuras. Dependiendo del trabajo requerido, se pueden 
 crear varios puestos de trabajo, cada uno de los cuales puede ser desglosado en etapas.

![10.png](modules%2F3%2Fims%2F3%2F10.png)

Además, el conductor también debe decidir cómo particionar los datos para que pueda distribuirse para procesamiento en 
paralelo. En consecuencia, el controlador asigna una partición de datos a cada tarea. De esta manera, cada tarea sabe qué 
dato es procesar. Una vez iniciado, cada la tarea recuperará de la fuente de datos original la partición de los datos 
asignada a ella. Los resultados de cada trabajo son luego regresó al conductor.

![11.png](modules%2F3%2Fims%2F3%2F11.png)

Veamos cómo funciona Spark administra estos clústeres. En un nivel mucho más bajo, Spark Core emplea un administrador de 
clústeres responsable del aprovisionamiento nodos dentro del clúster. En cada uno de estos escenarios, el controlador se 
ejecuta en se ejecuta un nodo 

![12.png](modules%2F3%2Fims%2F3%2F12.png)

de cada uno de los ejecutores su propio nodo individual. 

![13.png](modules%2F3%2Fims%2F3%2F13.png)

Sin embargo, gracias a Azure Databricks, no es necesario preocúpese por la gestión de clústeres como Databricks proporciona 
una solución robusta y de alto rendimiento administrador de clústeres como parte de su oferta general. 

![14.png](modules%2F3%2Fims%2F3%2F14.png)

 Desde la perspectiva de un desarrollador, su enfoque principal debería estar en lo siguiente: 
 
- el número de particiones en la que sus datos estan divididos.
- el número de ranuras que tienes disponibles pare la ejecución paralela, 
- cuántos trabajos son seleccionados
- las etapas en la que los trabajos se dividen.

## 3 Knowledge check
[< Back to index 3](#index-3)

![1.png](modules%2F3%2Ftest%2F1.png)

![2.png](modules%2F3%2Ftest%2F2.png)

![3.png](modules%2F3%2Ftest%2F3.png)

![4.png](modules%2F3%2Ftest%2F4.png)

![5.png](modules%2F3%2Ftest%2F5.png)

![6.png](modules%2F3%2Ftest%2F6.png)

## 3 Test prep
[< Back to index 3](#index-3)

![q1.png](modules%2F3%2Ftest%2Fq1.png)

![q2.png](modules%2F3%2Ftest%2Fq2.png)

![q3.png](modules%2F3%2Ftest%2Fq3.png)

![q4.png](modules%2F3%2Ftest%2Fq4.png)

![q5.png](modules%2F3%2Ftest%2Fq5.png)

![q6.png](modules%2F3%2Ftest%2Fq6.png)

![q7.png](modules%2F3%2Ftest%2Fq7.png)

![q8.png](modules%2F3%2Ftest%2Fq8.png)

![q9.png](modules%2F3%2Ftest%2Fq9.png)

![q10.png](modules%2F3%2Ftest%2Fq10.png)

## 3 Lesson summary
[< Back to index 3](#index-3)

![1.png](modules%2F3%2Fims%2F4%2F1.png)

![2.png](modules%2F3%2Fims%2F4%2F2.png)

![3.png](modules%2F3%2Fims%2F4%2F3.png)

![4.png](modules%2F3%2Fims%2F4%2F4.png)

![5.png](modules%2F3%2Fims%2F4%2F5.png)



# 4 Use Azure Databricks to Prepare the Data for Advanced Analytics and Machine Learning Operations

## INDEX 4:

- [4 Lesson introduction](#4-lesson-introduction)
- [4 Read data in CSV format](#4-read-data-in-csv-format)
- [4 Read data in JSON format](#4-read-data-in-json-format)
- [4 Read data in Parquet format](#4-read-data-in-parquet-format)
- [4 Read data stored in tables and views](#4-read-data-stored-in-tables-and-views)
- [4 Write data](#4-write-data)
- [4 Exercise: Read and Write Data](#4-exercise-read-and-write-data)
- [4 Exercise quiz](#4-exercise-quiz)
- [4 Knowledge check](#4-knowledge-check)
- [4 Lesson summary](#4-lesson-summary)


[< Back to index](#index-0)

## 4 Lesson introduction
[< Back to index 4](#index-4)

![1.png](modules%2F4%2Fims%2F1%2F1.png)

Bienvenido al segundo módulo de este curso. Este módulo se centra en cómo leer y escribir datos utilizando Azure Databricks. 

En este módulo, usted; 

- utilizará Azure Databricks para leer múltiples tipos de archivos, tanto con como sin esquema 
- combinará entradas de archivos y almacenes de datos como Azure SQL Database
- transformará y almacenará esos datos para análisis avanzados. 

![2.png](modules%2F4%2Fims%2F1%2F2.png)

¿Por qué es importante poder leer y escribir datos con la plataforma Azure Databricks? Considere el siguiente ejemplo. 
Suponga que trabaja para una empresa emergente de análisis de datos que se está expandiendo junto con su creciente base 
de clientes, la empresa recibe grandes cantidades de datos de clientes de múltiples fuentes en diferentes formatos sin procesar. 

![3.png](modules%2F4%2Fims%2F1%2F3.png)

Para manejar eficientemente estos datos, la empresa invierte en Azure Databricks. Su equipo analiza cómo Databricks soporta 
las funciones cotidianas de manejo de datos como lecturas, escrituras y consultas. Su equipo también realiza estas lecturas, 
escrituras y consultas para preparar los datos para operaciones avanzadas de análisis y aprendizaje automático. 

![4.png](modules%2F4%2Fims%2F1%2F4.png)

Este módulo le ayudará a descubrir más sobre cómo leer, escribir, y consultar estas diversas formas de datos a través de 
una serie de lecturas y ejercicios. Necesitará una suscripción a Azure para acceder a los ejercicios. Si no dispone de 
una suscripción a Azure, visite azure.microsoft.com/free para crear una cuenta gratuita.


## 4 Read data in CSV format
[< Back to index 4](#index-4)

> Nota: En esta lectura puede ver los pasos que intervienen en el proceso de lectura de datos en formato CSV.

### Clone el archivo Databricks

1. Si actualmente no tiene abierto su espacio de trabajo Azure Databricks: en el portal Azure, navegue hasta su espacio de trabajo Azure Databricks desplegado y seleccioneLanzar espacio de trabajo.

   ![a1.gif](modules%2F4%2Fims%2F2%2Fa1.gif)

2. En el panel izquierdo, seleccione Espacio detrabajo > Usuarios y seleccione su nombre de usuario (la entrada con el icono 
de la casa).

3. En el panel que aparece, seleccione la flecha situada junto a su nombre y seleccione Importar.

4. En el cuadro de diálogoImportar cuadernos, seleccione la dirección URL y pegue la siguiente:
   
   > https://github.com/solliancenet/microsoft-learning-paths-databricks-notebooks/blob/master/data-engineering/DBC/03-Reading-and-writing-data-in-Azure-Databricks.dbc?raw=true

5. Seleccione **Importar**.

6. Seleccione la carpeta **03-Reading-and-writing-data-in-Azure-Databricks** que aparece.

### Complete el siguiente cuaderno

Abra el cuaderno [1.Reading Data - CSV.ipynb](modules%2F4%2Fnotebooks%2F1.Reading%20Data%20-%20CSV.ipynb) Asegúrese de adjuntar su cluster al cuaderno antes de seguir las 
instrucciones y ejecutar las celdas que contiene.

![a2.gif](modules%2F4%2Fims%2F2%2Fa2.gif)

Dentro del cuaderno, usted

- Empezará a trabajar con la documentación de la API

- Introducir la claseSparkSession y otros puntos de entrada

- Introducir la claseDataFrameReader

- Leer datos de:
  - CSV sin un esquema
  - CSV con un esquema

Cuando haya completado el cuaderno, vuelva a esta pantalla y continúe con el siguiente paso.

## 4 Read data in JSON format
[< Back to index 4](#index-4)

> Nota: En esta lectura puede ver los pasos implicados en el proceso de lectura de datos en formato JSON.

En su espacio de trabajo Azure Databricks, abra la carpeta03-Reading-and-writing-data-in-Azure-Databricks que importó dentro de su carpeta de usuario.

Abra el cuaderno2.[2.Reading Data - JSON.ipynb](modules%2F4%2Fnotebooks%2F2.Reading%20Data%20-%20JSON.ipynb). Asegúrese de conectar su cluster al cuaderno antes de seguir las instrucciones y ejecutar las celdas que contiene.

Dentro del cuaderno, leerá datos de:

- JSON sin un esquema

- JSON con un esquema

Cuando haya completado el cuaderno, vuelva a esta pantalla y continúe con el siguiente paso.

## 4 Read data in Parquet format
[< Back to index 4](#index-4)

> Nota: En esta lectura puede ver los pasos implicados en el proceso de lectura de datos en formato Parquet.

En su espacio de trabajo Azure Databricks, abra la carpeta03-Reading-and-writing-data-in-Azure-Databricks que importó dentro de su carpeta de usuario.

Abra el cuaderno 3.[3.Reading Data - Parquet.ipynb](modules%2F4%2Fnotebooks%2F3.Reading%20Data%20-%20Parquet.ipynb). Asegúrese de adjuntar su cluster al cuaderno antes de seguir las instrucciones y ejecutar las celdas que contiene.

Dentro del cuaderno, usted

Introducir el formato de archivo Parquet

Leer datos de:
- Archivos Parquet sin un esquema
- Archivos Parquet con un esquema

Cuando haya completado el cuaderno, vuelva a esta pantalla y continúe con el paso siguiente.

## 4 Read data stored in tables and views
[< Back to index 4](#index-4)

> Nota: En esta lectura puede ver los pasos implicados en el proceso de lectura de datos almacenados en tablas y vistas.

En su espacio de trabajo Azure Databricks, abra la carpeta03-Reading-and-writing-data-in-Azure-Databricks que importó dentro de su carpeta de usuario.

Abra el cuaderno4.[4.Reading Data - Tables and Views.ipynb](modules%2F4%2Fnotebooks%2F4.Reading%20Data%20-%20Tables%20and%20Views.ipynb). Asegúrese de adjuntar su cluster al cuaderno antes de seguir las instrucciones y ejecutar las celdas que contiene.

Dentro del cuaderno, usted

- Demostrará cómo registrar previamente fuentes de datos en Azure Databricks

- Introducir vistas temporales sobre archivos

- Leerá datos de tablas/vistas

Una vez que haya completado el cuaderno, vuelva a esta pantalla y continúe con el siguiente paso.

## 4 Write data
[< Back to index 4](#index-4)

> Nota: En esta lectura puede ver los pasos del proceso de escritura de datos.

En su espacio de trabajo AzureDatabricks, abra la carpeta03-Reading-and-writing-data-in-Azure-Databricks que importó dentro de su carpeta de usuario.

Abra el cuaderno 5.[5.Writing Data.ipynb](modules%2F4%2Fnotebooks%2F5.Writing%20Data.ipynb). Asegúrese de adjuntar su cluster al cuaderno antes de seguir las instrucciones y ejecutar las celdas que contiene.

Dentro del cuaderno, usted hará lo siguiente

- Escribirá datos en un archivo Parquet

- Leerá de nuevo el archivo Parquet y mostrará los resultados

Cuando haya completado el cuaderno, vuelva a esta pantalla y continúe con el siguiente paso.

## 4 Exercise: Read and Write Data
[< Back to index 4](#index-4)

En su espacio de trabajo AzureDatabricks, abra la carpeta03-Reading-and-writing-data-in-Azure-Databricks que importó dentro de su carpeta de usuario.

Abra el cuaderno6. [6.Reading Data - Lab.ipynb](modules%2F4%2Fnotebooks%2F6.Reading%20Data%20-%20Lab.ipynb). Asegúrese de adjuntar su cluster al cuaderno antes de seguir las instrucciones y ejecutar las celdas que contiene.

El objetivo de este ejercicio es poner en práctica parte de lo que ha aprendido sobre la lectura de datos con Apache Spark. Las instrucciones se proporcionan dentro del cuaderno, junto con celdas vacías para que realice su trabajo. En la parte inferior del cuaderno hay celdas adicionales que le ayudarán a verificar que su trabajo es correcto.

> Nota: Encontrará un cuaderno [Solution - Reading Data 8 - Lab.ipynb](modules%2F4%2Fnotebooks%2FSolution%20-%20Reading%20Data%208%20-%20Lab.ipynb) correspondiente dentro de la subcarpetaSolutions. Éste contiene las celdas completadas para el ejercicio. Consulte el cuaderno si se queda atascado o simplemente quiere ver la solución.

Cuando haya completado el cuaderno, vuelva a esta pantalla y continúe con el siguiente paso.

## 4 Exercise quiz
[< Back to index 4](#index-4)

![0.png](modules%2F4%2Ftest%2F0.png)

## 4 Knowledge check
[< Back to index 4](#index-4)

![q1.png](modules%2F4%2Ftest%2Fq1.png)

![q2.png](modules%2F4%2Ftest%2Fq2.png)

## 4 Lesson summary
[< Back to index 4](#index-4)

![1.png](modules%2F4%2Fims%2F3%2F1.png)

En esta lección, ha explorado cómo leer y escribir datos en Azure Databricks. Tómese un momento para repasar lo que ha 
aprendido. Leer y escribir datos en Azure Databricks requiere estar familiarizado con los formatos de archivo CSV, JSON y 
Parquet. 

![2.png](modules%2F4%2Fims%2F3%2F2.png)

También es importante poder escribir archivos Parquet en el sistema de archivos Databricks o DBFS con opciones de compresión, 
y el mismo método dataframe.write puede utilizarse para dar salida a otros formatos. Ha completado este módulo y ahora sabe 
cómo leer y escribir datos en Azure Databricks. Bien hecho.

# 5 Work with DataFrames in Azure Databricks

## INDEX 5:

- [5 Lesson introduction](#5-lesson-introduction)
- [5 Describe a DataFrame](#5-describe-a-dataframe)
- [5 Use common DataFrame Methods](#5-use-common-dataframe-methods)
- [5 Use the display function](#5-use-the-display-function)
- [5 Exercise: Distinct Articles](#5-exercise-distinct-articles)
- [5 Knowledge check](#5-knowledge-check)
- [5 Test prep](#5-test-prep)
- [5 Lesson summary](#5-lesson-summary)


[< Back to index](#index-0)

## 5 Lesson introduction
[< Back to index 5](#index-5)

![1.png](modules%2F5%2Fims%2F1%2F1.png)

Bienvenido al tercer módulo de este curso. La primera lección explora el trabajo con DataFrames y Azure Databricks. En 
esta lección, utilizará el método count para contar filas en un DataFrame. Utilice la función display para mostrar un 
DataFrame en el cuaderno. Almacene en caché un DataFrame para realizar operaciones más rápidas si se necesitan los datos 
por segunda vez, y utilice la función limitar para mostrar un pequeño conjunto de filas de un DataFrame más grande.

![2.png](modules%2F5%2Fims%2F1%2F2.png)

 También utilizará la función seleccionar para seleccionar un subconjunto de columnas de un DataFrame. Utilice las funciones 
 distinct y drop duplicates para filtrar los datos duplicados, y utilice el método drop para eliminar columnas de un DataFrame. 

![3.png](modules%2F5%2Fims%2F1%2F3.png)

 ¿Por qué es importante poder trabajar con DataFrames en la plataforma Azure Databricks? Considere el siguiente ejemplo. 
 Suponga que trabaja para una startup de análisis de datos que se está expandiendo junto con su creciente base de clientes, 
 la empresa recibe grandes cantidades de datos de clientes de múltiples fuentes en diferentes formatos sin procesar.

![4.png](modules%2F5%2Fims%2F1%2F4.png)

Para manejar eficientemente estos datos la empresa invierte en Azure Databricks. Su equipo es responsable de analizar 
cómo Databricks soporta las funciones diarias de manejo de datos como lecturas, escrituras y consultas. Usted desea ir 
más allá de las simples funciones de manejo de datos y descubrir cómo simplificar la exploración de datos y realizar 
transformaciones de datos. Las tareas de exploración y transformación de datos ayudan a su equipo a realizar análisis 
avanzados y aprendizaje automático en etapas posteriores de la canalización de datos. 

![5.png](modules%2F5%2Fims%2F1%2F5.png)

Esta lección le ayudará a descubrir más sobre cómo trabajar con DataFrames a través de funciones y transformaciones. 
Explorará estas funciones y transformaciones con una serie de lecturas y ejercicios. 

## 5 Describe a DataFrame
[< Back to index 5](#index-5)

Abra el cuaderno1. [1.Describe-a-dataframe.ipynb](modules%2F5%2Fnotebooks%2F1.Describe-a-dataframe.ipynb). Asegúrese de adjuntar su cluster al cuaderno antes de seguir las instrucciones y ejecutar las celdas que contiene.

Dentro del cuaderno, usted

Familiarizarse con las API deDataFrame

Aprenderá las clases...
SparkSession
DataFrame(también conocidas comoDataset[Row])

Aprenderá las acciones...
count()

Cuando haya completado el cuaderno, vuelva a esta pantalla y continúe con el siguiente paso.

## 5 Use common DataFrame Methods
[< Back to index 5](#index-5)

Nota: En esta lectura puede ver los pasos implicados en el proceso de utilización de los métodos comunes de DataFrame.

En su espacio de trabajo Azure Databricks, abra la carpeta04-Working-With-Dataframes que importó dentro de su carpeta de usuario.

Abra el cuaderno2.[2.Use-common-dataframe-methods.ipynb](modules%2F5%2Fnotebooks%2F2.Use-common-dataframe-methods.ipynb). Asegúrese de adjuntar su cluster al cuaderno antes de seguir las instrucciones y ejecutar las celdas que contiene.

Dentro del cuaderno, usted

- Familiarizarse con las API deDataFrame

- Utilizará métodos comunes de DataFrame para mejorar el rendimiento

- Explorará la documentación de las API de Spark

Cuando haya completado el cuaderno, vuelva a esta pantalla y continúe con el siguiente paso.

## 5 Use the display function
[< Back to index 5](#index-5)

Nota: En esta lectura puede ver los pasos implicados en el proceso de utilización de la función de visualización.

En su espacio de trabajo Azure Databricks, abra la carpeta04-Working-With-Dataframes que importó dentro de su carpeta de usuario.

Abra el cuaderno3. [3.Display-function.ipynb](modules%2F5%2Fnotebooks%2F3.Display-function.ipynb). Asegúrese de adjuntar su cluster al cuaderno antes de seguir las instrucciones y ejecutar las celdas que contiene.

Dentro del cuaderno, aprenderá las transformaciones

- limit(..)

- select(..)

- drop(..)

- distinct()

- dropDuplicates(..)

y aprenderá las acciones:

- show(..)

- display(..)

Una vez que haya completado el cuaderno, vuelva a esta pantalla y continúe con el siguiente paso.

## 5 Exercise: Distinct Articles
[< Back to index 5](#index-5)

En su espacio de trabajo Azure Databricks, abra la carpeta04-Working-With-Dataframes que importó dentro de su carpeta de usuario.

Abra el cuaderno 4.Ejercicio: [Solution - Exercise_ Distinct Articles.ipynb](modules%2F5%2Fnotebooks%2FSolution%20-%20Exercise_%20Distinct%20Articles.ipynb). Asegúrese de adjuntar su cluster al cuaderno antes de seguir las instrucciones y ejecutar las celdas que contiene.

En este ejercicio, leerá los archivos Parquet, aplicará las transformaciones necesarias, realizará un recuento total de registros y, a continuación, comprobará que todos los datos se han cargado correctamente. 

Como extra, intente definir un esquema que se ajuste a los datos y actualice la operación de lectura para utilizar el esquema.

> Nota: Encontrará un cuaderno correspondiente dentro de la subcarpetaSolutions. Éste contiene las celdas completadas para el ejercicio. Consulte el cuaderno si se queda atascado o simplemente quiere ver la solución.

Cuando haya completado el cuaderno, vuelva a esta pantalla y continúe con el siguiente paso.



## 5 Knowledge check
[< Back to index 5](#index-5)

![q1.png](modules%2F5%2Fims%2Ftest%2Fq1.png)

![q2.png](modules%2F5%2Fims%2Ftest%2Fq2.png)

![q3.png](modules%2F5%2Fims%2Ftest%2Fq3.png)

## 5 Test prep
[< Back to index 5](#index-5)

![t1.png](modules%2F5%2Fims%2Ftest%2Ft1.png)

![t2.png](modules%2F5%2Fims%2Ftest%2Ft2.png)

![t3.png](modules%2F5%2Fims%2Ftest%2Ft3.png)

![t4.png](modules%2F5%2Fims%2Ftest%2Ft4.png)

![t5.png](modules%2F5%2Fims%2Ftest%2Ft5.png)

## 5 Lesson summary
[< Back to index 5](#index-5)

![1.png](modules%2F5%2Fims%2F2%2F1.png)

En esta lección, aprendiste cómo trabaje con marcos de datos, columnas y Azure Databricks. Repasemos la clave puntos de esta lección. Hay una sintaxis específica para especificar los valores de las columnas para el filtrado y las agregaciones. 

![2.png](modules%2F5%2Fims%2F2%2F2.png)

 Es importante que todos los usuarios de Azure Databricks entiende cómo usar correctamente la clase de columna. Todos los usuarios también deben saber cómo ordenar y filtrar un marco de datos en función de los valores de las columnas.

![3.png](modules%2F5%2Fims%2F2%2F3.png)

Por último, los usuarios de Azure Databricks deben saber cómo usar, recopilar y realizar transformaciones para regresar graba desde un marco de datos y los envía al controlador del clúster. Ha llegado al final de esta lección sobre cómo trabajar con columnas de marcos de datos y Azure Databricks. Bien hecho. 

# 6 Build and Query a Delta Lake

## INDEX 6:

- [6 Describe the open source Delta Lake](#6-describe-the-open-source-delta-lake)
- [6 Get started with Delta using Spark APIs](#6-get-started-with-delta-using-spark-apis)
- [6 Exercise: Work with basic Delta Lake functionality](#6-exercise-work-with-basic-delta-lake-functionality)
- [6 Exercise quiz 1](#6-exercise-quiz-1)
- [6 Describe how Azure Databricks manages Delta Lake](#6-describe-how-azure-databricks-manages-delta-lake)
- [6 Exercise: Use the Delta Lake Time Machine and perform optimization](#6-exercise-use-the-delta-lake-time-machine-and-perform-optimization)
- [6 Exercise quiz 2](#6-exercise-quiz-2)
- [6 Knowledge check](#6-knowledge-check)
- [6 Lesson summary](#6-lesson-summary)

[< Back to index](#index-0)

## 6 Describe the open source Delta Lake
[< Back to index 6](#index-6)

![1.png](modules%2F6%2Fims%2F1%2F1.png)

Bienvenido a la lección sobre construcción y consulta de un Delta Lake. Después de completar este módulo, usted será capaz 
de describir las características clave y casos de uso de Delta Lake. Utilice Delta Lake para crear, anexar y tablas absurdas, 
realizar optimizaciones en Delta Lake y comparar diferentes versiones de una tabla Delta utilizando la máquina del tiempo.

![2.png](modules%2F6%2Fims%2F1%2F2.png)

 ¿Por qué es importante saber cómo construir y consultar un Delta Lake? Considere un escenario en el que usted trabaja 
 como ingeniero de datos o científico de datos para una gran tienda minorista. Su organización utiliza Azure Data Lake 
 para almacenar todos sus datos de compras en línea. 

![3.png](modules%2F6%2Fims%2F1%2F3.png)

Sin embargo, a medida que aumenta el volumen de datos, actualizar y consultar la información del almacenamiento es cada 
vez más lento. Su responsabilidad es investigar el problema y encontrar una solución. Necesita una solución que se adapte 
a Data Lake en escalabilidad pero que también sea fiable y rápida. Delta Lake puede resolver su problema.

![4.png](modules%2F6%2Fims%2F1%2F4.png)

 Delta Lake es un formato de archivo que se integra con Spark y tiene ofertas tanto de código abierto como gestionadas. 


![5.png](modules%2F6%2Fims%2F1%2F5.png)

 Delta Lake se proporciona como oferta gestionada como parte de su cuenta Azure Databricks y le ayuda a combinar las mejores 
 capacidades de data lake, data warehousing y un sistema de ingestión de streaming. 

![6.png](modules%2F6%2Fims%2F1%2F6.png)

Al final de esta lección, habrá desarrollado los conjuntos de habilidades necesarios para construir y consultar un Delta Lake. 
Comencemos describiendo la oferta de código abierto de Delta Lake. Delta Lake es una capa de almacenamiento transaccional 
diseñada específicamente para trabajar con Apache Spark y el sistema de archivos de Databricks, DBFS.  Mantiene un registro de transacciones que realiza un seguimiento eficiente de los cambios en la tabla. 

![7.png](modules%2F6%2Fims%2F1%2F7.png)

Un lago de datos es un repositorio de almacenamiento que almacena de forma económica una gran cantidad de datos en bruto 
tanto actuales como históricos. Los datos se almacenan en formatos nativos como XML, JSON, CSV y Parquet. 

Puede contener bases de datos relacionales operativas con datos transaccionales en vivo. 

![8.png](modules%2F6%2Fims%2F1%2F8.png)

Las empresas gastan millones de dólares en introducir datos en los lagos de datos con Apache Spark. La aspiración es hacer 
ciencia de datos y aprendizaje automático en todos esos datos usando Apache Spark. 

![9.png](modules%2F6%2Fims%2F1%2F9.png)

Pero los datos no están listos para ciencia de datos y aprendizaje automático. La mayoría de estos proyectos están fracasando debido a datos poco fiables. ¿Por qué estos proyectos luchan con la fiabilidad y el rendimiento? El reto consiste en extraer información significativa de los lagos de datos. 

![10.png](modules%2F6%2Fims%2F1%2F10.png)

 Para hacerlo de forma fiable, debe resolver problemas como la aplicación del esquema cuando se introducen nuevas tablas, 
 la reparación de tablas cuando se inserta cualquier dato nuevo en el lago de datos, la actualización frecuente de metadatos, 
 los cuellos de botella de los archivos de pequeño tamaño para los cálculos distribuidos y la dificultad para ordenar 
 los datos mediante un índice si los datos están repartidos en muchos archivos y particionados.

![11.png](modules%2F6%2Fims%2F1%2F11.png)

Los lagos de datos también plantean problemas de fiabilidad de los datos. Los trabajos de producción fallidos dejan los 
datos en un estado corrupto que requiere una recuperación tediosa. La falta de esquema y de aplicación crea datos 
incoherentes y de baja calidad. La falta de coherencia hace casi imposible mezclar anexos y lecturas, lotes y streaming.

![12.png](modules%2F6%2Fims%2F1%2F12.png)

Por muy buenos que sean los lagos de datos para almacenar de forma económica nuestros datos en bruto, también conllevan 
problemas de rendimiento. Demasiados archivos pequeños o muy grandes significa más tiempo abriendo y cerrando archivos 
en lugar de leyendo contenidos. Esto es aún peor con el streaming. El particionamiento, también conocido como indexación 
de los pobres, se rompe si se eligen los campos equivocados o cuando los datos tienen muchas dimensiones, columnas de 
alta cardinalidad. No hay almacenamiento en caché. El rendimiento del almacenamiento en la nube es bajo. El almacenamiento 
de objetos en la nube es de 20-50 megabits por segundo núcleo frente a los 300 megabits por segundo núcleo de una fuente local. 

![13.png](modules%2F6%2Fims%2F1%2F13.png)

Delta Lake es un formato de archivo que puede ayudarle a construir un lago de datos compuesto por una o muchas tablas en 
formato Delta Lake. Delta Lake se integra estrechamente con Apache Spark y utiliza un formato abierto que se basa en 
Parquet. Al tratarse de un formato de código abierto, Delta Lake también es compatible con otras plataformas de datos, 
incluida Azure Synapse Analytics. 

![14.png](modules%2F6%2Fims%2F1%2F14.png)

Delta Lake se especializa en preparar los datos para el análisis. Delta Lake es una capa de almacenamiento de código 
abierto que aporta transacciones ácidas a Apache Spark y a las cargas de trabajo de big data. 

![15.png](modules%2F6%2Fims%2F1%2F15.png)

 Puede leer y escribir datos que estén almacenados en Delta Lake utilizando las API de flujo y lote SQL de Apache Spark. 
 Se trata de las mismas API conocidas que utiliza para trabajar con tablas Hive o directorios DBFS. 

![16.png](modules%2F6%2Fims%2F1%2F16.png)

Echemos un vistazo a la funcionalidad que proporciona Delta Lake. Los lagos de datos suelen tener múltiples conductos 
de datos, que leen y escriben datos de forma concurrente. Los ingenieros de datos deben pasar por un tedioso proceso 
para garantizar la integridad de los datos debido a la falta de transacciones.

![17.png](modules%2F6%2Fims%2F1%2F17.png)

 Delta Lake aporta transacciones ácidas a sus lagos de datos. Proporciona serializabilidad que es el nivel de aislamiento 
 más fuerte. En big data, incluso los propios metadatos pueden ser big data.

![18.png](modules%2F6%2Fims%2F1%2F18.png)

Delta Lake puede aprovechar el manejo escalable de metadatos que trata los metadatos igual que los datos. Para ello, 
aprovecha la potencia de procesamiento distribuida de Sparks para manejar todos sus metadatos. Como resultado, Delta Lake 
puede manejar eficazmente tablas a escala de petabytes con miles de millones de particiones y archivos. 

![19.png](modules%2F6%2Fims%2F1%2F19.png)

 La capacidad de deshacer un cambio o volver a una versión anterior es una de las características clave de las transacciones.

![20.png](modules%2F6%2Fims%2F1%2F20.png)

 Con su función de viaje en el tiempo, Delta Lake proporciona instantáneas de los datos que le permiten volver a versiones anteriores de los datos para auditorías, rollbacks, o para reproducir experimentos.

![21.png](modules%2F6%2Fims%2F1%2F21.png)

Apache Parquet es el formato de base para Delta Lake, permitiéndole aprovechar los eficientes esquemas de compresión y codificación que son nativos del formato.

![22.png](modules%2F6%2Fims%2F1%2F22.png)

Una tabla en Delta Lake es tanto una tabla por lotes como una fuente y un sumidero de flujo. 

![23.png](modules%2F6%2Fims%2F1%2F23.png)

 La ingesta de datos de flujo, el relleno histórico por lotes y las consultas interactivas todo funciona sin más. 

![24.png](modules%2F6%2Fims%2F1%2F24.png)

La aplicación de esquemas ayuda a garantizar que los tipos de datos son correctos y las columnas requeridas están presentes, evitando que los datos erróneos causen incoherencias en los datos.

![25.png](modules%2F6%2Fims%2F1%2F25.png)

Delta Lake le permite realizar cambios en el esquema de una tabla que pueden aplicarse automáticamente sin tener que escribir DDL de migración.

![26.png](modules%2F6%2Fims%2F1%2F26.png)

El registro de transacciones de Delta Lake registra detalles sobre cada cambio realizado en los datos proporcionando una pista de auditoría completa de los cambios

![27.png](modules%2F6%2Fims%2F1%2F27.png)

 Delta Lake es compatible con las API de Scala, Java, Python y SQL para una gran variedad de funcionalidades. La compatibilidad con operaciones de fusión, actualización, y eliminación le ayuda a cumplir los requisitos de conformidad. 

![28.png](modules%2F6%2Fims%2F1%2F28.png)

Los desarrolladores pueden utilizar Delta Lake con sus canalizaciones de datos existentes con cambios mínimos, ya que es totalmente compatible con las implementaciones existentes de Spark.

![29.png](modules%2F6%2Fims%2F1%2F29.png)

Para una documentación completa de las principales características, consulte la página de documentación de Delta Lake y el proyecto Delta Lake en GitHub.

![30.png](modules%2F6%2Fims%2F1%2F30.png)

Al final de este módulo, hay una lectura de recursos adicionales que le ofrece todos los enlaces que necesita, incluidos enlaces a fragmentos de código para los comandos DML de fusión, actualización y eliminación.

## 6 Get started with Delta using Spark APIs
[< Back to index 6](#index-6)

> Nota: En esta lectura podrá ver los pasos necesarios en el proceso de configuración de Delta mediante las API de Spark.

Delta Lake se incluye con Azure Databricks. Puede empezar a utilizarlo hoy mismo. Para empezar rápidamente con Delta Lake, haga lo siguiente:

En lugar deparquet...


```commandline
CREATE TABLE ...
USING parquet
...

dataframe
    .write
    .format("parquet")
    .save("/data")
```

diga simplemente delta

```commandline
CREATE TABLE ...
USING parquet
...

dataframe
    .write
    .format("delta")
    .save("/data")
```

### Utilización de Delta con sus tablas Parquet existentes

Paso 1: ConviertaParquet en tablasDelta:

```commandline
CONVERT TO DELTA parquet.`path/to/table` [NO STATISTICS]
[PARTITIONED BY (col_name1 col_type1, col_name2 col_type2, ...)]
```

Paso 2: Optimizar el diseño para consultas rápidas:

```commandline
OPTIMIZE events
WHERE date >= current_timestamp() - INTERVAL 1 day
ZORDER BY (eventType)
```

### Sintaxis básica

Dos de las características principales de Delta Lake son la realización de upserts (inserciones/actualizaciones) y las operaciones de Time Travel. Exploraremos más estos conceptos en los cuadernos de este módulo.

UPSERT significa "UPdate" (actualizar) e "inSERT" (insertar). En otras palabras, UPSERT son literalmente DOS operaciones. No está soportado en los lagos de datos tradicionales, ya que ejecutar un UPDATE podría invalidar los datos a los que accede la operación INSERT posterior.

Con Delta Lake, sin embargo, podemos hacer UPSERTS. Delta Lake combina estas operaciones para garantizar la atomicidad para

- INSERTAR una fila

- si la fila ya existe, UPDATE la fila.

### Sintaxis Upsert

Upserting, o fusión, en Delta Lake proporciona actualizaciones de grano fino de sus datos. La siguiente sintaxis muestra cómo realizar un Upsert:

```sql
MERGE INTO customers -- Delta table
USING updates
ON customers.customerId = source.customerId
WHEN MATCHED THEN
    UPDATE SET address = updates.address
WHEN NOT MATCHED
    THEN INSERT (customerId, address) VALUES (updates.customerId, updates.address)
```

Consulte
[la documentación sobre sintaxis de actualización de datos de tablas](https://docs.delta.io/latest/quick-start.html#update-table-data)

### Sintaxis de desplazamiento en el tiempo

Debido a que Delta Lake está controlado por versiones, usted tiene la opción de consultar versiones pasadas de los datos. 

Utilizando un único sistema de almacenamiento de archivos, ahora tiene acceso a varias versiones de sus datos históricos, lo que garantiza que sus analistas de datos podrán replicar sus informes (y comparar los cambios agregados a lo largo del tiempo) y sus científicos de datos podrán replicar sus experimentos.

Otros casos de uso del viaje en el tiempo son:

- Re-creación de análisis, informes o salidas (por ejemplo, la salida de un modelo de aprendizaje automático). Esto podría ser útil para depurar o auditar, especialmente en industrias reguladas.

- Escribir consultas temporales complejas.

- Corregir errores en los datos.

- Proporcionar aislamiento instantáneo a un conjunto de consultas para tablas que cambian rápidamente.

Ejemplo de uso del viaje en el tiempo para reproducir experimentos e informes:

```commandline
SELECT count(*) FROM events
TIMESTAMP AS OF timestamp

SELECT count(*) FROM events
VERSION AS OF version
```

```commandline
spark.read.format("delta").option("timestampAsOf", timestamp_string).load("/events/")
```

Si necesita revertir escrituras accidentales o erróneas:

```commandline
INSERT INTO my_table
    SELECT * FROM my_table TIMESTAMP AS OF
    date_sub( current_date(), 1)
```

Consulte
[la documentación sobre la sintaxis](https://docs.delta.io/latest/quick-start.html#read-older-versions-of-data-using-time-travel)
 de los viajes en el tiempo.

## 6 Exercise: Work with basic Delta Lake functionality
[< Back to index 6](#index-6)

En su espacio de trabajo Azure Databricks, abra la carpeta09-Building-And-Querying-A-Delta-Lake que importó dentro de su carpeta de usuario.

Abra el cuaderno2.[1.Open-Source-Delta-Lake.ipynb](modules%2F6%2Fnotebooks%2F1.Open-Source-Delta-Lake.ipynb). Asegúrese de adjuntar su cluster al cuaderno antes de seguir las instrucciones y ejecutar las celdas que contiene.

En este ejercicio, usted:

- Creará un nuevo lago Delta a partir de los datos agregados de un lago Delta existente.

- Insertará registros en un lago Delta.

- Añadir nuevos datos a un lago Delta existente.

Las instrucciones se proporcionan dentro del cuaderno, junto con celdas vacías para que realice su trabajo. En la parte inferior del cuaderno hay celdas adicionales que le ayudarán a verificar que su trabajo es correcto.

> Nota: Encontrará un cuaderno [Solution - Delta-Lake-Basics-Lab-1.ipynb](modules%2F6%2Fnotebooks%2FSolution%20-%20Delta-Lake-Basics-Lab-1.ipynb) correspondiente dentro de la subcarpetaSolutions. Éste contiene las celdas completadas para el ejercicio. Consulte el cuaderno si se queda atascado o simplemente quiere ver la solución.

Cuando haya completado el cuaderno, vuelva a esta pantalla y continúe con el siguiente paso.


## 6 Exercise quiz 1
[< Back to index 6](#index-6)

![1.png](modules%2F6%2Fims%2F2%2F1.png)

## 6 Describe how Azure Databricks manages Delta Lake
[< Back to index 6](#index-6)

> Nota: En esta lectura puede ver los pasos implicados en el proceso de utilización de Azure Databricks para gestionar Data Lake.

En su espacio de trabajo Azure Databricks, abra la carpeta09-Building-And-Querying-A-Delta-Lake que importó dentro de su carpeta de usuario.

Abra el cuaderno3. [3.Managed-Delta-Lake.ipynb](modules%2F6%2Fnotebooks%2F3.Managed-Delta-Lake.ipynb). Asegúrese de adjuntar su cluster al cuaderno antes de seguir las instrucciones y ejecutar las celdas que contiene.

Dentro del cuaderno, descubrirá las características clave de Delta Lake que permiten la optimización de las consultas y la recogida de basura, lo que se traduce en una mejora del rendimiento.

Una vez que haya completado el cuaderno, vuelva a esta pantalla y continúe con el siguiente paso.

## 6 Exercise: Use the Delta Lake Time Machine and perform optimization
[< Back to index 6](#index-6)

En su espacio de trabajo Azure Databricks, abra la carpeta09-Building-And-Querying-A-Delta-Lake que importó dentro de su carpeta de usuario.

Abra el cuaderno4.[Solution - Delta-Time-Machine-and-Optimization-Lab-2.ipynb](modules%2F6%2Fnotebooks%2FSolution%20-%20Delta-Time-Machine-and-Optimization-Lab-2.ipynb). Asegúrese de adjuntar su cluster al cuaderno antes de seguir las instrucciones y ejecutar las celdas que contiene.

En este ejercicio, usted:

- Comparará diferentes versiones de una tabla Delta utilizando Time Machine.

- Optimizará su lago Delta para aumentar la velocidad y reducir el número de archivos.

Las instrucciones se proporcionan dentro del cuaderno, junto con celdas vacías para que realice su trabajo. En la parte inferior del cuaderno hay celdas adicionales que le ayudarán a verificar que su trabajo es correcto.

> Nota: Encontrará un cuaderno correspondiente dentro de la subcarpetaSolutions. Éste contiene las celdas completadas para el ejercicio. Consulte el cuaderno si se queda atascado o simplemente quiere ver la solución.

Cuando haya completado el cuaderno, vuelva a esta pantalla y continúe con el siguiente paso.


## 6 Exercise quiz 2
[< Back to index 6](#index-6)

![2.png](modules%2F6%2Fims%2F2%2F2.png)

## 6 Knowledge check
[< Back to index 6](#index-6)

![q1.png](modules%2F6%2Ftest%2Fq1.png)

![q2.png](modules%2F6%2Ftest%2Fq2.png)

![q3.png](modules%2F6%2Ftest%2Fq3.png)

![q4.png](modules%2F6%2Ftest%2Fq4.png)


## 6 Lesson summary
[< Back to index 6](#index-6)

![3.png](modules%2F6%2Fims%2F2%2F3.png)

Ahora ya sabe cómo construir y limpiar un Delta Lake. Si busca un sistema de gestión de datos que sea rápido, fiable y 
capaz de manejar grandes volúmenes de datos en diferentes formatos rob, Delta Lake es la solución.

![4.png](modules%2F6%2Fims%2F2%2F4.png)

Delta Lake se especializa en preparar los datos para el análisis. Le proporciona lo mejor de los sistemas de lago de datos, almacenamiento de datos e ingestión de datos en streaming, bien hecho. Está en camino de dominar Delta Lake. 

![5.png](modules%2F6%2Fims%2F2%2F5.png)

En la próxima lección, verá cómo procesar datos en batch y en streaming y aprenderá cómo la arquitectura de Delta Lake permite la analítica unificada en streaming y en batch. 

# 7 Work with user-defined functions

## INDEX 7:

- [Lesson introduction](#7-lesson-introduction)
- [Write user-defined functions](#7-write-user-defined-functions)
- [Exercise: Perform Extract, Transform, Load (ETL) operations using user-defined functions](#7-exercise-perform-extract-transform-load-etl-operations-using-user-defined-functions)
- [Exercise quiz](#7-exercise-quiz)
- [Knowledge check](#7-knowledge-check)
- [Test prep](#7-test-prep)
- [Lesson summary](#7-lesson-summary)
- [Additional resources](#7-additional-resources)


[< Back to index](#index-0)

## 7 Lesson introduction
[< Back to index 7](#index-7)

![1.png](modules%2F7%2Fims%2F1%2F1.png)

Suponga que trabaja para una startup de análisis de datos que se está expandiendo junto con su creciente base de clientes. 
Usted recibe datos de clientes de múltiples fuentes en diferentes formatos sin procesar. Para manejar eficientemente 
enormes cantidades de datos de clientes, su empresa ha decidido invertir en Azure Databricks.

![2.png](modules%2F7%2Fims%2F1%2F2.png)

 Su equipo es responsable de preparar los datos para análisis avanzados y operaciones de aprendizaje automático. 

![3.png](modules%2F7%2Fims%2F1%2F3.png)

 Como parte de la ingeniería de funciones, hay transformaciones específicas a nivel de columna que necesita realizar y 
 que no son compatibles con las funciones integradas.

![4.png](modules%2F7%2Fims%2F1%2F4.png)

Está buscando ver cómo las funciones definidas por el usuario o UDF, pueden ayudarle a lograr su lógica de procesamiento 
de datos compleja y personalizada. 

![5.png](modules%2F7%2Fims%2F1%2F5.png)

Estos módulos de laboratorio pueden completarse de forma gratuita utilizando la prueba de 14 días de Databricks, 
pero no puede utilizar la suscripción de prueba gratuita de Azure para crear un espacio de trabajo de prueba de 
Databricks Para cambiar una suscripción de prueba gratuita a Pay-As-You-Go, vaya a su perfil y cambie la suscripción 
ofrecida a Pay-As-You-Go. 

![6.png](modules%2F7%2Fims%2F1%2F6.png)

Cuando cree su espacio de trabajo Azure Databricks, puede seleccionar el nivel de precios de prueba premium 14 días 
unidades Databricks gratuitas o DBUs para dar acceso al espacio de trabajo premium gratuito como sus DBUs Databricks 
durante 14 días. Tenga en cuenta que, con un espacio de trabajo gratuito y DBUs gratuitas, sigue siendo responsable 
de los costes de computación. En muchos casos, esto significa que querrá crear clusters con un máximo de una nota, 
independientemente de las instrucciones de los laboratorios individuales. 

![7.png](modules%2F7%2Fims%2F1%2F7.png)

En esta lección, aprenderá a crear funciones definidas por el usuario o UDFs, y a articular las ventajas de rendimiento de las UDFs vectorizadas en python. 

![8.png](modules%2F7%2Fims%2F1%2F8.png)

Antes de empezar, asegúrese de que dispone de una suscripción a Azure. Si no dispone de una suscripción a Azure, cree 
una cuenta gratuita desde el sitio web de Microsoft. Hay un enlace a este recurso desde las lecturas adicionales al final 
de este módulo.

## 7 Write user defined functions
[< Back to index 7](#index-7)

Complete el siguiente cuaderno

Abra el [1. User Defined Functions.ipynb](modules%2F7%2Fnotebooks%2F1.%20User%20Defined%20Functions.ipynb). Asegúrese de adjuntar su cluster al cuaderno antes de seguir las instrucciones y ejecutar las celdas que contiene.

Dentro del cuaderno, usted:

- Crear, registrar e invocar UDFs

- Crear, registrar e invocar UDF vectorizadas

Una vez que haya completado el cuaderno, vuelva a esta pantalla y continúe con el siguiente paso.

## 7 Exercise: Perform Extract, Transform, Load (ETL) operations using user-defined functions
[< Back to index 7](#index-7)

En su espacio de trabajo Azure Databricks, abra la carpeta udf que importó dentro de su carpeta de usuario.

Abra el [2. Exercise User Defined Functions.ipynb](modules%2F7%2Fnotebooks%2F2.%20Exercise%20User%20Defined%20Functions.ipynb). Ejercitar funciones definidas por el usuario. Asegúrese de adjuntar su cluster al cuaderno antes de seguir las instrucciones y ejecutar las celdas que contiene.

En este ejercicio, creará UDFs para hacer ETL. El conjunto de datos tiene registros duplicados y el formato de los números de la seguridad social es incoherente. El trabajo del ETL es eliminar los registros duplicados y estandarizar el formato para los números de la seguridad social.

>Nota
Encontrará el cuaderno correspondiente dentro de [2. Exercise User Defined Functions ans.ipynb](modules%2F7%2Fnotebooks%2F2.%20Exercise%20User%20Defined%20Functions%20ans.ipynb). Éste contiene las celdas completadas para el ejercicio. Consulte el cuaderno si se queda atascado o simplemente quiere ver la solución.

Cuando haya completado el cuaderno, vuelva a esta pantalla y continúe con el paso siguiente.


## 7 Exercise quiz
[< Back to index 7](#index-7)

![0.png](modules%2F7%2Ftest%2F0.png)

## 7 Knowledge check
[< Back to index 7](#index-7)

![q1.png](modules%2F7%2Ftest%2Fq1.png)

![q2.png](modules%2F7%2Ftest%2Fq2.png)

![q3.png](modules%2F7%2Ftest%2Fq3.png)

## 7 Test prep
[< Back to index 7](#index-7)

![t1.png](modules%2F7%2Ftest%2Ft1.png)

![t2.png](modules%2F7%2Ftest%2Ft2.png)

![t3.png](modules%2F7%2Ftest%2Ft3.png)



## 7 Lesson summary
[< Back to index 7](#index-7)

![9.png](modules%2F7%2Fims%2F1%2F9.png)

Las funciones definidas por el usuario en Azure Databricks le ayudan a ir más allá de las funciones incorporadas en la 
biblioteca de funciones de punto de secuela. Las UDF le permiten encapsular su lógica de procesamiento de datos compleja 
y personalizada. Como resultado, las UDFS hacen que la lógica personalizada sea reutilizable, fácil de entender, mantener 
y depurar.

![10.png](modules%2F7%2Fims%2F1%2F10.png)

Ahora que ha concluido esta lección, debería saber cómo escribir, registrar e invocar UDFS derecho registrar e invocar 
UDF vectorizadas, y articular las ventajas de rendimiento de las UDF vectorizadas.

## 7 Additional resources
[< Back to index 7](#index-7)

Si no dispone de una suscripción a Azure, cree una
[cuenta gratuita](https://azure.microsoft.com/free)


# 8 Perform Machine Learning with Azure Databricks

## INDEX 8:

- [8 Lesson introduction](#8-lesson-introduction)
- [8 Understand Machine Learning](#8-understand-machine-learning)
- [8 Exercise: Train a Model and Create Predictions](#8-exercise-train-a-model-and-create-predictions)
- [8 Exercise quiz 1](#8-exercise-quiz-1)
- [8 Understand Data Using Exploratory Data Analysis](#8-understand-data-using-exploratory-data-analysis)
- [8 Exercise: Perform Exploratory Data Analysis](#8-exercise-perform-exploratory-data-analysis)
- [8 Exercise quiz 2](#8-exercise-quiz-2)
- [8 Describe Machine Learning Workflows](#8-describe-machine-learning-workflows)
- [8 Exercise: Build and evaluate a baseline machine learning model](#8-exercise-build-and-evaluate-a-baseline-machine-learning-model)
- [8 Exercise quiz 3](#8-exercise-quiz-3)
- [8 Knowledge check](#8-knowledge-check)
- [8 Lesson summary](#8-lesson-summary)


[< Back to index](#index-0)

## 8 Lesson introduction
[< Back to index 8](#index-8)

![1.png](modules%2F8%2Fims%2F1%2F1.png)

Imagine que trabaja como científico de datos para una tienda minorista en línea. Su organización utiliza Azure Data Lake 
para almacenar todos sus datos de compras en línea, y el equipo de ingeniería de datos ya está utilizando Azure Databricks 
para las operaciones cotidianas de manejo de datos. 

![2.png](modules%2F8%2Fims%2F1%2F2.png)

Se ha encargado a su equipo que explore técnicas de aprendizaje automático para mejorar los modelos predictivos de rotación de clientes.

![3.png](modules%2F8%2Fims%2F1%2F3.png)

Su responsabilidad es educar al resto del equipo sobre los fundamentos del aprendizaje automático, establecer un proceso 
de desarrollo que el equipo debería adoptar al embarcarse en este nuevo proyecto. 

![4.png](modules%2F8%2Fims%2F1%2F4.png)

Estos módulos de laboratorio pueden completarse gratuitamente utilizando la versión de prueba de 14 días de Databricks, 
pero no puede utilizar una suscripción de prueba gratuita de Azure para crear un espacio de trabajo de prueba de Databricks.

![5.png](modules%2F8%2Fims%2F1%2F5.png)

En esta lección, comprenderá los fundamentos del aprendizaje automático, entenderá el flujo de trabajo del aprendizaje 
automático, y aprenderá a utilizar Azure Databricks y la biblioteca PySpark para implementar partes clave del flujo de 
trabajo del aprendizaje automático, análisis exploratorio de datos, modelado y evaluación. 

## 8 Understand Machine Learning
[< Back to index 8](#index-8)

### Clone el archivo Databricks

1. Si actualmente no tiene abierto su espacio de trabajo Azure Databricks: en el portal Azure, navegue hasta su espacio de trabajo Azure Databricks desplegado y seleccioneLanzar espacio de trabajo.

2. En el panel izquierdo, seleccione Espacio detrabajo>Usuarios y seleccione su nombre de usuario (la entrada con el icono de la casa).

3. En el panel que aparece, seleccione la flecha situada junto a su nombre y seleccioneImportar.

   ![6.png](modules%2F8%2Fims%2F1%2F6.png)

4. En el cuadro de diálogoImportar cuadernos, seleccione la dirección URL y pegue la siguiente:

   > https://github.com/MicrosoftDocs/mslearn_databricks/blob/main/ml/1.1.0/Labs.dbc

5. SeleccioneImportar.

6. Seleccione la carpeta ml que aparece.

### Complete el siguiente cuaderno

Abra el cuaderno1. [1. What is Machine Learning.ipynb](modules%2F8%2Fnotebooks%2F1.%20What%20is%20Machine%20Learning.ipynb). Asegúrese de adjuntar su cluster al cuaderno antes de seguir las instrucciones y ejecutar las celdas que contiene.

Dentro del cuaderno, usted

- Definir el aprendizaje automático

- Diferenciará las tareas supervisadas de las no supervisadas

- Identificará las tareas de regresión y clasificación

- Entrenar un modelo, interpretar los resultados y crear predicciones

Cuando haya completado el cuaderno, vuelva a esta pantalla y continúe con el siguiente paso.

## 8 Exercise: Train a Model and Create Predictions
[< Back to index 8](#index-8)

En su espacio de trabajo Azure Databricks, abra la carpetaml que importó dentro de su carpeta de usuario.

Abra el cuaderno2. [2. Exercise Train a Model and Create Predictions.ipynb](modules%2F8%2Fnotebooks%2F2.%20Exercise%20Train%20a%20Model%20and%20Create%20Predictions.ipynb). Asegúrese de adjuntar su cluster al cuaderno antes de seguir las instrucciones y ejecutar las celdas que contiene.

En este ejercicio, entrenará un modelo utilizando el conjunto de datos de viviendas de Boston para predecir el precio de una vivienda. Utilizará el modelo entrenado para predecir el precio de la vivienda en los datos de prueba.

> Nota
> Encontrará un cuaderno correspondiente dentro de la subcarpetaSolutions. Éste contiene las celdas completadas para el ejercicio. Consulte el cuaderno si se queda atascado o simplemente quiere ver la solución.

Cuando haya completado el cuaderno, vuelva a esta pantalla y continúe con el siguiente paso.

## 8 Exercise quiz 1
[< Back to index 8](#index-8)

![0.png](modules%2F8%2Fims%2Ftest%2F0.png)

## 8 Understand Data Using Exploratory Data Analysis
[< Back to index 8](#index-8)

En su espacio de trabajo Azure Databricks, abra la carpetaml que importó dentro de su carpeta de usuario.

Abra el cuaderno3. [3. Exploratory Analysis.ipynb](modules%2F8%2Fnotebooks%2F3.%20Exploratory%20Analysis.ipynb). Asegúrese de adjuntar su cluster al cuaderno antes de seguir las instrucciones y ejecutar las celdas que contiene.

Dentro del cuaderno, usted:

- Identificar los objetivos principales del análisis exploratorio

- Calcular momentos estadísticos para determinar el centro y la dispersión de los datos

- Crear gráficos de datos, incluidos histogramas y diagramas de dispersión

- Calcular correlaciones entre variables

- Explorar gráficos más avanzados para visualizar la relación entre variables

Cuando haya completado el cuaderno, vuelva a esta pantalla y continúe con el siguiente paso.

## 8 Exercise: Perform Exploratory Data Analysis
[< Back to index 8](#index-8)

En su espacio de trabajo Azure Databricks, abra la carpetaml que importó dentro de su carpeta de usuario.

Abra el cuaderno4. [4. Exercise Exploratory Analysis.ipynb](modules%2F8%2Fnotebooks%2F4.%20Exercise%20Exploratory%20Analysis.ipynb). Asegúrese de adjuntar su cluster al cuaderno antes de seguir las instrucciones y ejecutar las celdas que contiene.

En este ejercicio, realizará un análisis exploratorio sobre el conjunto de datos del uso compartido de bicicletas calculando e interpretando estadísticas de resumen, creando gráficos básicos y calculando correlaciones.

> Nota
>
> Encontrará un cuaderno correspondiente dentro de la subcarpetaSolutions. Éste contiene las celdas completadas para el ejercicio. Consulte el cuaderno si se queda atascado o simplemente quiere ver la solución.

Cuando haya completado el cuaderno, vuelva a esta pantalla y continúe con el siguiente paso.

## 8 Exercise quiz 2
[< Back to index 8](#index-8)

![1.png](modules%2F8%2Fims%2Ftest%2F1.png)

## 8 Describe Machine Learning Workflows
[< Back to index 8](#index-8)

En su espacio de trabajo Azure Databricks, abra la carpetaml que importó dentro de su carpeta de usuario.

Abra el cuaderno5. [5. ML Workflows.ipynb](modules%2F8%2Fnotebooks%2F5.%20ML%20Workflows.ipynb). Asegúrese de adjuntar su cluster al cuaderno antes de seguir las instrucciones y ejecutar las celdas que contiene.

Dentro del cuaderno, usted podrá:

- Definir el ciclo de desarrollo del análisis de datos

- Motivar y realizar una división entre datos de entrenamiento y de prueba

- Entrenar un modelo de referencia

- Evaluar el rendimiento de un modelo de referencia y mejorarlo

Cuando haya completado el cuaderno, vuelva a esta pantalla y continúe con el siguiente paso.

### INFORMACIÓN CONTENIDA EN EL NOTEBOOK


Los científicos de datos siguen un flujo de trabajo iterativo que mantiene su trabajo estrechamente alineado tanto con los problemas comerciales como con sus datos. Este ciclo comienza con una comprensión exhaustiva del problema comercial y de los datos en sí, un proceso llamado análisis exploratorio de datos. Una vez que se comprenden la pregunta comercial motivadora y los datos, el siguiente paso es preparar los datos para el modelado. Esto incluye eliminar o imputar valores faltantes y valores atípicos, así como crear características para entrenar el modelo. La mayor parte del trabajo de un científico de datos se realiza en estos pasos iniciales.

Después de preparar las características de manera que el modelo pueda beneficiarse de ellas, la etapa de modelado utiliza esas características para determinar la mejor manera de representar los datos. Los diversos modelos se evalúan y todo este proceso se repite hasta que se desarrolla y despliega la mejor solución en producción.

![7.png](modules%2F8%2Fims%2F1%2F7.png)

Para implementar el ciclo de desarrollo detallado anteriormente, los científicos de datos dividen primero sus datos de manera aleatoria en dos subconjuntos. Esto permite evaluar el modelo en datos no vistos.

- El conjunto de entrenamiento se utiliza para entrenar el modelo.
- El conjunto de prueba se utiliza para probar qué tan bien se desempeña el modelo en datos no vistos.

Esta división evita la memorización de datos, conocida como sobreajuste (overfitting). El sobreajuste ocurre cuando nuestro modelo aprende patrones causados por azar en lugar de señales verdaderas. Al evaluar el rendimiento del modelo en datos no vistos, podemos minimizar el sobreajuste.

La división de datos de entrenamiento y prueba debe hacerse de manera que la cantidad de datos en el conjunto de prueba sea una buena muestra de los datos en general. Una división del 80% de sus datos en el conjunto de entrenamiento y del 20% en el conjunto de prueba es un buen punto de partida.

![8.png](modules%2F8%2Fims%2F1%2F8.png)

## 8 Exercise: Build and evaluate a baseline machine learning model
[< Back to index 8](#index-8)

En su espacio de trabajo Azure Databricks, abra la carpetaml que importó dentro de su carpeta de usuario.

Abra el cuaderno6. [6. Exercise ML Workflows.ipynb](modules%2F8%2Fnotebooks%2F6.%20Exercise%20ML%20Workflows.ipynb). Asegúrese de adjuntar su cluster al cuaderno antes de seguir las instrucciones y ejecutar las celdas que contiene.

En este ejercicio, usted promulgará partes del flujo de trabajo de Aprendizaje Automático, como hacer la división entrenamiento-prueba en un conjunto de datos, y luego construir y evaluar un modelo de línea de base. Opcionalmente, puede intentar superar el modelo de línea base entrenando un modelo de regresión lineal.

> Nota
>
> Encontrará un cuaderno correspondiente dentro de la subcarpetaSolutions. Éste contiene las celdas completadas para el ejercicio. Consulte el cuaderno si se queda atascado o simplemente quiere ver la solución.

Cuando haya completado el cuaderno, vuelva a esta pantalla y continúe con el siguiente paso.

## 8 Exercise quiz 3
[< Back to index 8](#index-8)

![2.png](modules%2F8%2Fims%2Ftest%2F2.png)

## 8 Knowledge check
[< Back to index 8](#index-8)

![q1.png](modules%2F8%2Fims%2Ftest%2Fq1.png)

![q2.png](modules%2F8%2Fims%2Ftest%2Fq2.png)

![q3.png](modules%2F8%2Fims%2Ftest%2Fq3.png)

![q4.png](modules%2F8%2Fims%2Ftest%2Fq4.png)

![q5.png](modules%2F8%2Fims%2Ftest%2Fq5.png)

## 8 Lesson summary
[< Back to index 8](#index-8)

![9.png](modules%2F8%2Fims%2F1%2F9.png)

![10.png](modules%2F8%2Fims%2F1%2F10.png)

![11.png](modules%2F8%2Fims%2F1%2F11.png)

![12.png](modules%2F8%2Fims%2F1%2F12.png)

# 9 Train a Machine Learning Model

## INDEX 9:

- [9 Lesson introduction](#9-lesson-introduction)
- [9 Perform featurization of the dataset](#9-perform-featurization-of-the-dataset)
- [9 Exercise: Finish featurization of the dataset](#9-exercise-finish-featurization-of-the-dataset)
- [9 Exercise quiz 1](#9-exercise-quiz-1)
- [9 Understanding regression modeling](#9-understanding-regression-modeling)
- [9 Exercise: Build and interpret a regression model](#9-exercise-build-and-interpret-a-regression-model)
- [9 Exercise quiz 2](#9-exercise-quiz-2)
- [9 Knowledge check](#9-knowledge-check)
- [9 Test prep](#9-test-prep)
- [9 Lesson summary](#9-lesson-summary)
- [9 Additional resources](#9-additional-resources)


[< Back to index](#index-0)

## 9 Lesson introduction
[< Back to index 9](#index-9)

![1.png](modules%2F9%2Fims%2F1%2F1.png)

Imagine que trabaja como científico de datos para una empresa de bicicletas compartidas. Su organización utiliza Azure 
Data Lake para almacenar todos sus datos históricos de uso para la ciudad de Nueva York.

![2.png](modules%2F9%2Fims%2F1%2F2.png)

Usted es responsable de desarrollar un modelo de regresión basado en aprendizaje automático para predecir la demanda 
estacional de alquiler de bicicletas para la ciudad de Nueva York. 

![3.png](modules%2F9%2Fims%2F1%2F3.png)

Los laboratorios de este módulo pueden completarse gratuitamente utilizando la versión de prueba de 14 días de Databricks, 
pero no puede utilizar una suscripción de prueba gratuita de Azure para crear un espacio de trabajo de prueba de Databricks

![4.png](modules%2F9%2Fims%2F1%2F4.png)

En esta lección, aprenderá a comprender los tres bloques de construcción principales en la biblioteca de aprendizaje 
automático pice box, transformadores, estimadores y pipelines. Aprenda a construir pipelines para tareas comunes de 
featurización de datos y profundice en el modelado de regresión. Eso incluye el entrenamiento y la interpretación de un 
modelo de regresión entrenado.

## 9 Perform featurization of the dataset
[< Back to index 9](#index-9)

### Clone el archivo Databricks

1. Si actualmente no tiene abierto su espacio de trabajo Azure Databricks: en el portal Azure, navegue hasta su espacio de trabajo Azure Databricks desplegado y seleccioneLanzar espacio de trabajo.

2. En el panel izquierdo, seleccione Espacio detrabajo>Usuarios y seleccione su nombre de usuario (la entrada con el icono de la casa).

3. En el panel que aparece, seleccione la flecha situada junto a su nombre y seleccioneImportar.

   ![6.png](modules%2F8%2Fims%2F1%2F6.png)

4. En el cuadro de diálogoImportar cuadernos, seleccione la dirección URL y pegue la siguiente:

   > https://github.com/MicrosoftDocs/mslearn_databricks/blob/main/ml-model/1.1.0/Labs.dbc

5. SeleccioneImportar.

6. Seleccione la carpeta ml que aparece.

### Complete el siguiente cuaderno
Abra el cuaderno1. [1. Featurization.ipynb](modules%2F9%2Fnotebooks%2F1.%20Featurization.ipynb). Asegúrese de adjuntar su cluster al cuaderno antes de seguir las instrucciones y ejecutar las celdas que contiene.

Dentro del cuaderno, usted deberá:

- Diferenciará los transformadores, estimadores y canalizaciones de Spark

- Codificar características categóricas

- Imputar los datos que faltan

- Combinará diferentes etapas de featurización en una canalización

Cuando haya completado el cuaderno, vuelva a esta pantalla y continúe con el siguiente paso.

## 9 Exercise: Finish featurization of the dataset
[< Back to index 9](#index-9)

En su espacio de trabajo Azure Databricks, abra la carpetaml-model que importó dentro de su carpeta de usuario.

Abra el cuaderno2. [2. Exercise Featurization.ipynb](modules%2F9%2Fnotebooks%2F2.%20Exercise%20Featurization.ipynb). Asegúrese de adjuntar su cluster al cuaderno antes de seguir las instrucciones y ejecutar las celdas que contiene.

En este ejercicio va a featurizar datos categóricos dividiéndolos en bins. También eliminará los registros con valores de datos incorrectos y normalizará la columna de etiquetas.

> Nota 
> 
> Encontrará un cuaderno correspondiente dentro de la subcarpetaSolutions. Éste contiene las celdas completadas para el ejercicio. Consulte el cuaderno si se queda atascado o simplemente quiere ver la solución.

Cuando haya completado el cuaderno, vuelva a esta pantalla y continúe con el siguiente paso.

## 9 Exercise quiz 1
[< Back to index 9](#index-9)

![0.png](modules%2F9%2Fims%2Ftests%2F0.png)

## 9 Understanding regression modeling
[< Back to index 9](#index-9)

En su espacio de trabajo Azure Databricks, abra la carpetaml-model que importó dentro de su carpeta de usuario.

Abra el cuaderno3. [3. Regression Modeling.ipynb](modules%2F9%2Fnotebooks%2F3.%20Regression%20Modeling.ipynb). Asegúrese de adjuntar su cluster al cuaderno antes de seguir las instrucciones y ejecutar las celdas que contiene.

Dentro del cuaderno, usted

- Motivará el uso de la regresión lineal

- Entrenar un modelo de regresión simple

- Interpretar modelos de regresión

- Entrenar un modelo de regresión multivariante

Cuando haya completado el cuaderno, vuelva a esta pantalla y continúe con el siguiente paso.

## 9 Exercise: Build and interpret a regression model
[< Back to index 9](#index-9)

En su espacio de trabajo Azure Databricks, abra la carpetaml-model que importó dentro de su carpeta de usuario.

Abra el cuaderno4. [4. Exercise Regression Modeling.ipynb](modules%2F9%2Fnotebooks%2F4.%20Exercise%20Regression%20Modeling.ipynb). Asegúrese de adjuntar su cluster al cuaderno antes de seguir las instrucciones y ejecutar las celdas que contiene.

En este ejercicio, entrenará un modelo de regresión utilizando el conjunto de datos de viviendas de Boston para predecir el precio de una vivienda e interpretará la significación estadística de los coeficientes del modelo entrenado.

>Nota
>
>Encontrará un cuaderno correspondiente dentro de la subcarpetaSolutions. Éste contiene las celdas completadas para el ejercicio. Consulte el cuaderno si se queda atascado o simplemente quiere ver la solución.

Cuando haya completado el cuaderno, vuelva a esta pantalla y continúe con el siguiente paso.

## 9 Exercise quiz 2
[< Back to index 9](#index-9)

![1.png](modules%2F9%2Fims%2Ftests%2F1.png)

## 9 Knowledge check
[< Back to index 9](#index-9)

![q1.png](modules%2F9%2Fims%2Ftests%2Fq1.png)

![q2.png](modules%2F9%2Fims%2Ftests%2Fq2.png)

![q3.png](modules%2F9%2Fims%2Ftests%2Fq3.png)

![q4.png](modules%2F9%2Fims%2Ftests%2Fq4.png)

![q5.png](modules%2F9%2Fims%2Ftests%2Fq5.png)

## 9 Test prep
[< Back to index 9](#index-9)

![t1.png](modules%2F9%2Fims%2Ftests%2Ft1.png)

![t2.png](modules%2F9%2Fims%2Ftests%2Ft2.png)

![t3.png](modules%2F9%2Fims%2Ftests%2Ft3.png)

![t4.png](modules%2F9%2Fims%2Ftests%2Ft4.png)

![t5.png](modules%2F9%2Fims%2Ftests%2Ft5.png)

![t6.png](modules%2F9%2Fims%2Ftests%2Ft6.png)

![t7.png](modules%2F9%2Fims%2Ftests%2Ft7.png)

## 9 Lesson summary
[< Back to index 9](#index-9)

![5.png](modules%2F9%2Fims%2F1%2F5.png)

En esta lección, tú aprendió a usar la máquina PySparks paquete de aprendizaje para crear funcionalidades canaliza e 
interpreta un modele una regresión simple y entrene un multivariante modelo de regresión. 

Ahora que tienes Al concluir esta lección, debes saber cómo diferencie los transformadores Spark, los estimadores y 
las tuberías. Codificación en un solo paso características categóricas, imputa los datos faltantes y combina diferentes 
caracterizaciones etapas de una canalización. 

![6.png](modules%2F9%2Fims%2F1%2F6.png)

También aprendiste a entrenar un modelo de regresión simple, interpretar los modelos de regresión y entrenar un 
multivariante modelo de regresión. 

## 9 Additional resources
[< Back to index 9](#index-9)

Si no dispone de una suscripción a Azure, cree una
[cuenta gratuita](https://azure.microsoft.com/free)


# 10 Work with MLFlow in Azure Databricks

## INDEX 10:

- [Lesson introduction](#10-lesson-introduction)
- [Use MLFlow to track experiments, log metrics, and compare runs](#10-use-mlflow-to-track-experiments-log-metrics-and-compare-runs)
- [Exercise: Work with MLFlow to track experiment metrics, parameters, artifacts, and models](#10-exercise-work-with-mlflow-to-track-experiment-metrics-parameters-artifacts-and-models)
- [Exercise quiz](#10-exercise-quiz)
- [Knowledge check](#10-knowledge-check)
- [Lesson summary](#10-lesson-summary)


[< Back to index](#index-0)

## 10 Lesson introduction
[< Back to index 10](#index-10)

![1.png](modules%2F10%2Fims%2F1%2F1.png)

Imagine que trabaja como científico de datos para una empresa de bicicletas compartidas. Su equipo ha empezado a experimentar 
con algoritmos de aprendizaje automático para predecir la demanda estacional de alquiler de bicicletas.

![img.png](modules%2F10%2Fims%2F1%2Fimg.png)

Su equipo se da cuenta de que gestionar el flujo de trabajo del aprendizaje automático es todo un reto debido a la 
dificultad para realizar el seguimiento de varios experimentos, garantizar que los experimentos sean reproducibles, 
y proporcionar una forma estándar de empaquetar y desplegar modelos entrenados. 

![img_1.png](modules%2F10%2Fims%2F1%2Fimg_1.png)

Se le ha encomendado la tarea de investigar marcos para gestionar el ciclo de vida del aprendizaje automático con cambios 
mínimos para integrarse con su base de código existente. 

![img_2.png](modules%2F10%2Fims%2F1%2Fimg_2.png)

Los laboratorios de este módulo pueden completarse gratuitamente utilizando la prueba de 14 días de Databricks.

![img_3.png](modules%2F10%2Fims%2F1%2Fimg_3.png)

En esta lección, aprenderá a utilizar ML flow para rastrear experimentos, registrar métricas y comparar ejecuciones. 
Antes de comenzar, asegúrese de tener una suscripción a Azure. 

## 10 Use MLFlow to track experiments, log metrics, and compare runs
[< Back to index 10](#index-10)

### Clone el archivo Databricks

1. Si actualmente no tiene abierto su espacio de trabajo Azure Databricks: en el portal Azure, navegue hasta su espacio de trabajo Azure Databricks desplegado y seleccioneLanzar espacio de trabajo.

2. En el panel izquierdo, seleccione Espacio detrabajo>Usuarios y seleccione su nombre de usuario (la entrada con el icono de la casa).

3. En el panel que aparece, seleccione la flecha situada junto a su nombre y seleccioneImportar.

   ![6.png](modules%2F8%2Fims%2F1%2F6.png)

4. En el cuadro de diálogoImportar cuadernos, seleccione la dirección URL y pegue la siguiente:

   > https://github.com/MicrosoftDocs/mslearn_databricks/blob/main/mlflow/1.1.0/Labs.dbc

5. SeleccioneImportar.

6. Seleccione la carpeta ml que aparece.

### Complete el siguiente cuaderno
Abra el cuaderno1. [1. MLflow.ipynb](modules%2F10%2Fnotebooks%2F1.%20MLflow.ipynb). Asegúrese de adjuntar su cluster al cuaderno antes de seguir las instrucciones y ejecutar las celdas que contiene.

Dentro del cuaderno, usted:

- Utilizar MLflow para realizar un seguimiento de los experimentos, registrar métricas y comparar ejecuciones

Una vez que haya completado el cuaderno, vuelva a esta pantalla y continúe con el siguiente paso.

## 10 Exercise: Work with MLFlow to track experiment metrics, parameters, artifacts and models
[< Back to index 10](#index-10)

En su espacio de trabajo Azure Databricks, abra la carpetamlflow que importó dentro de su carpeta de usuario.

Abra el cuaderno2. [2. Exercise MLflow.ipynb](modules%2F10%2Fnotebooks%2F2.%20Exercise%20MLflow.ipynb). Asegúrese de adjuntar su cluster al cuaderno antes de seguir las instrucciones y ejecutar las celdas que contiene.

En este ejercicio, utilizará el conjunto de datos de diabetes en scikit-learn y predecirá la métrica de progresión (una medida cuantitativa de la progresión de la enfermedad al cabo de un año) basándose en el IMC y la presión arterial. Utilizará el modelo de regresión lineal ElasticNet de scikit-learn, en el que variará los parámetros alfa y l1_ratio para su ajuste. Utilizará MLflow para registrar las métricas, los parámetros, los artefactos y el modelo.

> Nota
>
> Encontrará un cuaderno correspondiente dentro de la subcarpeta de soluciones. Éste contiene las celdas completadas para el ejercicio. Consulte el cuaderno si se queda atascado o simplemente quiere ver la solución.


Cuando haya completado el cuaderno, vuelva a esta pantalla y continúe con el siguiente paso.

## 10 Exercise quiz
[< Back to index 10](#index-10)

![img_4.png](modules%2F10%2Fims%2F1%2Fimg_4.png)

## 10 Knowledge check
[< Back to index 10](#index-10)

![q1.png](modules%2F10%2Fims%2Ftests%2Fq1.png)

![q2.png](modules%2F10%2Fims%2Ftests%2Fq2.png)

![q3.png](modules%2F10%2Fims%2Ftests%2Fq3.png)

![q4.png](modules%2F10%2Fims%2Ftests%2Fq4.png)

## 10 Lesson summary
[< Back to index 10](#index-10)

![img_5.png](modules%2F10%2Fims%2F1%2Fimg_5.png)

En esta lección, ha aprendido a trabajar con MLflow en Azure Databricks. O, más concretamente, a utilizar MLflow para 
realizar un seguimiento de los experimentos, registrar métricas y comparar ejecuciones. 

# 11 Perform Model Selection with Hyperparameter Tuning

## INDEX 11:

- [11 Lesson introduction](#11-lesson-introduction)
- [11 Describe model selection and hyperparameter tuning](#11-describe-model-selection-and-hyperparameter-tuning)
- [11 Exercise: Select optimal model by tuning hyperparameters](#11-exercise-select-optimal-model-by-tuning-hyperparameters)
- [11 Exercise quiz](#11-exercise-quiz)
- [11 Knowledge check](#11-knowledge-check)
- [11 Test prep](#11-test-prep)
- [11 Lesson summary](#11-lesson-summary)
- [11 Additional resources](#11-additional-resources)


[< Back to index](#index-0)

## 11 Lesson introduction
[< Back to index 11](#index-11)

## 11 Describe model selection and hyperparameter tuning
[< Back to index 11](#index-11)

### Clone el archivo Databricks

1. Si actualmente no tiene abierto su espacio de trabajo Azure Databricks: en el portal Azure, navegue hasta su espacio de trabajo Azure Databricks desplegado y seleccioneLanzar espacio de trabajo.

2. En el panel izquierdo, seleccione Espacio detrabajo>Usuarios y seleccione su nombre de usuario (la entrada con el icono de la casa).

3. En el panel que aparece, seleccione la flecha situada junto a su nombre y seleccioneImportar.

   ![6.png](modules%2F8%2Fims%2F1%2F6.png)

4. En el cuadro de diálogoImportar cuadernos, seleccione la dirección URL y pegue la siguiente:

   > https://github.com/MicrosoftDocs/mslearn_databricks/blob/main/hyperparameter/1.1.0/Labs.dbc

5. SeleccioneImportar.

6. Seleccione la carpeta ml que aparece.

### Complete el siguiente cuaderno
Abra el cuaderno1. [1. Model Selection.ipynb](modules%2F11%2Fnotebooks%2F1.%20Model%20Selection.ipynb). Asegúrese de adjuntar su cluster al cuaderno antes de seguir las instrucciones y ejecutar las celdas que contiene.

Dentro del cuaderno, usted:

- Definir los hiperparámetros y motivar su papel en el aprendizaje automático

- Ajustar los hiperparámetros utilizando la búsqueda en cuadrícula

- Validar el rendimiento del modelo utilizando la validación cruzada

- Guardar un modelo entrenado y sus predicciones

Cuando haya completado el cuaderno, vuelva a esta pantalla y continúe con el siguiente paso.

## 11 Exercise: Select optimal model by tuning hyperparameters
[< Back to index 11](#index-11)

En su espacio de trabajo Azure Databricks, abra la carpeta dehiperparámetros que importó dentro de su carpeta de usuario.

Abra el cuaderno2. [2. Exercise Model Selection.ipynb](modules%2F11%2Fnotebooks%2F2.%20Exercise%20Model%20Selection.ipynb). Asegúrese de adjuntar su cluster al cuaderno antes de seguir las instrucciones y ejecutar las celdas que contiene.

En este ejercicio, utilizará la búsqueda en cuadrícula y la validación cruzada para ajustar los hiperparámetros de un modelo de regresión logística.

> Nota
>
> Encontrará un cuaderno correspondiente dentro de la subcarpeta de soluciones. Éste contiene las celdas completadas para el ejercicio. Consulte el cuaderno si se queda atascado o simplemente quiere ver la solución.

Cuando haya completado el cuaderno, vuelva a esta pantalla y continúe con el siguiente paso.

## 11 Exercise quiz
[< Back to index 11](#index-11)

![img_5.png](modules%2F11%2Fims%2F1%2Fimg_5.png)

## 11 Knowledge check
[< Back to index 11](#index-11)

![q1.png](modules%2F11%2Fims%2Ftest%2Fq1.png)

![q2.png](modules%2F11%2Fims%2Ftest%2Fq2.png)

![q3.png](modules%2F11%2Fims%2Ftest%2Fq3.png)

![q4.png](modules%2F11%2Fims%2Ftest%2Fq4.png)

![q5.png](modules%2F11%2Fims%2Ftest%2Fq5.png)

## 11 Test prep
[< Back to index 11](#index-11)

![t1.png](modules%2F11%2Fims%2Ftest%2Ft1.png)

![t2.png](modules%2F11%2Fims%2Ftest%2Ft2.png)

![t3.png](modules%2F11%2Fims%2Ftest%2Ft3.png)

![t4.png](modules%2F11%2Fims%2Ftest%2Ft4.png)

![t5.png](modules%2F11%2Fims%2Ftest%2Ft5.png)

![t6.png](modules%2F11%2Fims%2Ftest%2Ft6.png)

![t7.png](modules%2F11%2Fims%2Ftest%2Ft7.png)

## 11 Lesson summary
[< Back to index 11](#index-11)

![img_6.png](modules%2F11%2Fims%2F1%2Fimg_6.png)

En este módulo, ha aprendido sobre la Biblioteca PySparks de Aprendizaje Automático para ajuste de hiperparámetros y 
técnicas de validación para ayudar a seleccionar el modelo de mejor rendimiento. Ahora que ha concluido este módulo, 
debería saber cómo: 

- Definir hiperparámetros y modificar su función en el Aprendizaje Automático. 
- Ajuste los hiperparámetros utilizando la búsqueda en cuadrícula.
- Valide el rendimiento del modelo utilizando la validación cruzada. 
- Guarde un modelo entrenado y sus predicciones.

## 11 Additional resources
[< Back to index 11](#index-11)

Si no dispone de una suscripción a Azure, cree una
[cuenta gratuita](https://azure.microsoft.com/free)


# 12 Deep Learning with Horovod for distributed training

## INDEX 12:

- [12 Lesson introduction](#12-lesson-introduction)
- [12 Use Horovod to train a Deep Learning Model](#12-use-horovod-to-train-a-deep-learning-model)
- [12 Use Petastorm to read in Apache Parquet format with Horovod for distributed model training](#12-use-petastorm-to-read-in-apache-parquet-format-with-horovod-for-distributed-model-training)
- [12 Exercise: Work with Horovod and Petastorm for training a deep learning model](#12-exercise-work-with-horovod-and-petastorm-for-training-a-deep-learning-model)
- [12 Exercise quiz](#12-exercise-quiz)
- [12 Knowledge check](#12-knowledge-check)
- [12 Lesson summary](#12-lesson-summary)


[< Back to index](#index-0)

## 12 Lesson introduction
[< Back to index 12](#index-12)

![img.png](modules%2F12%2Fims%2F1%2Fimg.png)

Bienvenido. Imagine que trabaja como científico de datos para una gran tienda minorista en línea. Ha entrenado un 
modelo de aprendizaje profundo utilizando TensorFlow en un subconjunto de los datos de entrenamiento disponibles en una 
máquina GPU de un solo nodo para predecir la pérdida de clientes. 

![img_1.png](modules%2F12%2Fims%2F1%2Fimg_1.png)

Ahora, está listo para entrenar la siguiente iteración de el modelo utilizando todos los datos de entrenamiento disponibles. 
Sin embargo, se da cuenta de que el tiempo de entrenamiento en un solo nodo será del orden de semanas. 

![img_2.png](modules%2F12%2Fims%2F1%2Fimg_2.png)

Su siguiente tarea es buscar marcos de trabajo de código abierto que soporten el entrenamiento distribuido de modelos de 
aprendizaje profundo que sea a la vez de alto rendimiento y fácil de usar. 

![img_3.png](modules%2F12%2Fims%2F1%2Fimg_3.png)

Los laboratorios de este módulo pueden completarse gratuitamente utilizando la prueba de 14 días de Databricks, pero no 
puede utilizar una suscripción de prueba gratuita de Azure para crear un espacio de trabajo de prueba de Databricks.

![img_4.png](modules%2F12%2Fims%2F1%2Fimg_4.png)

En esta lección, aprenderá cómo utilizar Azure Databricks y 

- ejecutar trabajos de entrenamiento de aprendizaje profundo distribuidos de larga duración en Spark 
- utilizar Petastorm para permitir el entrenamiento y la evaluación de modelos de aprendizaje profundo a partir de conjuntos de datos en formato Apache Parquet.

## 12 Use Horovod to train a Deep Learning Model
[< Back to index 12](#index-12)

1. Si actualmente no tiene abierto su espacio de trabajo Azure Databricks: en el portal Azure, navegue hasta su espacio de trabajo Azure Databricks desplegado y seleccioneLanzar espacio de trabajo.

2. En el panel izquierdo, seleccione Espacio detrabajo>Usuarios y seleccione su nombre de usuario (la entrada con el icono de la casa).

3. En el panel que aparece, seleccione la flecha situada junto a su nombre y seleccioneImportar.

   ![6.png](modules%2F8%2Fims%2F1%2F6.png)

4. En el cuadro de diálogoImportar cuadernos, seleccione la dirección URL y pegue la siguiente:

   > https://github.com/solliancenet/microsoft-learning-paths-databricks-notebooks/blob/master/data-science/11-Deep-Learning.dbc?raw=true

5. SeleccioneImportar.

6. Seleccione la carpeta ml que aparece.

### Complete el siguiente cuaderno
Abra el cuaderno1. [1. Horovod.ipynb](modules%2F12%2Fnotebooks%2F1.%20Horovod.ipynb). Asegúrese de adjuntar su cluster al cuaderno antes de seguir las instrucciones y ejecutar las celdas que contiene.

Dentro del cuaderno, usted:

Utilizar Horovod para entrenar una red neuronal distribuida

Una vez que haya completado el cuaderno, vuelva a esta pantalla y continúe con el siguiente paso.

## 12 Use Petastorm to read in Apache Parquet format with Horovod for distributed model training
[< Back to index 12](#index-12)

En su espacio de trabajo Azure Databricks, abra la carpeta11-Deep-Learning que importó dentro de su carpeta de usuario.

Abra el cuaderno2. [2. Horovod Petastorm.ipynb](modules%2F12%2Fnotebooks%2F2.%20Horovod%20Petastorm.ipynb). Asegúrese de adjuntar su cluster al cuaderno antes de seguir las instrucciones y ejecutar las celdas que contiene.

**Dentro del cuaderno, usted:**

- Utilizar Horovod para entrenar una red neuronal distribuida utilizando archivos Parquet + Petastorm

Una vez que haya completado el cuaderno, vuelva a esta pantalla y continúe con el siguiente paso.

## 12 Exercise: Work with Horovod and Petastorm for training a deep learning model
[< Back to index 12](#index-12)

En su espacio de trabajo Azure Databricks, abra la carpeta11-Deep-Learning que importó dentro de su carpeta de usuario.

Abra el cuaderno3. [3. Exercise Horovod Petastorm.ipynb](modules%2F12%2Fnotebooks%2F3.%20Exercise%20Horovod%20Petastorm.ipynb). Asegúrese de adjuntar su cluster al cuaderno antes de seguir las instrucciones y ejecutar las celdas que contiene.

En este ejercicio construirá un modelo sobre el conjunto de datos de viviendas de Boston y distribuirá el proceso de entrenamiento de aprendizaje profundo utilizando tanto HorovodRunner como Petastorm.

> Nota 
>
> Encontrará un cuaderno correspondiente dentro de la subcarpetaSolutions. Éste contiene las casillas completadas para el ejercicio. Consulte el cuaderno si se queda atascado o simplemente quiere ver la solución.

Cuando haya completado el cuaderno, vuelva a esta pantalla y continúe con el siguiente paso.

## 12 Exercise quiz
[< Back to index 12](#index-12)

![img.png](modules%2F12%2Fims%2Ftests%2Fimg.png)

## 12 Knowledge check
[< Back to index 12](#index-12)

![img_1.png](modules%2F12%2Fims%2Ftests%2Fimg_1.png)

![img_2.png](modules%2F12%2Fims%2Ftests%2Fimg_2.png)

![img_3.png](modules%2F12%2Fims%2Ftests%2Fimg_3.png)

![img_4.png](modules%2F12%2Fims%2Ftests%2Fimg_4.png)

![img_5.png](modules%2F12%2Fims%2Ftests%2Fimg_5.png)

## 12 Lesson summary
[< Back to index 12](#index-12)

![img_5.png](modules%2F12%2Fims%2F1%2Fimg_5.png)

En esta lección, aprendió a utilizar Azure Databricks y HorovodRunner para ejecutar cargas de trabajo distribuidas de 
aprendizaje profundo. Aprendió a fragmentar los datos de entrenamiento utilizando tanto Pandas dataframe como archivos 
Parquet con Petastorm. 

![img_6.png](modules%2F12%2Fims%2F1%2Fimg_6.png)

 Por último, puso sus conocimientos a prueba completando un ejercicio que requería que construyera un modelo y distribuyera 
 el proceso de entrenamiento de aprendizaje profundo utilizando tanto HorovodRunner como Petastorm. 

![img_7.png](modules%2F12%2Fims%2F1%2Fimg_7.png)

Ahora que ha concluido esta lección, debería saber cómo 

- utilizar Horovod para entrenar una red neuronal distribuida y utilizar Horovod para entrenar una red neuronal distribuida 
- utilizando archivos Parquet y Petastorm

# 13 Work with Azure Machine Learning to deploy serving models

## INDEX 13:

- [13 Lesson introduction](#13-lesson-introduction)
- [13 Use Azure Machine Learning to Deploy Serving Models](#13-use-azure-machine-learning-to-deploy-serving-models)
- [13 Knowledge check](#13-knowledge-check)
- [13 Test prep](#13-test-prep)
- [13 Lesson summary](#13-lesson-summary)
- [13 Additional resources](#13-additional-resources)

[< Back to index](#index-0)

## 13 Lesson introduction
[< Back to index 13](#index-13)

![img.png](modules%2F13%2Fims%2F1%2Fimg.png)

Imagine que trabaja como científico de datos para una gran tienda minorista en línea. Su equipo realizó una investigación 
y decidió adoptar Azure Machine Learning como la plataforma para la gestión del ciclo de vida del aprendizaje automático.

![img_1.png](modules%2F13%2Fims%2F1%2Fimg_1.png)

Se le ha encomendado la tarea de comprender cómo Azure Machine Learning soporta la operacionalización de los modelos de 
aprendizaje automático. 

![img_2.png](modules%2F13%2Fims%2F1%2Fimg_2.png)

Quiere investigar enfoques para desplegar y probar sus modelos como un servicio web de puntuación tanto en desarrollo 
como en producción. También quiere entender cómo actualizar modelos que ya están desplegados en producción. 

![img_3.png](modules%2F13%2Fims%2F1%2Fimg_3.png)

Los laboratorios de este módulo pueden completarse de forma gratuita utilizando la versión de prueba de 14 días de 
Databricks, pero no puede utilizar una suscripción de prueba gratuita de Azure para crear un espacio de trabajo de prueba de Databricks. 

![img_4.png](modules%2F13%2Fims%2F1%2Fimg_4.png)

En esta lección, utilizará el paquete mlflow.azureml, un kit de desarrollo de software Azure Machine Learning Python o 
SDK para crear o cargar el espacio de trabajo Azure machine learning o ML. Registre el modelo de flujo ML con Azure ML y 
cree el despliegue de la imagen del contenedor. Despliegue el modelo en Azure Container Instances o ACI. Despliegue el 
modelo en Azure Kubernetes Service o AKS. Pruebe el despliegue de su modelo y actualice el despliegue del modelo en AKS. 
Antes de comenzar, asegúrese de tener una suscripción a Azure. 

## 13 Use Azure Machine Learning to Deploy Serving Models
[< Back to index 13](#index-13)

1. Si actualmente no tiene abierto su espacio de trabajo Azure Databricks: en el portal Azure, navegue hasta su espacio de trabajo Azure Databricks desplegado y seleccioneLanzar espacio de trabajo.

2. En el panel izquierdo, seleccione Espacio detrabajo>Usuarios y seleccione su nombre de usuario (la entrada con el icono de la casa).

3. En el panel que aparece, seleccione la flecha situada junto a su nombre y seleccioneImportar.

   ![6.png](modules%2F8%2Fims%2F1%2F6.png)

4. En el cuadro de diálogoImportar cuadernos, seleccione la dirección URL y pegue la siguiente:

   > https://github.com/MicrosoftDocs/mslearn_databricks/blob/main/azure-ml/1.1.0/Labs.dbc

5. SeleccioneImportar.

6. Seleccione la carpeta ml que aparece.

### Complete el siguiente cuaderno

Abra el cuaderno1. [1. Serving Models with Microsoft Azure ML.ipynb](modules%2F13%2Fnotebooks%2F1.%20Serving%20Models%20with%20Microsoft%20Azure%20ML.ipynb). Asegúrese de conectar su clúster al cuaderno antes de seguir las instrucciones y ejecutar las celdas que contiene.

Dentro del cuaderno, usted deberá:

Crear o cargar un Azure ML Workspace

Construirá una imagen de contenedor Azure para el despliegue del modelo

Desplegar el modelo en "dev" utilizando ACI

Consultará el modelo desplegado en "dev"

Desplegar el modelo en producción utilizando AKS

Consulte el modelo desplegado en producción

Actualice el despliegue en producción

Limpie los despliegues

Una vez que haya completado el cuaderno, vuelva a esta pantalla y continúe con el siguiente paso.

## 13 Knowledge check
[< Back to index 13](#index-13)

![q1.png](modules%2F13%2Fims%2Ftests%2Fq1.png)

![q2.png](modules%2F13%2Fims%2Ftests%2Fq2.png)

![q3.png](modules%2F13%2Fims%2Ftests%2Fq3.png)

![q4.png](modules%2F13%2Fims%2Ftests%2Fq4.png)

![q5.png](modules%2F13%2Fims%2Ftests%2Fq5.png)


## 13 Test prep
[< Back to index 13](#index-13)

![img.png](modules%2F13%2Fims%2Ftests%2Fimg.png)

![img_1.png](modules%2F13%2Fims%2Ftests%2Fimg_1.png)

![img_2.png](modules%2F13%2Fims%2Ftests%2Fimg_2.png)

![img_3.png](modules%2F13%2Fims%2Ftests%2Fimg_3.png)

![img_4.png](modules%2F13%2Fims%2Ftests%2Fimg_4.png)

## 13 Lesson summary
[< Back to index 13](#index-13)

![img_5.png](modules%2F13%2Fims%2F1%2Fimg_5.png)

En esta lección, ha aprendido a utilizar Azure Databricks, ML flow, y Azure Machine Learning Python SDK para registrar 
modelos entrenados con el servicio Azure Machine Learning

Ha aprendido a construir imágenes de contenedor que pueden desplegarse como un servicio web de puntuación en ACI para 
desarrollo y pruebas, y posteriormente en AKS para dar soporte a aplicaciones de producción. 

![img_6.png](modules%2F13%2Fims%2F1%2Fimg_6.png)

A medida que desarrolle nuevas versiones de su modelo, también habrá aprendido a actualizar un despliegue AKS existente.

![img_7.png](modules%2F13%2Fims%2F1%2Fimg_7.png)

Ahora que ha concluido esta lección, debería saber cómo servir modelos con Azure Machine Learning. 

## 13 Additional resources
[< Back to index 13](#index-13)

Si no dispone de una suscripción a Azure, cree una
[cuenta gratuita](https://azure.microsoft.com/free)


