{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Create a Batch Inferencing Service\n",
        "\n",
        "Imagine a health clinic takes patient measurements all day, saving the details for each patient in a separate file. Then overnight, the diabetes prediction model can be used to process all of the day's patient data as a batch, generating predictions that will be waiting the following morning so that the clinic can follow up with patients who are predicted to be at risk of diabetes. With Azure Machine Learning, you can accomplish this by creating a *batch inferencing pipeline*; and that's what you'll implement in this exercise."
      ],
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect to your workspace\n",
        "\n",
        "To get started, connect to your workspace.\n",
        "\n",
        "> **Note**: If you haven't already established an authenticated session with your Azure subscription, you'll be prompted to authenticate by clicking a link, entering an authentication code, and signing into Azure."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import azureml.core\n",
        "from azureml.core import Workspace\n",
        "\n",
        "# Load the workspace from the saved config file\n",
        "ws = Workspace.from_config()\n",
        "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Ready to use Azure ML 1.51.0 to work with mlw-dp100-labs\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1698950918258
        },
        "vscode": {
          "languageId": "python"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and register a model\n",
        "\n",
        "Now let's train and register a model to deploy in a batch inferencing pipeline."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Experiment\n",
        "from azureml.core import Model\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "# Create an Azure ML experiment in your workspace\n",
        "experiment = Experiment(workspace=ws, name='mslearn-train-diabetes')\n",
        "run = experiment.start_logging()\n",
        "print(\"Starting experiment:\", experiment.name)\n",
        "\n",
        "# load the diabetes dataset\n",
        "print(\"Loading Data...\")\n",
        "diabetes = pd.read_csv('data/diabetes.csv')\n",
        "\n",
        "# Separate features and labels\n",
        "X, y = diabetes[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']].values, diabetes['Diabetic'].values\n",
        "\n",
        "# Split data into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
        "\n",
        "# Train a decision tree model\n",
        "print('Training a decision tree model')\n",
        "model = DecisionTreeClassifier().fit(X_train, y_train)\n",
        "\n",
        "# calculate accuracy\n",
        "y_hat = model.predict(X_test)\n",
        "acc = np.average(y_hat == y_test)\n",
        "print('Accuracy:', acc)\n",
        "run.log('Accuracy', np.float(acc))\n",
        "\n",
        "# calculate AUC\n",
        "y_scores = model.predict_proba(X_test)\n",
        "auc = roc_auc_score(y_test,y_scores[:,1])\n",
        "print('AUC: ' + str(auc))\n",
        "run.log('AUC', np.float(auc))\n",
        "\n",
        "# Save the trained model\n",
        "model_file = 'diabetes_model.pkl'\n",
        "joblib.dump(value=model, filename=model_file)\n",
        "run.upload_file(name = 'outputs/' + model_file, path_or_stream = './' + model_file)\n",
        "\n",
        "# Complete the run\n",
        "run.complete()\n",
        "\n",
        "# Register the model\n",
        "run.register_model(model_path='outputs/diabetes_model.pkl', model_name='diabetes_model',\n",
        "                   tags={'Training context':'Inline Training'},\n",
        "                   properties={'AUC': run.get_metrics()['AUC'], 'Accuracy': run.get_metrics()['Accuracy']})\n",
        "\n",
        "print('Model trained and registered.')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Starting experiment: mslearn-train-diabetes\nLoading Data...\nTraining a decision tree model\nAccuracy: 0.8926666666666667\nAUC: 0.8803323548435243\nModel trained and registered.\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/tmp/ipykernel_5339/2140859650.py:34: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  run.log('Accuracy', np.float(acc))\n/tmp/ipykernel_5339/2140859650.py:40: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  run.log('AUC', np.float(auc))\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "gather": {
          "logged": 1698951057430
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate and upload batch data\n",
        "\n",
        "Since we don't actually have a fully staffed clinic with patients from whom to get new data for this exercise, you'll generate a random sample from our diabetes CSV file, upload that data to a datastore in the Azure Machine Learning workspace, and register a dataset for it."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Datastore, Dataset\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Set default data store\n",
        "ws.set_default_datastore('workspaceblobstore')\n",
        "default_ds = ws.get_default_datastore()\n",
        "\n",
        "# Enumerate all datastores, indicating which is the default\n",
        "for ds_name in ws.datastores:\n",
        "    print(ds_name, \"- Default =\", ds_name == default_ds.name)\n",
        "\n",
        "# Load the diabetes data\n",
        "diabetes = pd.read_csv('data/diabetes2.csv')\n",
        "# Get a 100-item sample of the feature columns (not the diabetic label)\n",
        "sample = diabetes[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']].sample(n=100).values\n",
        "\n",
        "# Create a folder\n",
        "batch_folder = './batch-data'\n",
        "os.makedirs(batch_folder, exist_ok=True)\n",
        "print(\"Folder created!\")\n",
        "\n",
        "# Save each sample as a separate file\n",
        "print(\"Saving files...\")\n",
        "for i in range(100):\n",
        "    fname = str(i+1) + '.csv'\n",
        "    sample[i].tofile(os.path.join(batch_folder, fname), sep=\",\")\n",
        "print(\"files saved!\")\n",
        "\n",
        "# Upload the files to the default datastore\n",
        "print(\"Uploading files to datastore...\")\n",
        "default_ds = ws.get_default_datastore()\n",
        "default_ds.upload(src_dir=\"batch-data\", target_path=\"batch-data\", overwrite=True, show_progress=True)\n",
        "\n",
        "# Register a dataset for the input data\n",
        "batch_data_set = Dataset.File.from_files(path=(default_ds, 'batch-data/'), validate=False)\n",
        "try:\n",
        "    batch_data_set = batch_data_set.register(workspace=ws, \n",
        "                                             name='batch-data',\n",
        "                                             description='batch data',\n",
        "                                             create_new_version=True)\n",
        "except Exception as ex:\n",
        "    print(ex)\n",
        "\n",
        "print(\"Done!\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "azureml_globaldatasets - Default = False\nworkspaceblobstore - Default = True\nworkspacefilestore - Default = False\nworkspaceartifactstore - Default = False\nworkspaceworkingdirectory - Default = False\nFolder created!\nSaving files...\nfiles saved!\nUploading files to datastore...\nUploading an estimated of 100 files\nUploading batch-data/1.csv\nUploaded batch-data/1.csv, 1 files out of an estimated total of 100\nUploading batch-data/10.csv\nUploaded batch-data/10.csv, 2 files out of an estimated total of 100\nUploading batch-data/100.csv\nUploaded batch-data/100.csv, 3 files out of an estimated total of 100\nUploading batch-data/11.csv\nUploaded batch-data/11.csv, 4 files out of an estimated total of 100\nUploading batch-data/12.csv\nUploaded batch-data/12.csv, 5 files out of an estimated total of 100\nUploading batch-data/13.csv\nUploaded batch-data/13.csv, 6 files out of an estimated total of 100\nUploading batch-data/14.csv\nUploaded batch-data/14.csv, 7 files out of an estimated total of 100\nUploading batch-data/15.csv\nUploaded batch-data/15.csv, 8 files out of an estimated total of 100\nUploading batch-data/16.csv\nUploaded batch-data/16.csv, 9 files out of an estimated total of 100\nUploading batch-data/17.csv\nUploaded batch-data/17.csv, 10 files out of an estimated total of 100\nUploading batch-data/18.csv\nUploaded batch-data/18.csv, 11 files out of an estimated total of 100\nUploading batch-data/19.csv\nUploaded batch-data/19.csv, 12 files out of an estimated total of 100\nUploading batch-data/2.csv\nUploaded batch-data/2.csv, 13 files out of an estimated total of 100\nUploading batch-data/20.csv\nUploaded batch-data/20.csv, 14 files out of an estimated total of 100\nUploading batch-data/21.csv\nUploaded batch-data/21.csv, 15 files out of an estimated total of 100\nUploading batch-data/22.csv\nUploaded batch-data/22.csv, 16 files out of an estimated total of 100\nUploading batch-data/25.csv\nUploaded batch-data/25.csv, 17 files out of an estimated total of 100\nUploading batch-data/23.csv\nUploaded batch-data/23.csv, 18 files out of an estimated total of 100\nUploading batch-data/24.csv\nUploaded batch-data/24.csv, 19 files out of an estimated total of 100\nUploading batch-data/26.csv\nUploaded batch-data/26.csv, 20 files out of an estimated total of 100\nUploading batch-data/27.csv\nUploaded batch-data/27.csv, 21 files out of an estimated total of 100\nUploading batch-data/28.csv\nUploaded batch-data/28.csv, 22 files out of an estimated total of 100\nUploading batch-data/29.csv\nUploaded batch-data/29.csv, 23 files out of an estimated total of 100\nUploading batch-data/3.csv\nUploaded batch-data/3.csv, 24 files out of an estimated total of 100\nUploading batch-data/30.csv\nUploaded batch-data/30.csv, 25 files out of an estimated total of 100\nUploading batch-data/31.csv\nUploaded batch-data/31.csv, 26 files out of an estimated total of 100\nUploading batch-data/32.csv\nUploaded batch-data/32.csv, 27 files out of an estimated total of 100\nUploading batch-data/33.csv\nUploaded batch-data/33.csv, 28 files out of an estimated total of 100\nUploading batch-data/34.csv\nUploaded batch-data/34.csv, 29 files out of an estimated total of 100\nUploading batch-data/35.csv\nUploaded batch-data/35.csv, 30 files out of an estimated total of 100\nUploading batch-data/36.csv\nUploaded batch-data/36.csv, 31 files out of an estimated total of 100\nUploading batch-data/37.csv\nUploaded batch-data/37.csv, 32 files out of an estimated total of 100\nUploading batch-data/38.csv\nUploaded batch-data/38.csv, 33 files out of an estimated total of 100\nUploading batch-data/39.csv\nUploaded batch-data/39.csv, 34 files out of an estimated total of 100\nUploading batch-data/4.csv\nUploaded batch-data/4.csv, 35 files out of an estimated total of 100\nUploading batch-data/40.csv\nUploaded batch-data/40.csv, 36 files out of an estimated total of 100\nUploading batch-data/41.csv\nUploaded batch-data/41.csv, 37 files out of an estimated total of 100\nUploading batch-data/44.csv\nUploaded batch-data/44.csv, 38 files out of an estimated total of 100\nUploading batch-data/45.csv\nUploaded batch-data/45.csv, 39 files out of an estimated total of 100\nUploading batch-data/42.csv\nUploaded batch-data/42.csv, 40 files out of an estimated total of 100\nUploading batch-data/43.csv\nUploaded batch-data/43.csv, 41 files out of an estimated total of 100\nUploading batch-data/46.csv\nUploaded batch-data/46.csv, 42 files out of an estimated total of 100\nUploading batch-data/47.csv\nUploaded batch-data/47.csv, 43 files out of an estimated total of 100\nUploading batch-data/48.csv\nUploaded batch-data/48.csv, 44 files out of an estimated total of 100\nUploading batch-data/49.csv\nUploaded batch-data/49.csv, 45 files out of an estimated total of 100\nUploading batch-data/5.csv\nUploaded batch-data/5.csv, 46 files out of an estimated total of 100\nUploading batch-data/50.csv\nUploaded batch-data/50.csv, 47 files out of an estimated total of 100\nUploading batch-data/51.csv\nUploaded batch-data/51.csv, 48 files out of an estimated total of 100\nUploading batch-data/52.csv\nUploaded batch-data/52.csv, 49 files out of an estimated total of 100\nUploading batch-data/53.csv\nUploaded batch-data/53.csv, 50 files out of an estimated total of 100\nUploading batch-data/54.csv\nUploaded batch-data/54.csv, 51 files out of an estimated total of 100\nUploading batch-data/55.csv\nUploaded batch-data/55.csv, 52 files out of an estimated total of 100\nUploading batch-data/56.csv\nUploaded batch-data/56.csv, 53 files out of an estimated total of 100\nUploading batch-data/57.csv\nUploaded batch-data/57.csv, 54 files out of an estimated total of 100\nUploading batch-data/58.csv\nUploaded batch-data/58.csv, 55 files out of an estimated total of 100\nUploading batch-data/59.csv\nUploaded batch-data/59.csv, 56 files out of an estimated total of 100\nUploading batch-data/6.csv\nUploaded batch-data/6.csv, 57 files out of an estimated total of 100\nUploading batch-data/60.csv\nUploaded batch-data/60.csv, 58 files out of an estimated total of 100\nUploading batch-data/61.csv\nUploaded batch-data/61.csv, 59 files out of an estimated total of 100\nUploading batch-data/62.csv\nUploaded batch-data/62.csv, 60 files out of an estimated total of 100\nUploading batch-data/63.csv\nUploaded batch-data/63.csv, 61 files out of an estimated total of 100\nUploading batch-data/64.csv\nUploaded batch-data/64.csv, 62 files out of an estimated total of 100\nUploading batch-data/65.csv\nUploaded batch-data/65.csv, 63 files out of an estimated total of 100\nUploading batch-data/66.csv\nUploaded batch-data/66.csv, 64 files out of an estimated total of 100\nUploading batch-data/67.csv\nUploaded batch-data/67.csv, 65 files out of an estimated total of 100\nUploading batch-data/68.csv\nUploaded batch-data/68.csv, 66 files out of an estimated total of 100\nUploading batch-data/69.csv\nUploaded batch-data/69.csv, 67 files out of an estimated total of 100\nUploading batch-data/7.csv\nUploaded batch-data/7.csv, 68 files out of an estimated total of 100\nUploading batch-data/70.csv\nUploaded batch-data/70.csv, 69 files out of an estimated total of 100\nUploading batch-data/71.csv\nUploaded batch-data/71.csv, 70 files out of an estimated total of 100\nUploading batch-data/72.csv\nUploaded batch-data/72.csv, 71 files out of an estimated total of 100\nUploading batch-data/73.csv\nUploaded batch-data/73.csv, 72 files out of an estimated total of 100\nUploading batch-data/74.csv\nUploaded batch-data/74.csv, 73 files out of an estimated total of 100\nUploading batch-data/75.csv\nUploaded batch-data/75.csv, 74 files out of an estimated total of 100\nUploading batch-data/76.csv\nUploaded batch-data/76.csv, 75 files out of an estimated total of 100\nUploading batch-data/77.csv\nUploaded batch-data/77.csv, 76 files out of an estimated total of 100\nUploading batch-data/78.csv\nUploaded batch-data/78.csv, 77 files out of an estimated total of 100\nUploading batch-data/79.csv\nUploaded batch-data/79.csv, 78 files out of an estimated total of 100\nUploading batch-data/8.csv\nUploaded batch-data/8.csv, 79 files out of an estimated total of 100\nUploading batch-data/80.csv\nUploaded batch-data/80.csv, 80 files out of an estimated total of 100\nUploading batch-data/81.csv\nUploaded batch-data/81.csv, 81 files out of an estimated total of 100\nUploading batch-data/82.csv\nUploaded batch-data/82.csv, 82 files out of an estimated total of 100\nUploading batch-data/83.csv\nUploaded batch-data/83.csv, 83 files out of an estimated total of 100\nUploading batch-data/84.csv\nUploaded batch-data/84.csv, 84 files out of an estimated total of 100\nUploading batch-data/85.csv\nUploaded batch-data/85.csv, 85 files out of an estimated total of 100\nUploading batch-data/86.csv\nUploaded batch-data/86.csv, 86 files out of an estimated total of 100\nUploading batch-data/87.csv\nUploaded batch-data/87.csv, 87 files out of an estimated total of 100\nUploading batch-data/88.csv\nUploaded batch-data/88.csv, 88 files out of an estimated total of 100\nUploading batch-data/89.csv\nUploaded batch-data/89.csv, 89 files out of an estimated total of 100\nUploading batch-data/9.csv\nUploaded batch-data/9.csv, 90 files out of an estimated total of 100\nUploading batch-data/90.csv\nUploaded batch-data/90.csv, 91 files out of an estimated total of 100\nUploading batch-data/91.csv\nUploaded batch-data/91.csv, 92 files out of an estimated total of 100\nUploading batch-data/92.csv\nUploaded batch-data/92.csv, 93 files out of an estimated total of 100\nUploading batch-data/93.csv\nUploaded batch-data/93.csv, 94 files out of an estimated total of 100\nUploading batch-data/94.csv\nUploaded batch-data/94.csv, 95 files out of an estimated total of 100\nUploading batch-data/95.csv\nUploaded batch-data/95.csv, 96 files out of an estimated total of 100\nUploading batch-data/96.csv\nUploaded batch-data/96.csv, 97 files out of an estimated total of 100\nUploading batch-data/98.csv\nUploaded batch-data/98.csv, 98 files out of an estimated total of 100\nUploading batch-data/97.csv\nUploaded batch-data/97.csv, 99 files out of an estimated total of 100\nUploading batch-data/99.csv\nUploaded batch-data/99.csv, 100 files out of an estimated total of 100\nUploaded 100 files\nDone!\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\"Datastore.upload\" is deprecated after version 1.0.69. Please use \"Dataset.File.upload_directory\" to upload your files             from a local directory and create FileDataset in single method call. See Dataset API change notice at https://aka.ms/dataset-deprecation.\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "gather": {
          "logged": 1698951139040
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create compute\n",
        "\n",
        "We'll need a compute context for the pipeline, so we'll use the following code to specify an Azure Machine Learning compute cluster (it will be created if it doesn't already exist).\n",
        "\n",
        "> **Important**: Change *your-compute-cluster* to the name of your compute cluster in the code below before running it! Cluster names must be globally unique names between 2 to 16 characters in length. Valid characters are letters, digits, and the - character."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "\n",
        "cluster_name = \"coursera-cluster\"\n",
        "\n",
        "try:\n",
        "    # Check for existing compute target\n",
        "    inference_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
        "    print('Found existing cluster, use it.')\n",
        "except ComputeTargetException:\n",
        "    # If it doesn't already exist, create it\n",
        "    try:\n",
        "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=2)\n",
        "        inference_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
        "        inference_cluster.wait_for_completion(show_output=True)\n",
        "    except Exception as ex:\n",
        "        print(ex)\n",
        "    "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Found existing cluster, use it.\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "gather": {
          "logged": 1698951456813
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Note**: Compute instances and clusters are based on standard Azure virtual machine images. For this exercise, the *Standard_DS11_v2* image is recommended to achieve the optimal balance of cost and performance. If your subscription has a quota that does not include this image, choose an alternative image; but bear in mind that a larger image may incur higher cost and a smaller image may not be sufficient to complete the tasks. Alternatively, ask your Azure administrator to extend your quota.\n",
        "\n",
        "## Create a pipeline for batch inferencing\n",
        "\n",
        "Now we're ready to define the pipeline we'll use for batch inferencing. Our pipeline will need Python code to perform the batch inferencing, so let's create a folder where we can keep all the files used by the pipeline:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Create a folder for the experiment files\n",
        "experiment_folder = 'batch_pipeline'\n",
        "os.makedirs(experiment_folder, exist_ok=True)\n",
        "\n",
        "print(experiment_folder)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "batch_pipeline\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "gather": {
          "logged": 1698951487282
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll create a Python script to do the actual work, and save it in the pipeline folder:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $experiment_folder/batch_diabetes.py\n",
        "import os\n",
        "import numpy as np\n",
        "from azureml.core import Model\n",
        "import joblib\n",
        "\n",
        "\n",
        "def init():\n",
        "    # Runs when the pipeline step is initialized\n",
        "    global model\n",
        "\n",
        "    # load the model\n",
        "    model_path = Model.get_model_path('diabetes_model')\n",
        "    model = joblib.load(model_path)\n",
        "\n",
        "\n",
        "def run(mini_batch):\n",
        "    # This runs for each batch\n",
        "    resultList = []\n",
        "\n",
        "    # process each file in the batch\n",
        "    for f in mini_batch:\n",
        "        # Read the comma-delimited data into an array\n",
        "        data = np.genfromtxt(f, delimiter=',')\n",
        "        # Reshape into a 2-dimensional array for prediction (model expects multiple items)\n",
        "        prediction = model.predict(data.reshape(1, -1))\n",
        "        # Append prediction to results\n",
        "        resultList.append(\"{}: {}\".format(os.path.basename(f), prediction[0]))\n",
        "    return resultList"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Writing batch_pipeline/batch_diabetes.py\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pipeline will need an environment in which to run, so we'll create a Conda specification that includes the packages that the code uses."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile $experiment_folder/batch_environment.yml\n",
        "name: batch_environment\n",
        "dependencies:\n",
        "- python=3.6.2\n",
        "- scikit-learn\n",
        "- pip\n",
        "- pip:\n",
        "  - azureml-defaults"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Writing batch_pipeline/batch_environment.yml\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we'll define a run context that includes the Conda environment."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Environment\n",
        "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
        "\n",
        "# Create an Environment for the experiment\n",
        "batch_env = Environment.from_conda_specification(\"experiment_env\", experiment_folder + \"/batch_environment.yml\")\n",
        "batch_env.docker.base_image = DEFAULT_CPU_IMAGE\n",
        "print('Configuration ready.')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Configuration ready.\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "gather": {
          "logged": 1698951521512
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You're going to use a pipeline to run the batch prediction script, generate predictions from the input data, and save the results as a text file in the output folder. To do this, you can use a **ParallelRunStep**, which enables the batch data to be processed in parallel and the results collated in a single output file named *parallel_run_step.txt*."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.steps import ParallelRunConfig, ParallelRunStep\n",
        "from azureml.data import OutputFileDatasetConfig\n",
        "\n",
        "output_dir = OutputFileDatasetConfig(name='inferences')\n",
        "\n",
        "parallel_run_config = ParallelRunConfig(\n",
        "    source_directory=experiment_folder,\n",
        "    entry_script=\"batch_diabetes.py\",\n",
        "    mini_batch_size=\"5\",\n",
        "    error_threshold=10,\n",
        "    output_action=\"append_row\",\n",
        "    environment=batch_env,\n",
        "    compute_target=inference_cluster,\n",
        "    node_count=2)\n",
        "\n",
        "parallelrun_step = ParallelRunStep(\n",
        "    name='batch-score-diabetes',\n",
        "    parallel_run_config=parallel_run_config,\n",
        "    inputs=[batch_data_set.as_named_input('diabetes_batch')],\n",
        "    output=output_dir,\n",
        "    arguments=[],\n",
        "    allow_reuse=True\n",
        ")\n",
        "\n",
        "print('Steps defined')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Steps defined\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "gather": {
          "logged": 1698951598038
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it's time to put the step into a pipeline, and run it.\n",
        "\n",
        "> **Note**: This may take some time!"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Experiment\n",
        "from azureml.pipeline.core import Pipeline\n",
        "\n",
        "pipeline = Pipeline(workspace=ws, steps=[parallelrun_step])\n",
        "pipeline_run = Experiment(ws, 'mslearn-diabetes-batch').submit(pipeline)\n",
        "pipeline_run.wait_for_completion(show_output=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Created step batch-score-diabetes [f5a801f4][e123cbd0-99d2-4de7-a750-715968006aac], (This step is eligible to reuse a previous run's output)\nSubmitted PipelineRun 5e6b2383-9c79-49e1-af08-b21801d023ef\nLink to Azure Machine Learning Portal: https://ml.azure.com/runs/5e6b2383-9c79-49e1-af08-b21801d023ef?wsid=/subscriptions/42233ac3-1d6b-40e9-920d-ae1ce13376ed/resourcegroups/rg-dp100-labs/workspaces/mlw-dp100-labs&tid=54f76ea3-fed9-41fc-a628-e91383900a39\nPipelineRunId: 5e6b2383-9c79-49e1-af08-b21801d023ef\nLink to Azure Machine Learning Portal: https://ml.azure.com/runs/5e6b2383-9c79-49e1-af08-b21801d023ef?wsid=/subscriptions/42233ac3-1d6b-40e9-920d-ae1ce13376ed/resourcegroups/rg-dp100-labs/workspaces/mlw-dp100-labs&tid=54f76ea3-fed9-41fc-a628-e91383900a39\nPipelineRun Status: NotStarted\nPipelineRun Status: Running\n\n\nStepRunId: af41090a-d21d-4451-a28c-3c2a29103ab6\nLink to Azure Machine Learning Portal: https://ml.azure.com/runs/af41090a-d21d-4451-a28c-3c2a29103ab6?wsid=/subscriptions/42233ac3-1d6b-40e9-920d-ae1ce13376ed/resourcegroups/rg-dp100-labs/workspaces/mlw-dp100-labs&tid=54f76ea3-fed9-41fc-a628-e91383900a39\nStepRun( batch-score-diabetes ) Status: Queued\nStepRun( batch-score-diabetes ) Status: Running\n\nStepRun(batch-score-diabetes) Execution Summary\n================================================\nStepRun( batch-score-diabetes ) Status: Failed\n\nWarnings:\nAzureMLCompute job failed\n: [REDACTED]\nExecution failed. User process '/azureml-envs/azureml_e220b045f6c3c3008b1a386af067185d/bin/python' exited with status code 42. Please check log file 'user_logs/std_log_1.txt' for error details. Error: Traceback (most recent call last):\n  File \"driver/amlbi_main.py\", line 275, in <module>\n    main()\n  File \"driver/amlbi_main.py\", line 226, in main\n    sys.exit(exitcode_candidate)\nSystemExit: 42\n\n\n"
        },
        {
          "output_type": "error",
          "ename": "ActivityFailedException",
          "evalue": "ActivityFailedException:\n\tMessage: Activity Failed:\n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Execution failed. User process '/azureml-envs/azureml_e220b045f6c3c3008b1a386af067185d/bin/python' exited with status code 42. Please check log file 'user_logs/std_log_1.txt' for error details. Error: Traceback (most recent call last):\\n  File \\\"driver/amlbi_main.py\\\", line 275, in <module>\\n    main()\\n  File \\\"driver/amlbi_main.py\\\", line 226, in main\\n    sys.exit(exitcode_candidate)\\nSystemExit: 42\\n\\n\",\n        \"messageParameters\": {},\n        \"details\": []\n    },\n    \"time\": \"0001-01-01T00:00:00.000Z\",\n    \"componentName\": \"CommonRuntime\"\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Activity Failed:\\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"Execution failed. User process '/azureml-envs/azureml_e220b045f6c3c3008b1a386af067185d/bin/python' exited with status code 42. Please check log file 'user_logs/std_log_1.txt' for error details. Error: Traceback (most recent call last):\\\\n  File \\\\\\\"driver/amlbi_main.py\\\\\\\", line 275, in <module>\\\\n    main()\\\\n  File \\\\\\\"driver/amlbi_main.py\\\\\\\", line 226, in main\\\\n    sys.exit(exitcode_candidate)\\\\nSystemExit: 42\\\\n\\\\n\\\",\\n        \\\"messageParameters\\\": {},\\n        \\\"details\\\": []\\n    },\\n    \\\"time\\\": \\\"0001-01-01T00:00:00.000Z\\\",\\n    \\\"componentName\\\": \\\"CommonRuntime\\\"\\n}\"\n    }\n}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mActivityFailedException\u001b[0m                   Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline(workspace\u001b[38;5;241m=\u001b[39mws, steps\u001b[38;5;241m=\u001b[39m[parallelrun_step])\n\u001b[1;32m      5\u001b[0m pipeline_run \u001b[38;5;241m=\u001b[39m Experiment(ws, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmslearn-diabetes-batch\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msubmit(pipeline)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mpipeline_run\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_for_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshow_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/pipeline/core/run.py:295\u001b[0m, in \u001b[0;36mPipelineRun.wait_for_completion\u001b[0;34m(self, show_output, timeout_seconds, raise_on_error)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 295\u001b[0m     \u001b[43mstep_run\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_for_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_seconds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtime_elapsed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mraise_on_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraise_on_error\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;66;03m# If there are package conflicts in the user's environment, the run rehydration\u001b[39;00m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;66;03m# will not work and we will receive a Run object instead of StepRun.\u001b[39;00m\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;66;03m# Run.wait_for_completion() does not have a parameter timeout_seconds, which\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;66;03m# will generate a TypeError here.  As a workaround, call the method without\u001b[39;00m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;66;03m# this parameter.\u001b[39;00m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(step_run, StepRun):\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/pipeline/core/run.py:738\u001b[0m, in \u001b[0;36mStepRun.wait_for_completion\u001b[0;34m(self, show_output, timeout_seconds, raise_on_error)\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m show_output:\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 738\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream_run_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout_seconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_seconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mraise_on_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraise_on_error\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    740\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    741\u001b[0m         error_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe output streaming for the run interrupted.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m    742\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBut the run is still executing on the compute target. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m    743\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetails for canceling the run can be found here: \u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m    744\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://aka.ms/aml-docs-cancel-run\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/pipeline/core/run.py:831\u001b[0m, in \u001b[0;36mStepRun._stream_run_output\u001b[0;34m(self, timeout_seconds, raise_on_error)\u001b[0m\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mdumps(error, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mand\u001b[39;00m raise_on_error:\n\u001b[0;32m--> 831\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ActivityFailedException(error_details\u001b[38;5;241m=\u001b[39mjson\u001b[38;5;241m.\u001b[39mdumps(error, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m    833\u001b[0m \u001b[38;5;28mprint\u001b[39m(final_details)\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[0;31mActivityFailedException\u001b[0m: ActivityFailedException:\n\tMessage: Activity Failed:\n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Execution failed. User process '/azureml-envs/azureml_e220b045f6c3c3008b1a386af067185d/bin/python' exited with status code 42. Please check log file 'user_logs/std_log_1.txt' for error details. Error: Traceback (most recent call last):\\n  File \\\"driver/amlbi_main.py\\\", line 275, in <module>\\n    main()\\n  File \\\"driver/amlbi_main.py\\\", line 226, in main\\n    sys.exit(exitcode_candidate)\\nSystemExit: 42\\n\\n\",\n        \"messageParameters\": {},\n        \"details\": []\n    },\n    \"time\": \"0001-01-01T00:00:00.000Z\",\n    \"componentName\": \"CommonRuntime\"\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Activity Failed:\\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"Execution failed. User process '/azureml-envs/azureml_e220b045f6c3c3008b1a386af067185d/bin/python' exited with status code 42. Please check log file 'user_logs/std_log_1.txt' for error details. Error: Traceback (most recent call last):\\\\n  File \\\\\\\"driver/amlbi_main.py\\\\\\\", line 275, in <module>\\\\n    main()\\\\n  File \\\\\\\"driver/amlbi_main.py\\\\\\\", line 226, in main\\\\n    sys.exit(exitcode_candidate)\\\\nSystemExit: 42\\\\n\\\\n\\\",\\n        \\\"messageParameters\\\": {},\\n        \\\"details\\\": []\\n    },\\n    \\\"time\\\": \\\"0001-01-01T00:00:00.000Z\\\",\\n    \\\"componentName\\\": \\\"CommonRuntime\\\"\\n}\"\n    }\n}"
          ]
        }
      ],
      "execution_count": 12,
      "metadata": {
        "scrolled": true,
        "vscode": {
          "languageId": "python"
        },
        "gather": {
          "logged": 1698955075941
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When the pipeline has finished running, the resulting predictions will have been saved in the outputs of the experiment associated with the first (and only) step in the pipeline. You can retrieve it as follows:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "\n",
        "# Remove the local results folder if left over from a previous run\n",
        "shutil.rmtree('diabetes-results', ignore_errors=True)\n",
        "\n",
        "# Get the run for the first step and download its output\n",
        "prediction_run = next(pipeline_run.get_children())\n",
        "prediction_output = prediction_run.get_output_data('inferences')\n",
        "prediction_output.download(local_path='diabetes-results')\n",
        "\n",
        "# Traverse the folder hierarchy and find the results file\n",
        "for root, dirs, files in os.walk('diabetes-results'):\n",
        "    for file in files:\n",
        "        if file.endswith('parallel_run_step.txt'):\n",
        "            result_file = os.path.join(root,file)\n",
        "\n",
        "# cleanup output format\n",
        "df = pd.read_csv(result_file, delimiter=\":\", header=None)\n",
        "df.columns = [\"File\", \"Prediction\"]\n",
        "\n",
        "# Display the first 20 results\n",
        "df.head(20)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'inferences'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Get the run for the first step and download its output\u001b[39;00m\n\u001b[1;32m      8\u001b[0m prediction_run \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(pipeline_run\u001b[38;5;241m.\u001b[39mget_children())\n\u001b[0;32m----> 9\u001b[0m prediction_output \u001b[38;5;241m=\u001b[39m \u001b[43mprediction_run\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_output_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minferences\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m prediction_output\u001b[38;5;241m.\u001b[39mdownload(local_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiabetes-results\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Traverse the folder hierarchy and find the results file\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/pipeline/core/run.py:715\u001b[0m, in \u001b[0;36mStepRun.get_output_data\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_output_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[1;32m    706\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;124;03m    Get the output data from a given output.\u001b[39;00m\n\u001b[1;32m    708\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;124;03m    :rtype: azureml.pipeline.core.PortDataReference\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mget_port_data_reference()\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/pipeline/core/run.py:703\u001b[0m, in \u001b[0;36mStepRun.get_output\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_output\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[1;32m    694\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;124;03m    Get the node output with the given name.\u001b[39;00m\n\u001b[1;32m    696\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;124;03m    :rtype: azureml.pipeline.core.StepRunOutput\u001b[39;00m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_run_provider\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_output\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pipeline_run_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_node_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/azureml/pipeline/core/_aeva_provider.py:1320\u001b[0m, in \u001b[0;36m_AevaStepRunProvider.get_output\u001b[0;34m(self, step_run, context, pipeline_run_id, node_id, name)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Gets an output of pipeline run.\u001b[39;00m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;124;03m:param step_run: step run object\u001b[39;00m\n\u001b[1;32m   1309\u001b[0m \u001b[38;5;124;03m:type step_run: StepRun\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1317\u001b[0m \u001b[38;5;124;03m:type name: str\u001b[39;00m\n\u001b[1;32m   1318\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1319\u001b[0m node_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service_caller\u001b[38;5;241m.\u001b[39mget_node_outputs_async(pipeline_run_id, node_id)\n\u001b[0;32m-> 1320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StepRunOutput(context, pipeline_run_id, step_run, name, \u001b[43mnode_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m)\n",
            "\u001b[0;31mKeyError\u001b[0m: 'inferences'"
          ]
        }
      ],
      "execution_count": 13,
      "metadata": {
        "vscode": {
          "languageId": "python"
        },
        "gather": {
          "logged": 1698956171842
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Publish the Pipeline and use its REST Interface\n",
        "\n",
        "Now that you have a working pipeline for batch inferencing, you can publish it and use a REST endpoint to run it from an application."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "published_pipeline = pipeline_run.publish_pipeline(\n",
        "    name='diabetes-batch-pipeline', description='Batch scoring of diabetes data', version='1.0')\n",
        "\n",
        "published_pipeline"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the published pipeline has an endpoint, which you can see in the Azure portal. You can also find it as a property of the published pipeline object:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "rest_endpoint = published_pipeline.endpoint\n",
        "print(rest_endpoint)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use the endpoint, client applications need to make a REST call over HTTP. This request must be authenticated, so an authorization header is required. To test this out, we'll use the authorization header from your current connection to your Azure workspace, which you can get using the following code:\n",
        "\n",
        "> **Note**: A real application would require a service principal with which to be authenticated."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.authentication import InteractiveLoginAuthentication\n",
        "\n",
        "interactive_auth = InteractiveLoginAuthentication()\n",
        "auth_header = interactive_auth.get_authentication_header()\n",
        "print('Authentication header ready.')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we're ready to call the REST interface. The pipeline runs asynchronously, so we'll get an identifier back, which we can use to track the pipeline experiment as it runs:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "rest_endpoint = published_pipeline.endpoint\n",
        "response = requests.post(rest_endpoint, \n",
        "                         headers=auth_header, \n",
        "                         json={\"ExperimentName\": \"mslearn-diabetes-batch\"})\n",
        "run_id = response.json()[\"Id\"]\n",
        "run_id"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we have the run ID, we can use the **RunDetails** widget to view the experiment as it runs:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.pipeline.core.run import PipelineRun\n",
        "from azureml.widgets import RunDetails\n",
        "\n",
        "published_pipeline_run = PipelineRun(ws.experiments['mslearn-diabetes-batch'], run_id)\n",
        "\n",
        "# Block until the run completes\n",
        "published_pipeline_run.wait_for_completion(show_output=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wait for the pipeline run to complete, and then run the following cell to see the results.\n",
        "\n",
        "As before, the results are in the output of the first pipeline step:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "\n",
        "# Remove the local results folder if left over from a previous run\n",
        "shutil.rmtree('diabetes-results', ignore_errors=True)\n",
        "\n",
        "# Get the run for the first step and download its output\n",
        "prediction_run = next(pipeline_run.get_children())\n",
        "prediction_output = prediction_run.get_output_data('inferences')\n",
        "prediction_output.download(local_path='diabetes-results')\n",
        "\n",
        "# Traverse the folder hierarchy and find the results file\n",
        "for root, dirs, files in os.walk('diabetes-results'):\n",
        "    for file in files:\n",
        "        if file.endswith('parallel_run_step.txt'):\n",
        "            result_file = os.path.join(root,file)\n",
        "\n",
        "# cleanup output format\n",
        "df = pd.read_csv(result_file, delimiter=\":\", header=None)\n",
        "df.columns = [\"File\", \"Prediction\"]\n",
        "\n",
        "# Display the first 20 results\n",
        "df.head(20)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you have a pipeline that can be used to batch process daily patient data.\n",
        "\n",
        "**More Information**: For more details about using pipelines for batch inferencing, see the [How to Run Batch Predictions](https://docs.microsoft.com/azure/machine-learning/how-to-run-batch-predictions) in the Azure Machine Learning documentation."
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}